const g=(o,a)=>{const i=o.toLowerCase(),e=a.toLowerCase(),s=[];let n=0,l=0;const c=(t,p=!1)=>{let r="";l===0?r=t.length>20?`… ${t.slice(-20)}`:t:p?r=t.length+l>100?`${t.slice(0,100-l)}… `:t:r=t.length>20?`${t.slice(0,20)} … ${t.slice(-20)}`:t,r&&s.push(r),l+=r.length,p||(s.push(["strong",a]),l+=a.length,l>=100&&s.push(" …"))};let h=i.indexOf(e,n);if(h===-1)return null;for(;h>=0;){const t=h+e.length;if(c(o.slice(n,h)),n=t,l>100)break;h=i.indexOf(e,n)}return l<100&&c(o.slice(n),!0),s},d=Object.entries,y=Object.keys,f=o=>o.reduce((a,{type:i})=>a+(i==="title"?50:i==="heading"?20:i==="custom"?10:1),0),$=(o,a)=>{var i;const e={};for(const[s,n]of d(a)){const l=((i=a[s.replace(/\/[^\\]*$/,"")])==null?void 0:i.title)||"",c=`${l?`${l} > `:""}${n.title}`,h=g(n.title,o);h&&(e[c]=[...e[c]||[],{type:"title",path:s,display:h}]),n.customFields&&d(n.customFields).forEach(([t,p])=>{p.forEach(r=>{const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"custom",path:s,index:t,display:u}])})});for(const t of n.contents){const p=g(t.header,o);p&&(e[c]=[...e[c]||[],{type:"heading",path:s+(t.slug?`#${t.slug}`:""),display:p}]);for(const r of t.contents){const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"content",header:t.header,path:s+(t.slug?`#${t.slug}`:""),display:u}])}}}return y(e).sort((s,n)=>f(e[s])-f(e[n])).map(s=>({title:s,contents:e[s]}))},m=JSON.parse("{\"/\":{\"/intro.html\":{\"title\":\"介绍页\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"欢迎来到本站，本站仅是个人代码的记录和分享，相关内容均是开源\",\"加密内容是还没准备好的内容，不用保持好奇\",\"欢迎到媒体站留言 或者 提交Issue到Github\",\"本站采用以下技术支撑，感谢开发这些技术的大佬\",\"静态页面引擎：VuePress\",\"主题：vuepress-theme-hope (vuejs.press)\",\"本站内容，转载需要遵照协议注明出处\"]}]},\"/demo/disable.html\":{\"title\":\"布局与功能禁用\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"禁用\"]}},\"/demo/encrypt.html\":{\"title\":\"密码加密的文章\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"文章加密\"]}},\"/demo/markdown.html\":{\"title\":\"Markdown 展示\",\"contents\":[{\"header\":\"Markdown 介绍\",\"slug\":\"markdown-介绍\",\"contents\":[]},{\"header\":\"Markdown 配置\",\"slug\":\"markdown-配置\",\"contents\":[]},{\"header\":\"Markdown 扩展\",\"slug\":\"markdown-扩展\",\"contents\":[]},{\"header\":\"VuePress 扩展\",\"slug\":\"vuepress-扩展\",\"contents\":[]},{\"header\":\"主题扩展\",\"slug\":\"主题扩展\",\"contents\":[]},{\"header\":\"自定义容器\",\"slug\":\"自定义容器\",\"contents\":[]},{\"header\":\"代码块\",\"slug\":\"代码块\",\"contents\":[]},{\"header\":\"上下角标\",\"slug\":\"上下角标\",\"contents\":[]},{\"header\":\"自定义对齐\",\"slug\":\"自定义对齐\",\"contents\":[]},{\"header\":\"Attrs\",\"slug\":\"attrs\",\"contents\":[]},{\"header\":\"脚注\",\"slug\":\"脚注\",\"contents\":[]},{\"header\":\"标记\",\"slug\":\"标记\",\"contents\":[]},{\"header\":\"任务列表\",\"slug\":\"任务列表\",\"contents\":[]},{\"header\":\"图片增强\",\"slug\":\"图片增强\",\"contents\":[]}],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"Markdown\"]}},\"/demo/page.html\":{\"title\":\"页面配置\",\"contents\":[{\"header\":\"页面信息\",\"slug\":\"页面信息\",\"contents\":[]},{\"header\":\"页面内容\",\"slug\":\"页面内容\",\"contents\":[]}],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"页面配置\",\"使用指南\"]}},\"/demo/\":{\"title\":\"主要功能与配置演示\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"]}},\"/wait/Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98.html\":{\"title\":\"Hadoop概览\",\"contents\":[{\"header\":\"什么是Hadoop？它解决了什么问题？\",\"slug\":\"什么是hadoop-它解决了什么问题\",\"contents\":[]},{\"header\":\"Hadoop的核心组件是什么？分别解释它们的作用。\",\"slug\":\"hadoop的核心组件是什么-分别解释它们的作用。\",\"contents\":[]},{\"header\":\"HDFS是什么？它的特点是什么？\",\"slug\":\"hdfs是什么-它的特点是什么\",\"contents\":[]},{\"header\":\"MapReduce是什么？它的工作原理是什么？\",\"slug\":\"mapreduce是什么-它的工作原理是什么\",\"contents\":[]},{\"header\":\"YARN是什么？它的作用是什么？\",\"slug\":\"yarn是什么-它的作用是什么\",\"contents\":[]},{\"header\":\"Hadoop生态系统中的其他组件有哪些，比如HBase、Hive、Pig等？它们的用途是什么？\",\"slug\":\"hadoop生态系统中的其他组件有哪些-比如hbase、hive、pig等-它们的用途是什么\",\"contents\":[]},{\"header\":\"解释一下Hadoop的三层架构。\",\"slug\":\"解释一下hadoop的三层架构。\",\"contents\":[]},{\"header\":\"Hadoop的高可用性是如何实现的？\",\"slug\":\"hadoop的高可用性是如何实现的\",\"contents\":[]},{\"header\":\"讲解一下Hadoop集群的架构，包括主节点和从节点。\",\"slug\":\"讲解一下hadoop集群的架构-包括主节点和从节点。\",\"contents\":[]},{\"header\":\"YARN的作用是什么？它如何实现资源管理和作业调度？\",\"slug\":\"yarn的作用是什么-它如何实现资源管理和作业调度\",\"contents\":[]},{\"header\":\"什么是任务（Task）？Map任务和Reduce任务有什么区别？\",\"slug\":\"什么是任务-task-map任务和reduce任务有什么区别\",\"contents\":[]},{\"header\":\"请解释一下任务调度器（Scheduler）和资源管理器（ResourceManager）的作用。\",\"slug\":\"请解释一下任务调度器-scheduler-和资源管理器-resourcemanager-的作用。\",\"contents\":[]},{\"header\":\"如何在Hadoop中处理大量的小文件？\",\"slug\":\"如何在hadoop中处理大量的小文件\",\"contents\":[]},{\"header\":\"Hadoop的数据写入流程是什么？涉及哪些步骤？\",\"slug\":\"hadoop的数据写入流程是什么-涉及哪些步骤\",\"contents\":[]},{\"header\":\"如何在MapReduce中处理数据倾斜问题？\",\"slug\":\"如何在mapreduce中处理数据倾斜问题\",\"contents\":[]},{\"header\":\"如何优化MapReduce作业的性能？\",\"slug\":\"如何优化mapreduce作业的性能\",\"contents\":[]},{\"header\":\"Hadoop中常见的故障有哪些？如何排除这些故障？\",\"slug\":\"hadoop中常见的故障有哪些-如何排除这些故障\",\"contents\":[]},{\"header\":\"Hadoop的安全性如何确保？有哪些层面的安全措施？\",\"slug\":\"hadoop的安全性如何确保-有哪些层面的安全措施\",\"contents\":[]},{\"header\":\"什么是Kerberos？它在Hadoop中的作用是什么？\",\"slug\":\"什么是kerberos-它在hadoop中的作用是什么\",\"contents\":[]},{\"header\":\"举例说明一个适合使用Hadoop的实际应用场景。\",\"slug\":\"举例说明一个适合使用hadoop的实际应用场景。\",\"contents\":[]}],\"customFields\":{\"1\":[\"文章加密\"]}},\"/wait/%E5%B7%A5%E4%BD%9C%E6%B5%81/Lua%E8%84%9A%E6%9C%AC.html\":{\"title\":\"Redis与Lua脚本\",\"contents\":[{\"header\":\"如何执行Lua脚本\",\"slug\":\"如何执行lua脚本\",\"contents\":[]},{\"header\":\"执行命令\",\"slug\":\"执行命令\",\"contents\":[]},{\"header\":\"EVAL\",\"slug\":\"eval\",\"contents\":[]},{\"header\":\"EVALSHA\",\"slug\":\"evalsha\",\"contents\":[]},{\"header\":\"辅助命令\",\"slug\":\"辅助命令\",\"contents\":[]},{\"header\":\"script load\",\"slug\":\"script-load\",\"contents\":[]},{\"header\":\"script exists\",\"slug\":\"script-exists\",\"contents\":[]},{\"header\":\"script kill\",\"slug\":\"script-kill\",\"contents\":[]},{\"header\":\"script flush\",\"slug\":\"script-flush\",\"contents\":[]}],\"customFields\":{\"1\":[\"Lua\",\"Redis\"]}},\"/zh/%E5%85%B6%E4%BB%96/%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BB%AC.html\":{\"title\":\"学习工具论\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"对于软件行业来说，技术更新，非常快，不断学习，是我们行业生涯要做的事。没有天赋的我，便开始研究起学习方法论了。这篇博客，便是介绍我的学习工具，希望对大家有所帮助。\",\"除了软件工具，我想把物质的放在第一位，这些看似无关的物质条件，可以帮助我们保持更好的状态。\",\"首先最重要的便是水，水可以帮助我们更好的思考，解决久坐、熬夜、用眼过度带来的问题，希望大家能多喝水\",\"第二重要的便是咖啡因饮料（无论茶、咖啡还是咖啡因含汽饮料），谁年轻不是精力旺盛的，彷佛一天不够用一样。但是专注对于学习和工作的效率都太重要了，特别是年龄增长以后。我曾经是个拒绝咖啡因的人。上瘾，听起来对人体不太好。但这只是工具。\",\"含碳水的小零食也应该常备，胖子也会低血糖，别低估你的大脑消耗。\",\"相信不少人会有工具纠结症。尤其是开发初学时候，我常常纠结eclipse和IDEA谁更好（别纠结，没什么用）。同样的，做学习笔记，我也不纠结，根据需求去选择工具，相信这样对你有所参考。\",\"首先是博客，记录和分享是我常用的形式，记录让我不会忘记，方便搜索查找，分享会让我认真对待文字，让我过一段时间都能看懂。其次是随手记录，很多笔记软件，往往PC上看着挺好用的，实际很手机上很不方便。当然我是做IT的，代码示例方便与否都是重要的参考项。当然还有成本和可迁移性。\",\"代码上让我首先排除掉Word等一种文档工具，支持代码展示的Markdown一类就是我的首选。通过本地来看我选择Typora，体验太好了，而且我有OneDrive，可以节省很多成本，但是Typora不支持移动端。不能满足我随手记的需求。然后发现了Markdown的云文档产品，可以补充我的随手记需求，语雀、金山文档、Notion都是可以的（大厂，不担心跑路）。最后是分享，开始我是在微信、知乎做分享，后面不想出卖自己的数据，干脆通过GitHub Pages 搞了一个，节省了不少成本和时间。\",\"总体来说就是云文档（我用的免费的）随手记，东西多了可以直接导出Markdown，通过Typora整理保存在OneDrive，不错的通过GitHub Pages分享出来。\",\"国人很注重私密分享，一个好的技术群，能收获不少技术和发展的点，光靠着百度的学习指南，太过概念了，很多细节技术，要多交流才能了解到，所以加个群吧\"]}],\"customFields\":{\"1\":[\"学习方法论\",\"工具\"]}},\"/zh/%E5%85%B6%E4%BB%96/%E6%90%AD%E5%BB%BA%E9%97%AE%E9%A2%98.html\":{\"title\":\"搭建博客\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"搭建过程中遇到一些问题，使用了一些解决方案，在此处写下来，提供给大家参考\"]},{\"header\":\"图片问题\",\"slug\":\"图片问题\",\"contents\":[]},{\"header\":\"路径问题\",\"slug\":\"路径问题\",\"contents\":[\"Vuepress对于带有空格的文件夹支持都不太好，图片无法读取\",\"建议： 不要使用带有空格的文件夹 or 条件允许使用图片服务器吧\",\"但是文件带有空格也会有问题， typora 能正常访问，最好不要使用带有空格的路径\"]},{\"header\":\"CSS渲染问题\",\"slug\":\"css渲染问题\",\"contents\":[\"构建过程中遇到明明dev环境正常查看，但是build后css失效问题，这个和使用的vue-theme-hope主题有关，具体解决方法参考这位博主的文章\",\"关于vuepress-theme-hope运行build后静态网页的css样式失效的问题 - 掘金 (juejin.cn)\"]}]},\"/zh/%E5%89%8D%E7%AB%AF/\":{\"title\":\"前端相关文章\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"前端相关文章\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/\":{\"title\":\"后端\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"后端相关文章\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/\":{\"title\":\"Java\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/\":{\"title\":\"Java大数据\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Java大数据相关文章\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/Java%20SE%E8%A6%81%E7%82%B9.html\":{\"title\":\"Java SE 要点\",\"contents\":[{\"header\":\"枚举\",\"slug\":\"枚举\",\"contents\":[\"原文：枚举为什么可以用==判断相等\",\"详细：答案是肯定的，因为枚举有着严格的实例化控制，所以你可以用 == 去做比较符，这个用法，在官方文档中也有明确的说明。而且枚举equals（）方法的底层就是用==判断。\",\" /** * Returns true if the specified object is equal to this * enum constant. * * @param other the object to be compared for equality with this object. * @return true if the specified object is equal to this * enum constant. */ public final boolean equals(Object other) { return this==other; } \",\"基础类型的包装类型也用 == 判断\"]},{\"header\":\"Pattern.compile函数的用法\",\"slug\":\"pattern-compile函数的用法\",\"contents\":[\"在使用Pattern.compile函数时，可以加入控制正则表达式的匹配行为的参数： Pattern Pattern.compile(String regex, int flag) \",\"flag的取值范围如下：\",\"Pattern.CANON_EQ 当且仅当两个字符的\\\"正规分解(canonical decomposition)\\\"都完全相同的情况下，才认定匹配。比如用了这个标志之后，表达式\\\"\\\\u030A\\\"会匹配?。默认情况下，不考虑规范相等性(canonical equivalence)\\\"。\",\"Pattern.CASE_INSENSITIVE 默认情况下，大小写不明感的匹配只适用于US-ASCII字符集。这个标志能让表达式忽略大小写进行匹配。要想对Unicode字符进行大小不明感的匹 配，只要将UNICODE_CASE与这个标志合起来就行了。\",\"Pattern.COMMENTS 在这种模式下，匹配时会忽略(正则表达式里的)空格字符(译者注：不是指表达式里的\\\"\\\\s\\\"，而是指表达式里的空格，tab，回车之类)。注释从#开始，一直到这行结束。可以通过嵌入式的标志来启用Unix行模式。\",\"Pattern.DOTALL 在这种模式下，表达式'.'可以匹配任意字符，包括表示一行的结束符。默认情况下，表达式'.'不匹配行的结束符。\",\"Pattern.MULTILINE 在这种模式下， ^ 和 $ 分别匹配一行的开始和结束。此外，^ 仍然匹配字符串的开始，$ 也匹配字符串的结束。默认情况下，这两个表达式仅仅匹配字符串的开始和结束。\",\"Pattern.UNICODE_CASE 在这个模式下，如果你还启用了CASE_INSENSITIVE标志，那么它会对Unicode字符进行大小写不明感的匹配。默认情况下，大小写不敏感的匹配只适用于US-ASCII字符集。\",\"Pattern.UNIX_LINES 在这个模式下，只有'\\\\n'才被认作一行的中止，并且与.，^，以及$进行匹配。\"]},{\"header\":\"lamda表达式\",\"slug\":\"lamda表达式\",\"contents\":[]},{\"header\":\"ParallelStream\",\"slug\":\"parallelstream\",\"contents\":[\"​\\t\\tparallelStream 是 Java lamda表达式当中并行流写法，本质上并行方法，效率很高，但是需要注意会产生多线程下的各种问题，比如使用非线程安全的集合类，会导致空指针和数组下标越界等问题。如果使用.collect最终将结果收集起来就不会有个这个问题或者使用CopyOnWriteArrayList， CopyOnWriteArraySet 这类线程安全的集合来避免问题。千万不能使用 parallelStream 去循环处理非线程安全的流程。\"]}],\"customFields\":{\"1\":[\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/%E7%BA%BF%E7%A8%8B%E6%B1%A0.html\":{\"title\":\"线程池\",\"contents\":[{\"header\":\"始\",\"slug\":\"始\",\"contents\":[\"​ 多线程是项目扩大以后，必不可少需要应用到的技术。线程池的应用，更加便利多线程的使用。 Java 默认支持线程池，在 Java 线程池的基础上，Pivotal 在 Spring 框架内，提供了 Spring 线程池。两者皆是主流的线程池。\"]},{\"header\":\"基本概念\",\"slug\":\"基本概念\",\"contents\":[\"线程池，本质上是一种对象池，用于管理线程资源。\",\"在任务执行前，需要从线程池中拿出线程来执行。\",\"在任务执行完成之后，需要把线程放回线程池。\",\"通过线程的这种反复利用机制，可以有效地避免直接创建线程所带来的坏处。\"]},{\"header\":\"优势\",\"slug\":\"优势\",\"contents\":[\"降低资源的消耗。线程本身是一种资源，创建和销毁线程会有CPU开销；创建的线程也会占用一定的内存。\",\"提高任务执行的响应速度。任务执行时，可以不必等到线程创建完之后再执行。\",\"提高线程的可管理性。线程不能无限制地创建，需要进行统一的分配、调优和监控。\"]},{\"header\":\"缺点\",\"slug\":\"缺点\",\"contents\":[\"频繁的线程创建和销毁会占用更多的CPU和内存\",\"频繁的线程创建和销毁会对 GC 产生比较大的压力\",\"线程太多，线程切换带来的开销将不可忽视\",\"线程太少，多核CPU得不到充分利用，是一种浪费\"]},{\"header\":\"线程池处理流程\",\"slug\":\"线程池处理流程\",\"contents\":[\"判断核心线程池是否已满，如果不是，则创建线程执行任务\",\"如果核心线程池满了，判断队列是否满了，如果队列没满，将任务放在队列中\",\"如果队列满了，则判断线程池是否已满，如果没满，创建线程执行任务\",\"如果线程池也满了，则按照拒绝策略对任务进行处理\"]},{\"header\":\"Java 线程池\",\"slug\":\"java-线程池\",\"contents\":[\"JDK 里 ThreadPoolExecutor 的线程池处理流程\"]},{\"header\":\"入门案例\",\"slug\":\"入门案例\",\"contents\":[\"public class ThreadPoolTest { public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(5); for (int i = 0; i < 10; i++) { executor.submit(() -> { System.out.println(\\\"thread id is: \\\" + Thread.currentThread().getId()); try { Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } }); } } } \",\"​ 在这个例子中，我们首先创建了一个固定长度为5的线程池。然后使用循环的方式往线程池中提交了10个任务，每个任务休眠1秒。在任务休眠之前，将任务所在的线程id进行打印输出。所以，理论上只会打印5个不同的线程id，且每个线程id会被打印2次。\"]},{\"header\":\"Executors-Java线程池\",\"slug\":\"executors-java线程池\",\"contents\":[\"Executors是一个线程池工厂，提供了很多的工厂方法。\",\"// 创建单一线程的线程池 public static ExecutorService newSingleThreadExecutor(); // 创建固定数量的线程池 public static ExecutorService newFixedThreadPool(int nThreads); // 创建带缓存的线程池 public static ExecutorService newCachedThreadPool(); // 创建定时调度的线程池 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize); // 创建流式（fork-join）线程池 public static ExecutorService newWorkStealingPool(); \"]},{\"header\":\"创建单一线程的线程池\",\"slug\":\"创建单一线程的线程池\",\"contents\":[\"顾名思义，这个线程池只有一个线程。若多个任务被提交到此线程池，那么会被缓存到队列（队列长度为Integer.MAX_VALUE）。当线程空闲的时候，按照FIFO的方式进行处理。\"]},{\"header\":\"创建固定数量的线程池\",\"slug\":\"创建固定数量的线程池\",\"contents\":[\"和 创建单一线程的线程池 类似，只是这儿可以并行处理任务的线程数更多一些罢了。若多个任务被提交到此线程池，会有下面的处理过程。\",\"如果线程的数量未达到指定数量，则创建线程来执行任务\",\"如果线程池的数量达到了指定数量，并且有线程是空闲的，则取出空闲线程执行任务\",\"如果没有线程是空闲的，则将任务缓存到队列（队列长度为Integer.MAX_VALUE）。当线程空闲的时候，按照FIFO的方式进行处理\"]},{\"header\":\"创建带缓存的线程池\",\"slug\":\"创建带缓存的线程池\",\"contents\":[\"这种方式创建的线程池，核心线程池的长度为0，线程池最大长度为Integer.MAX_VALUE 。由于本身使用SynchronousQueue 作为等待队列的缘故，导致往队列里面每插入一个元素，必须等待另一个线程从这个队列删除一个元素。\"]},{\"header\":\"创建定时调度的线程池\",\"slug\":\"创建定时调度的线程池\",\"contents\":[\"和上面3个工厂方法返回的线程池类型有所不同，它返回的是ScheduledThreadPoolExecutor类型的线程池。平时我们实现定时调度功能的时候，可能更多的是使用第三方类库，比如：quartz等。但是对于更底层的功能，我们仍然需要了解。下面是案例\",\"public class ThreadPoolTest { public static void main(String[] args) { ScheduledExecutorService executor = Executors.newScheduledThreadPool(2); // 定时调度，每个调度任务会至少等待`period`的时间， // 如果任务执行的时间超过`period`，则等待的时间为任务执行的时间 executor.scheduleAtFixedRate(() -> { try { Thread.sleep(10000); System.out.println(System.currentTimeMillis() / 1000); } catch (InterruptedException e) { e.printStackTrace(); } }, 0, 2, TimeUnit.SECONDS); // 定时调度，第二个任务执行的时间 = 第一个任务执行时间 + `delay` executor.scheduleWithFixedDelay(() -> { try { Thread.sleep(5000); System.out.println(System.currentTimeMillis() / 1000); } catch (InterruptedException e) { e.printStackTrace(); } }, 0, 2, TimeUnit.SECONDS); // 定时调度，延迟`delay`后执行，且只执行一次 executor.schedule(() -> System.out.println(\\\"5 秒之后执行 schedule\\\"), 5, TimeUnit.SECONDS); } } \",\"上述代码不同调度方法简述：\",\"scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)，定时调度，每个调度任务会至少等待 period 的时间，如果任务执行的时间超过period，则等待的时间为任务执行的时间\",\"scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)，定时调度，第二个任务执行的时间 = 第一个任务执行时间 + `delay\",\"schedule(Runnable command, long delay, TimeUnit unit)，定时调度，延迟delay后执行，且只执行一次\"]},{\"header\":\"手动创建线程池\",\"slug\":\"手动创建线程池\",\"contents\":[\"​ 理论上，我们可以通过Executors来创建线程池，这种方式非常简单。但正是因为简单，所以限制了线程池的功能。比如：无长度限制的队列，可能因为任务堆积导致OOM，这是非常严重的bug，应尽可能地避免。怎么避免？归根结底，还是需要我们通过更底层的方式来创建线程池。\",\"​ 抛开定时调度的线程池不管，我们看看ThreadPoolExecutor。它提供了好几个构造方法，但是最底层的构造方法却只有一个。那么，我们就从这个构造方法着手分析。\",\"public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler); \",\"参数解析：\",\"corePoolSize，线程池中的核心线程数\",\"maximumPoolSize，线程池中的最大线程数\",\"keepAliveTime，空闲时间，当线程池数量超过核心线程数时，多余的空闲线程存活的时间，即：这些线程多久被销毁。\",\"unit，空闲时间的单位，可以是毫秒、秒、分钟、小时和天，等等\",\"workQueue，等待队列，线程池中的线程数超过核心线程数时，任务将放在等待队列，它是一个BlockingQueue类型的对象\",\"threadFactory，线程工厂，我们可以使用它来创建一个线程\",\"handler，拒绝策略，当线程池和等待队列都满了之后，需要通过该对象的回调函数进行回调处理\"]},{\"header\":\"等待队列-workQueue\",\"slug\":\"等待队列-workqueue\",\"contents\":[\"等待队列是BlockingQueue类型的，理论上只要是它的子类，我们都可以用来作为等待队列。\",\"同时，jdk内部自带一些阻塞队列，我们来看看大概有哪些。\",\"ArrayBlockingQueue，队列是有界的，基于数组实现的阻塞队列\",\"LinkedBlockingQueue，队列可以有界，也可以无界。基于链表实现的阻塞队列\",\"SynchronousQueue，不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作将一直处于阻塞状态。该队列也是Executors.newCachedThreadPool()的默认队列\",\"PriorityBlockingQueue，带优先级的无界阻塞队列\",\"通常情况下，我们需要指定阻塞队列的上界（比如1024）。另外，如果执行的任务很多，我们可能需要将任务进行分类，然后将不同分类的任务放到不同的线程池中执行。\"]},{\"header\":\"线程工厂-threadFactory\",\"slug\":\"线程工厂-threadfactory\",\"contents\":[\"ThreadFactory是一个接口，只有一个方法。既然是线程工厂，那么我们就可以用它生产一个线程对象。来看看这个接口的定义。\",\"public interface ThreadFactory { /** * Constructs a new {@code Thread}. Implementations may also initialize * priority, name, daemon status, {@code ThreadGroup}, etc. * * @param r a runnable to be executed by new thread instance * @return constructed thread, or {@code null} if the request to * create a thread is rejected */ Thread newThread(Runnable r); } \",\"Executors的实现使用了默认的线程工厂-DefaultThreadFactory。它的实现主要用于创建一个线程，线程的名字为pool-{poolNum}-thread-{threadNum}。\",\"static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \\\"pool-\\\" + poolNumber.getAndIncrement() + \\\"-thread-\\\"; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; } } \",\"很多时候，我们需要自定义线程名字。我们只需要自己实现ThreadFactory，用于创建特定场景的线程即可。\"]},{\"header\":\"拒绝策略-handler\",\"slug\":\"拒绝策略-handler\",\"contents\":[\"所谓拒绝策略，就是当线程池满了、队列也满了的时候，我们对任务采取的措施。或者丢弃、或者执行、或者其他...\",\"jdk自带4种拒绝策略，我们来看看。\",\"CallerRunsPolicy // 在调用者线程执行\",\"AbortPolicy // 直接抛出RejectedExecutionException异常\",\"DiscardPolicy // 任务直接丢弃，不做任何处理\",\"DiscardOldestPolicy // 丢弃队列里最旧的那个任务，再尝试执行当前任务\",\"这四种策略各有优劣，比较常用的是DiscardPolicy，但是这种策略有一个弊端就是任务执行的轨迹不会被记录下来。所以，我们往往需要实现自定义的拒绝策略， 通过实现RejectedExecutionHandler接口的方式。\"]},{\"header\":\"提交任务的几种方式\",\"slug\":\"提交任务的几种方式\",\"contents\":[\"往线程池中提交任务，主要有两种方法，execute()和submit()。\",\"execute()用于提交不需要返回结果的任务，我们看一个例子。\",\"public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(2); executor.execute(() -> System.out.println(\\\"hello\\\")); } \",\"submit()用于提交一个需要返回结果的任务。该方法返回一个Future对象，通过调用这个对象的get()方法，我们就能获得返回结果。get()方法会一直阻塞，直到返回结果返回。另外，我们也可以使用它的重载方法get(long timeout, TimeUnit unit)，这个方法也会阻塞，但是在超时时间内仍然没有返回结果时，将抛出异常TimeoutException。\",\"public static void main(String[] args) throws Exception { ExecutorService executor = Executors.newFixedThreadPool(2); Future<Long> future = executor.submit(() -> { System.out.println(\\\"task is executed\\\"); return System.currentTimeMillis(); }); System.out.println(\\\"task execute time is: \\\" + future.get()); } \"]},{\"header\":\"关闭线程池\",\"slug\":\"关闭线程池\",\"contents\":[\"在线程池使用完成之后，我们需要对线程池中的资源进行释放操作，这就涉及到关闭功能。我们可以调用线程池对象的 shutdown() 和 shutdownNow() 方法来关闭线程池。\",\"这两个方法都是关闭操作，又有什么不同呢？\",\"shutdown()会将线程池状态置为SHUTDOWN，不再接受新的任务，同时会等待线程池中已有的任务执行完成再结束。\",\"shutdownNow()会将线程池状态置为SHUTDOWN，对所有线程执行interrupt()操作，清空队列，并将队列中的任务返回回来。\",\"另外，关闭线程池涉及到两个返回boolean的方法，isShutdown()和isTerminated，分别表示是否关闭和是否终止。\"]},{\"header\":\"如何正确配置线程池的参数\",\"slug\":\"如何正确配置线程池的参数\",\"contents\":[\"前面我们讲到了手动创建线程池涉及到的几个参数，那么我们要如何设置这些参数才算是正确的应用呢？实际上，需要根据任务的特性来分析。\",\"任务的性质：CPU密集型、IO密集型和混杂型\",\"任务的优先级：高中低\",\"任务执行的时间：长中短\",\"任务的依赖性：是否依赖数据库或者其他系统资源\",\"不同的性质的任务，我们采取的配置将有所不同。在《Java并发编程实践》中有相应的计算公式。\",\"通常来说，如果任务属于CPU密集型，那么我们可以将线程池数量设置成CPU的个数，以减少线程切换带来的开销。如果任务属于IO密集型，我们可以将线程池数量设置得更多一些，比如CPU个数*2。\",\"PS：我们可以通过Runtime.getRuntime().availableProcessors()来获取CPU的个数\"]},{\"header\":\"线程池监控\",\"slug\":\"线程池监控\",\"contents\":[\"如果系统中大量用到了线程池，那么我们有必要对线程池进行监控。利用监控，我们能在问题出现前提前感知到，也可以根据监控信息来定位可能出现的问题。\",\"那么我们可以监控哪些信息？又有哪些方法可用于我们的扩展支持呢？\",\"首先，ThreadPoolExecutor自带了一些方法。\",\"long getTaskCount()，获取已经执行或正在执行的任务数\",\"long getCompletedTaskCount()，获取已经执行的任务数\",\"int getLargestPoolSize()，获取线程池曾经创建过的最大线程数，根据这个参数，我们可以知道线程池是否满过\",\"int getPoolSize()，获取线程池线程数\",\"int getActiveCount()，获取活跃线程数（正在执行任务的线程数）\",\"其次，ThreadPoolExecutor留给我们自行处理的方法有3个，它在ThreadPoolExecutor中为空实现（也就是什么都不做）。\",\"protected void beforeExecute(Thread t, Runnable r) // 任务执行前被调用\",\"protected void afterExecute(Runnable r, Throwable t) // 任务执行后被调用\",\"protected void terminated() // 线程池结束后被调用\",\"public class ThreadPoolTest { public static void main(String[] args) { ExecutorService executor = new ThreadPoolExecutor(1, 1, 1, TimeUnit.SECONDS, new ArrayBlockingQueue<>(1)) { @Override protected void beforeExecute(Thread t, Runnable r) { System.out.println(\\\"beforeExecute is called\\\"); } @Override protected void afterExecute(Runnable r, Throwable t) { System.out.println(\\\"afterExecute is called\\\"); } @Override protected void terminated() { System.out.println(\\\"terminated is called\\\"); } }; executor.submit(() -> System.out.println(\\\"this is a task\\\")); executor.shutdown(); } } \",\"PS：在使用submit()的时候一定要注意它的返回对象Future，为了避免任务执行异常被吞掉的问题，我们需要调用Future.get()方法。另外，使用execute()将不会出现这种问题。\"]},{\"header\":\"Spring 线程池\",\"slug\":\"spring-线程池\",\"contents\":[]},{\"header\":\"常用线程池\",\"slug\":\"常用线程池\",\"contents\":[]},{\"header\":\"TaskExecutor接口相关实现类\",\"slug\":\"taskexecutor接口相关实现类\",\"contents\":[\"名字\",\"特点\",\"SimpleAsyncTaskExecutor\",\"每次请求新开线程，没有最大线程数设置.不是真的线程池，这个类不重用线程，每次调用都会创建一个新的线程。 --【1】\",\"SyncTaskExecutor\",\"不是异步的线程.同步可以用SyncTaskExecutor，但这个可以说不算一个线程池，因为还在原线程执行。这个类没有实现异步调用，只是一个同步操作。\",\"ConcurrentTaskExecutor\",\"Executor的适配类，不推荐使用。如果ThreadPoolTaskExecutor不满足要求时，才用考虑使用这个类。\",\"SimpleThreadPoolTaskExecutor\",\"监听Spring’s lifecycle callbacks，并且可以和Quartz的Component兼容.是Quartz的SimpleThreadPool的类。线程池同时被quartz和非quartz使用，才需要使用此类。\",\"ThreadPoolTaskExecutor\",\"最常用。要求jdk版本大于等于5。可以在程序而不是xml里修改线程池的配置.其实质是对java.util.concurrent.ThreadPoolExecutor的包装。\",\"TimerTaskExecutor\",\"此实现使用CommonJ WorkManager作为其后备服务提供程序，并且是在Spring应用程序上下文中在WebLogic或WebSphere上设置基于CommonJ的线程池集成的中心便利类。\",\"WorkManagerTaskExecutor\",\"此实现在JSR-236兼容的运行时环境（例如Java EE 7+应用程序服务器）中使用JNDI获取的ManagedExecutorService，为此目的替换CommonJ WorkManager。(说明了就是依赖环境)\"]},{\"header\":\"相关注解\",\"slug\":\"相关注解\",\"contents\":[\"注解名\",\"解释\",\"@EnableAsync\",\"开启异步执行。官方文档中解释:该注解添加到@Configuration标注的类上以开始异步执行。开启后@Async标注的方法或类即可异步执行。\",\"@Async\",\"异步执行注解。可标注类和方法。标注类时，则该类下所有方法均可使用异步执行。标注方法时，则该方法可使用异步执行。当标注有@Configuration注解的配置类上标注了@EnableAsync注解后即可生效。\"]},{\"header\":\"SyncTaskExecutor 同步线程池\",\"slug\":\"synctaskexecutor-同步线程池\",\"contents\":[\"SyncTaskExecutor：同步可以用SyncTaskExecutor，但这个可以说不算一个线程池，因为还在原线程执行。这个类没有实现异步调用，只是一个同步操作。\",\"也可以用 ThreadPoolTaskExecutor 结合 FutureTask 做到同步。\",\"SyncTaskExecutor与ThreadPoolTaskExecutor区别，前者是同步执行器，执行任务同步，后者是线程池，执行任务异步。\"]},{\"header\":\"Spring 异步线程池实现原理\",\"slug\":\"spring-异步线程池实现原理\",\"contents\":[\"@EnableAsync 注解加入时，导入类AsyncConfigurationSelector，在容器种注册一个ProxyAsyncConfiguration，继承关系如下\",\"AbstractAsyncConfiguration 源码：\",\"@Configuration public abstract class AbstractAsyncConfiguration implements ImportAware { @Nullable protected AnnotationAttributes enableAsync; @Nullable protected Supplier<Executor> executor; @Nullable protected Supplier<AsyncUncaughtExceptionHandler> exceptionHandler; // 这里主要就是检查将其导入的类上是否有EnableAsync注解 // 如果没有的话就报错 @Override public void setImportMetadata(AnnotationMetadata importMetadata) { this.enableAsync = AnnotationAttributes.fromMap( importMetadata.getAnnotationAttributes(EnableAsync.class.getName(), false)); if (this.enableAsync == null) { throw new IllegalArgumentException( \\\"@EnableAsync is not present on importing class \\\" + importMetadata.getClassName()); } } // 将容器中配置的AsyncConfigurer注入 @Autowired(required = false) void setConfigurers(Collection<AsyncConfigurer> configurers) { if (CollectionUtils.isEmpty(configurers)) { return; } if (configurers.size() > 1) { throw new IllegalStateException(\\\"Only one AsyncConfigurer may exist\\\"); } AsyncConfigurer configurer = configurers.iterator().next(); // 异步执行嘛，所以我们可以配置使用的线程池 this.executor = configurer::getAsyncExecutor; // 另外也可以配置异常处理器 this.exceptionHandler = configurer::getAsyncUncaughtExceptionHandler; } } \",\"ProxyAsyncConfiguration 源码：\",\" @Configuration @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public class ProxyAsyncConfiguration extends AbstractAsyncConfiguration { @Bean(name = TaskManagementConfigUtils.ASYNC_ANNOTATION_PROCESSOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public AsyncAnnotationBeanPostProcessor asyncAdvisor() { AsyncAnnotationBeanPostProcessor bpp = new AsyncAnnotationBeanPostProcessor(); // 将通过AsyncConfigurer配置好的线程池跟异常处理器设置到这个后置处理器中 bpp.configure(this.executor, this.exceptionHandler); Class<? extends Annotation> customAsyncAnnotation = this.enableAsync.getClass(\\\"annotation\\\"); if (customAsyncAnnotation != AnnotationUtils.getDefaultValue(EnableAsync.class, \\\"annotation\\\")) { bpp.setAsyncAnnotationType(customAsyncAnnotation); } // bpp.setProxyTargetClass(this.enableAsync.getBoolean(\\\"proxyTargetClass\\\")); bpp.setOrder(this.enableAsync.<Integer>getNumber(\\\"order\\\")); return bpp; } } \",\"​ 这个类本身是一个配置类，它的作用是向容器中添加一个AsyncAnnotationBeanPostProcessor。到这一步我们基本上就可以明白了，@Async注解的就是通过AsyncAnnotationBeanPostProcessor这个后置处理器生成一个代理对象来实现异步的，接下来我们就具体看看AsyncAnnotationBeanPostProcessor是如何生成代理对象的，我们主要关注一下几点即可：\",\"是在生命周期的哪一步完成的代理？\",\"切点的逻辑是怎么样的？它会对什么样的类进行拦截？\",\"通知的逻辑是怎么样的？是如何实现异步的？\"]},{\"header\":\"是在生命周期的哪一步完成的代理？\",\"slug\":\"是在生命周期的哪一步完成的代理\",\"contents\":[\"​ 在这个后置处理器的postProcessAfterInitialization方法中完成了代理，直接定位到这个方法，这个方法位于父类AbstractAdvisingBeanPostProcessor中，具体代码如下：\",\"public Object postProcessAfterInitialization(Object bean, String beanName) { // 没有通知，或者是AOP的基础设施类，那么不进行代理 if (this.advisor == null || bean instanceof AopInfrastructureBean) { return bean; } // 对已经被代理的类，不再生成代理，只是将通知添加到代理类的逻辑中 // 这里通过beforeExistingAdvisors决定是将通知添加到所有通知之前还是添加到所有通知之后 // 在使用@Async注解的时候，beforeExistingAdvisors被设置成了true // 意味着整个方法及其拦截逻辑都会异步执行 if (bean instanceof Advised) { Advised advised = (Advised) bean; if (!advised.isFrozen() && isEligible(AopUtils.getTargetClass(bean))) { if (this.beforeExistingAdvisors) { advised.addAdvisor(0, this.advisor); } else { advised.addAdvisor(this.advisor); } return bean; } } // 判断需要对哪些Bean进行来代理 if (isEligible(bean, beanName)) { ProxyFactory proxyFactory = prepareProxyFactory(bean, beanName); if (!proxyFactory.isProxyTargetClass()) { evaluateProxyInterfaces(bean.getClass(), proxyFactory); } proxyFactory.addAdvisor(this.advisor); customizeProxyFactory(proxyFactory); return proxyFactory.getProxy(getProxyClassLoader()); } return bean; } \"]},{\"header\":\"接着我们就要思考，切点的过滤规则是什么呢？\",\"slug\":\"接着我们就要思考-切点的过滤规则是什么呢\",\"contents\":[\"类上添加了@Async注解或者类中含有被@Async注解修饰的方法。基于此，我们看看这个isEligible这个方法的实现逻辑，这个方位位于AbstractBeanFactoryAwareAdvisingPostProcessor中，也是AsyncAnnotationBeanPostProcessor的父类，对应代码如下：\",\"// AbstractBeanFactoryAwareAdvisingPostProcessor的isEligible方法 // 调用了父类 protected boolean isEligible(Object bean, String beanName) { return (!AutoProxyUtils.isOriginalInstance(beanName, bean.getClass()) && super.isEligible(bean, beanName)); } protected boolean isEligible(Object bean, String beanName) { return isEligible(bean.getClass()); } protected boolean isEligible(Class<?> targetClass) { Boolean eligible = this.eligibleBeans.get(targetClass); if (eligible != null) { return eligible; } if (this.advisor == null) { return false; } // 这里完成的判断 eligible = AopUtils.canApply(this.advisor, targetClass); this.eligibleBeans.put(targetClass, eligible); return eligible; } \",\"实际上最后就是根据advisor来确定是否要进行代理，在Spring中AOP相关的API及源码解析，原来AOP是这样子的这篇文章中我们提到过，advisor实际就是一个绑定了切点的通知，那么AsyncAnnotationBeanPostProcessor这个advisor是什么时候被初始化的呢？我们直接定位到AsyncAnnotationBeanPostProcessor的setBeanFactory方法，其源码如下：\",\"public void setBeanFactory(BeanFactory beanFactory) { super.setBeanFactory(beanFactory); // 在这里new了一个AsyncAnnotationAdvisor AsyncAnnotationAdvisor advisor = new AsyncAnnotationAdvisor(this.executor, this.exceptionHandler); if (this.asyncAnnotationType != null) { advisor.setAsyncAnnotationType(this.asyncAnnotationType); } advisor.setBeanFactory(beanFactory); // 完成了初始化 this.advisor = advisor; } // 我们来看看AsyncAnnotationAdvisor中的切点匹配规程是怎么样的，直接定位到这个类的buildPointcut方法中，其源码如下： protected Pointcut buildPointcut(Set<Class<? extends Annotation>> asyncAnnotationTypes) { ComposablePointcut result = null; for (Class<? extends Annotation> asyncAnnotationType : asyncAnnotationTypes) { // 就是根据这两个匹配器进行匹配的 Pointcut cpc = new AnnotationMatchingPointcut(asyncAnnotationType, true); Pointcut mpc = new AnnotationMatchingPointcut(null, asyncAnnotationType, true); if (result == null) { result = new ComposablePointcut(cpc); } else { result.union(cpc); } result = result.union(mpc); } return (result != null ? result : Pointcut.TRUE); } \",\"代码很简单，就是根据cpc跟mpc两个匹配器来进行匹配的，第一个是检查类上是否有@Async注解，第二个是检查方法是是否有@Async注解。\"]},{\"header\":\"通知的逻辑是怎么样的？是如何实现异步的？\",\"slug\":\"通知的逻辑是怎么样的-是如何实现异步的\",\"contents\":[\"前面也提到了advisor是一个绑定了切点的通知，前面分析了它的切点，那么现在我们就来看看它的通知逻辑，直接定位到AsyncAnnotationAdvisor中的buildAdvice方法，源码如下：\",\"protected Advice buildAdvice(@Nullable Supplier<Executor> executor, @Nullable Supplier<AsyncUncaughtExceptionHandler> exceptionHandler) { AnnotationAsyncExecutionInterceptor interceptor = new AnnotationAsyncExecutionInterceptor(null); interceptor.configure(executor, exceptionHandler); return interceptor; } \",\"简单吧，加了一个拦截器而已，对于interceptor类型的对象，我们关注它的核心方法invoke就行了，代码如下：\",\"public Object invoke(final MethodInvocation invocation) throws Throwable { Class<?> targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); Method specificMethod = ClassUtils.getMostSpecificMethod(invocation.getMethod(), targetClass); final Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); // 异步执行嘛，先获取到一个线程池 AsyncTaskExecutor executor = determineAsyncExecutor(userDeclaredMethod); if (executor == null) { throw new IllegalStateException( \\\"No executor specified and no default executor set on AsyncExecutionInterceptor either\\\"); } // 然后将这个方法封装成一个 Callable对象传入到线程池中执行 Callable<Object> task = () -> { try { Object result = invocation.proceed(); if (result instanceof Future) { return ((Future<?>) result).get(); } } catch (ExecutionException ex) { handleError(ex.getCause(), userDeclaredMethod, invocation.getArguments()); } catch (Throwable ex) { handleError(ex, userDeclaredMethod, invocation.getArguments()); } return null; }; // 将任务提交到线程池 return doSubmit(task, executor, invocation.getMethod().getReturnType()); } \"]},{\"header\":\"Spring 异步线程池\",\"slug\":\"spring-异步线程池\",\"contents\":[\"Spring 异步线程池配置\",\"@Configuration // @EnableAsync添加到配置文件或者Spring Boot 启动类上 @EnableAsync // 继承 AsyncConfigurer 接口 public class CustomAsyncConfigurer implements AsyncConfigurer { /** * 设置线程池相关的配置 * @return ThreadPoolTaskExecutor */ @Override public Executor getAsyncExecutor() { // 使用Spring Boot 异步线程池 ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); // 设置核心线程池大小(这里设置为初始化10个) executor.setMaxPoolSize(30); // 设置最大线程池大小(当核心线程池不够用时候，会自动在原基础上增加。最大为30个) executor.setQueueCapacity(2000); // 设置队列容量为2000个。 executor.initialize(); return executor; } } \",\"// @Async 加在类或者方法上声明这是个异步类或者异步方法 @Async public void testThreadPoolTaskExecutor(Long id) { System.err.println(\\\"--- testThreadPoolTaskExecutor --\\\" + id); } \"]},{\"header\":\"SimpleAsyncTaskExecutor\",\"slug\":\"simpleasynctaskexecutor\",\"contents\":[\"​ 异步执行用户任务的 SimpleAsyncTaskExecutor 。每次执行客户提交给它的任务时，它会启动新的线程，并允许开发者控制并发线程的上限（concurrencyLimit），从而起到一定的资源节流作用。默认时，concurrencyLimit 取值为 -1，即不启用资源节流。\",\"SimpleAsyncTaskExecutor实现源码：\",\"public class SimpleAsyncTaskExecutor extends CustomizableThreadCreator implements AsyncListenableTaskExecutor, Serializable { //限流主要实现 private final SimpleAsyncTaskExecutor.ConcurrencyThrottleAdapter concurrencyThrottle = new SimpleAsyncTaskExecutor.ConcurrencyThrottleAdapter(); private ThreadFactory threadFactory; //设置最大的线程数量 public void setConcurrencyLimit(int concurrencyLimit) { this.concurrencyThrottle.setConcurrencyLimit(concurrencyLimit); } //是否开启了限流 限流数量大于0？ public final boolean isThrottleActive() { return this.concurrencyThrottle.isThrottleActive(); } public void execute(Runnable task, long startTimeout) { Assert.notNull(task, \\\"Runnable must not be null\\\"); //1.是否开启限流 否则不开启限流处理 if(this.isThrottleActive() && startTimeout > 0L) { //2.执行开始之前检测是否可以满足要求 当前数量++ this.concurrencyThrottle.beforeAccess(); //3.开启限流将执行的Runable进行封装，执行完成调用final方法 当前数量-- this.doExecute(new SimpleAsyncTaskExecutor.ConcurrencyThrottlingRunnable(task)); } else { this.doExecute(task); } } //异步提交有返回值 public Future<?> submit(Runnable task) { FutureTask future = new FutureTask(task, (Object)null); this.execute(future, 9223372036854775807L); return future; } public <T> Future<T> submit(Callable<T> task) { FutureTask future = new FutureTask(task); this.execute(future, 9223372036854775807L); return future; } public ListenableFuture<?> submitListenable(Runnable task) { ListenableFutureTask future = new ListenableFutureTask(task, (Object)null); this.execute(future, 9223372036854775807L); return future; } public <T> ListenableFuture<T> submitListenable(Callable<T> task) { ListenableFutureTask future = new ListenableFutureTask(task); this.execute(future, 9223372036854775807L); return future; } protected void doExecute(Runnable task) { //拥有工厂？没有的话调用父类可以设置各种参数的创建线程 Thread thread = this.threadFactory != null?this.threadFactory.newThread(task):this.createThread(task); thread.start(); } //父类的方法，方便配置线程，方便xml设置线程参数CustomizableThreadCreator public Thread createThread(Runnable runnable) { Thread thread = new Thread(getThreadGroup(), runnable, nextThreadName()); thread.setPriority(getThreadPriority()); thread.setDaemon(isDaemon()); return thread; } } \"]},{\"header\":\"ThreadPoolTaskExecutor\",\"slug\":\"threadpooltaskexecutor\",\"contents\":[\"ThreadPoolTaskExecutor 拥有强大的功能，相比 SimpleAsyncTaskExecutor， 提供了线程复用，不用每一次都新启一个线程，效率更高\"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"[Java线程池的使用 - 简书 (jianshu.com)https://www.cnblogs.com/duanxz/p/9435343.html)\",\"Spring自带的线程池ThreadPoolTaskExecutor - 知乎 (zhihu.com)\",\"spring任务执行器与任务调度器(TaskExecutor And TaskScheduler) - 简书 (jianshu.com)\",\"(4条消息) @Async的使用、原理及使用时可能导致的问题_程序猿DD-CSDN博客\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatch.html\":{\"title\":\"Spring Batch Java 批处理框架\",\"contents\":[{\"header\":\"Spring Batch 概念\",\"slug\":\"spring-batch-概念\",\"contents\":[]},{\"header\":\"批处理的核心场景\",\"slug\":\"批处理的核心场景\",\"contents\":[\"从某个位置读取大量的记录，位置可以是数据库、文件或者外部推送队列（MQ）。\",\"根据业务需要实时处理读取的数据。\",\"将处理后的数据写入某个位置，可以第一条一样，可以是数据库、文件或者推送到队列。\"]},{\"header\":\"Spring Batch能解决的批处理场景\",\"slug\":\"spring-batch能解决的批处理场景\",\"contents\":[\"​ Spring Batch为批处理提供了一个轻量化的解决方案，它根据批处理的需要迭代处理各种记录，提供事物功能。但是Spring Batch仅仅适用于\\\"脱机\\\"场景，在处理的过程中不能和外部进行任何交互，也不允许有任何输入。\"]},{\"header\":\"Spring Batch的目标\",\"slug\":\"spring-batch的目标\",\"contents\":[\"开发人员仅关注业务逻辑，底层框架的交互交由Spring Batch去处理。\",\"能够清晰分离业务与框架，框架已经限定了批处理的业务切入点，业务开发只需关注这些切入点（Read、Process、Write）。\",\"提供开箱即用的通用接口。\",\"快速轻松的融入Spring 框架，基于Spring Framework能够快速扩展各种功能。\",\"所有现有核心服务都应易于更换或扩展，而不会对基础架构层产生任何影响。\"]},{\"header\":\"Spring Batch结构\",\"slug\":\"spring-batch结构\",\"contents\":[\"​ 如上图，通常情况下一个独立的JVM程序就是仅仅用于处理批处理，而不要和其他功能重叠。 在最后一层基础设置（Infrastructure）部分主要分为3个部分。JobLauncher、Job以及Step。每一个Step又细分为ItemReader、ItemProcessor、ItemWirte。使用Spring Batch主要就是知道每一个基础设置负责的内容，然后在对应的设施中实现对应的业务。\"]},{\"header\":\"Spring Batch 批处理原则与建议\",\"slug\":\"spring-batch-批处理原则与建议\",\"contents\":[\"当我们构建一个批处理的过程时，必须注意以下原则：\",\"通常情况下，批处理的过程对系统和架构的设计要够要求比较高，因此尽可能的使用通用架构来处理批量数据处理，降低问题发生的可能性。Spring Batch是一个是一个轻量级的框架，适用于处理一些灵活并没有到海量的数据。\",\"批处理应该尽可能的简单，尽量避免在单个批处理中去执行过于复杂的任务。我们可以将任务分成多个批处理或者多个步骤去实现。\",\"保证数据处理和物理数据紧密相连。笼统的说就是我们在处理数据的过程中有很多步骤，在某些步骤执行完时应该就写入数据，而不是等所有都处理完。\",\"尽可能减少系统资源的使用、尤其是耗费大量资源的IO以及跨服务器引用，尽量分配好数据处理的批量。\",\"定期分析系统的IO使用情况、SQL语句的执行情况等，尽可能的减少不必要的IO操作。优化的原则有： \",\"尽量在一次事物中对同一数据进行读取或写缓存。\",\"一次事物中，尽可能在开始就读取所有需要使用的数据。\",\"优化索引，观察SQL的执行情况，尽量使用主键索引，尽量避免全表扫描或过多的索引扫描。\",\"不要在批处理中对相同的数据执行2次相同的操作。\",\"对于批处理程序而言应该在批处理启动之前就分配足够的内存，以免处理的过程中去重新申请新的内存页。\",\"对数据的完整性应该从最差的角度来考虑，每一步的处理都应该建立完备的数据校验。\",\"对于数据的总量我们应该有一个和数据记录在数据结构的某个字段上。\",\"所有的批处理系统都需要进行压力测试。\",\"如果整个批处理的过程是基于文件系统，在处理的过程中请切记完成文件的备份以及文件内容的校验。\"]},{\"header\":\"批处理的通用策略\",\"slug\":\"批处理的通用策略\",\"contents\":[\"和软件开发的设计模式一样，批处理也有各种各样的现成模式可供参考。当一个开发（设计）人员开始执行批处理任务时，应该将业务逻辑拆分为一下的步骤或者板块分批执行：\",\"数据转换：某个（某些）批处理的外部数据可能来自不同的外部系统或者外部提供者，这些数据的结构千差万别。在统一进行批量数据处理之前需要对这些数据进行转换，合并为一个统一的结构。因此在数据开始真正的执行业务处理之前，可以先搭建批处理任务将这些数据统一转换。\",\"数据校验：批处理是对大量数据进行处理，并且数据的来源千差万别，所以批处理的输入数据需要对数据的完整性性进行校验（比如校验字段数据是否缺失）。另外批处理输出的数据也需要进行合适的校验（例如处理了100条数据，校验100条数据是否校验成功）\",\"提取数据：批处理的工作是逐条从数据库或目标文件读取记录（records），提取时可以通过一些规则从数据源中进行数据筛选。\",\"数据实时更新处理：根据业务要求，对实时数据进行处理。某些时候一行数据记录的处理需要绑定在一个事物之下。\",\"输出记录到标准的文档格式：数据处理完成之后需要根据格式写入到对应的外部数据系统中。\",\"以上五个步骤是一个标准的数据批处理过程，Spring batch框架为业务实现提供了以上几个功能入口。\"]},{\"header\":\"数据额外处理\",\"slug\":\"数据额外处理\",\"contents\":[\"某些情况需要实现对数据进行额外处理，在进入批处理之前通过其他方式将数据进行处理。主要内容有：\",\"排序：由于批处理是以独立的行数据（record）进行处理的，在处理的时候并不知道记录前后关系。因此如果需要对整体数据进行排序，最好事先使用其他方式完成。\",\"分割：数据拆分也建议使用独立的任务来完成。理由类似排序，因为批处理的过程都是以行记录为基本处理单位的，无法再对分割之后的数据进行扩展处理。\",\"合并：理由如上。\"]},{\"header\":\"常规数据源\",\"slug\":\"常规数据源\",\"contents\":[\"批处理的数据源通常包括：\",\"数据库驱动链接（链接到数据库）对数据进行逐条提取。\",\"文件驱动链接，对文件数据进行提取\",\"消息驱动链接，从MQ、kafka等消息系统提取数据。\"]},{\"header\":\"典型的处理过程\",\"slug\":\"典型的处理过程\",\"contents\":[\"在业务停止的窗口期进行批数据处理，例如银行对账、清结算都是在12点日切到黎明之间。简称为离线处理。\",\"在线或并发批处理，但是需要对实际业务或用户的响应进行考量。\",\"并行处理多种不同的批处理作业。\",\"分区处理：将相同的数据分为不同的区块，然后按照相同的步骤分为许多独立的批处理任务对不同的区块进行处理。\",\"以上处理过程进行组合。\",\"在执行2,3点批处理时需要注意事物隔离等级。\"]},{\"header\":\"Spring Batch批处理的核心概念\",\"slug\":\"spring-batch批处理的核心概念\",\"contents\":[\"下图是批处理的核心流程图。\",\"​ Spring Batch同样按照批处理的标准实现了各个层级的组件。并且在框架级别保证数据的完整性和事物性。\",\"​ 如图所示，在一个标准的批处理任务中组要涵盖的核心概念有JobLauncher、Job、Step，一个Job可以涵盖多个Step，一个Job对应一个启动的JobLauncher。一个Step中分为ItemReader、ItemProcessor、ItemWriter，根据字面意思它们分别对应数据提取、数据处理和数据写入。此外JobLauncher、Job、Step也称之为批处理的元数据（Metadata），它们会被存储到JobRepository中。\"]},{\"header\":\"Job\",\"slug\":\"job\",\"contents\":[\"简单的说Job是封装一个批处理过程的实体，与其他的Spring项目类似，Job可以通过XML或Java类配置，称职为”Job Configuration“.如下图Job是单个批处理的最顶层。\",\"为了便于理解，可以建立的理解为Job就是每一步（Step）实例的容器。他结合了多个Step，为它们提供统一的服务同时也为Step提供个性化的服务，比如步骤重启。通常情况下Job的配置包含以下内容：\",\"Job的名称\",\"定义和排序Step执行实例。\",\"标记每个Step是否可以重启。\",\"Spring Batch为Job接口提供了默认的实现——SimpleJob类，在类中实现了一些标准的批处理方法。下面的代码展示了如可申明一个Job (在Config类中)。\",\"@Bean public Job footballJob() { return this.jobBuilderFactory //get中命名了Job的名称 .get(\\\"footballJob\\\") //playerLoad、gameLoad、playerSummarization都是Step .start(playerLoad()) .next(gameLoad()) .next(playerSummarization()) .end() .build(); } \"]},{\"header\":\"JobInstance\",\"slug\":\"jobinstance\",\"contents\":[\"JobInstance是指批处理作业运行的实例。例如一个批处理必须在每天执行一次，系统在2019年5月1日执行了一次我们称之为2019-05-01的实例，类似的还会有2019-05-02、2019-05-03实例。在特定的运行实践中，一个Job只有一个JobInstance以及对应的JobParameters ，但是可以有多个JobExecution 。（JobParameters 、JobExecution 见后文）。同一个JobInstance 具有相同的上下文（ExecutionContext 内容见后文）。\"]},{\"header\":\"JobParameters\",\"slug\":\"jobparameters\",\"contents\":[\"前面讨论了JobInstance 与Job 的区别，但是具体的区别内容都是通过JobParameters 体现的。一个JobParameters 对象中包含了一系列Job运行相关的参数，这些参数可以用于参考或者用于实际的业务使用。对应的关系如下图：\",\"当我们执行2个不同的JobInstance时JobParameters中的属性都会有差异。可以简单的认为一个JobInstance的标识就是 Job + JobParameters 。\"]},{\"header\":\"JobExecution\",\"slug\":\"jobexecution\",\"contents\":[\"​ JobExecution 可以理解为单次运行Job的容器。一次JobInstance执行的结果可能是成功、也可能是失败。但是对于Spring Batch框架而言，只有返回运行成功才会视为完成一次批处理。例如2019-05-01执行了一次JobInstance，但是执行的过程失败，因此第二次还会有一个“相同的”的 JobInstance 被执行。\",\"​ Job 可以定义批处理如何执行，JobInstance 纯粹的就是一个处理对象，把所有的内容、对象组织在一起，主要是为了当面临问题时定义正确的重启参数。而JobExecution是运行时的“容器”，记录动态运行时的各种属性和上线文，主要有一下内容：\",\"属性\",\"说明\",\"status\",\"状态类名为BatchStatus，它指示了执行的状态。在执行的过程中状态为BatchStatus#STARTED，失败：BatchStatus#FAILED，完成：BatchStatus#COMPLETED\",\"startTime\",\"java.util.Date对象，标记批处理任务启动的系统时间，批处理任务未启动数据为空\",\"endTime\",\"java.util.Date对象，结束时间无论是否成功都包含该数据，如未处理完为空\",\"exitStatus\",\"ExitStatus类，记录运行结果。\",\"createTime\",\"java.util.Date, JobExecution的创建时间，某些使用execution已经创建但是并未开始运行。\",\"lastUpdate\",\"java.util.Date，最后一次更新时间\",\"executionContext\",\"批处理任务执行的所有用户数据\",\"failureExceptions\",\"记录在执行Job时的异常，对于排查问题非常有用\",\"对应的每次执行的结果会在元数据库中体现为：\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_EXECUTION_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"IDENTIFYING\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"BATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2019-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"​\",\"​ 当某个Job批处理任务失败之后会在对应的数据库表中路对应的状态。假设1月1号执行的任务失败，技术团队花费了大量的时间解决这个问题到了第二天21才继续执行这个任务。\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"2\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_EXECUTION_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"IDENTIFYING\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"2\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"3\",\"DATE\",\"schedule.Date\",\"2019-01-02\",\"TRUE\",\"BATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2019-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"2\",\"1\",\"2019-01-02 21:00\",\"2017-01-02 21:30\",\"COMPLETED\",\"3\",\"2\",\"2019-01-02 21:31\",\"2017-01-02 22:29\",\"COMPLETED\",\"从数据上看好似JobInstance是一个接一个顺序执行的，但是对于Spring Batch并没有进行任何控制。不同的JobInstance很有可能是同时在运行（相同的JobInstance同时运行会抛出JobExecutionAlreadyRunningException异常）。\"]},{\"header\":\"Step\",\"slug\":\"step\",\"contents\":[\"​ Step是批处理重复运行的最小单元，它按照顺序定义了一次执行的必要过程。因此每个Job可以视作由一个或多个多个Step组成。一个Step包含了所有所有进行批处理的必要信息，这些信息的内容是由开发人员决定的并没有统一的标准。一个Step可以很简单，也可以很复杂。他可以是复杂业务的组合，也有可能仅仅用于迁移数据。与JobExecution的概念类似，Step也有特定的StepExecution，关系结构如下：\"]},{\"header\":\"StepExecution\",\"slug\":\"stepexecution\",\"contents\":[\"​ StepExecution表示单次执行Step的容器，每次Step执行时都会有一个新的StepExecution被创建。与JobExecution不同的是，当某个Step执行失败后并不会再次尝试重新执行该Step。StepExecution包含以下属性：\",\"属性\",\"说明\",\"status\",\"状态类名为BatchStatus，它指示了执行的状态。在执行的过程中状态为BatchStatus#STARTED，失败：BatchStatus#FAILED，完成：BatchStatus#COMPLETED\",\"startTime\",\"java.util.Date对象，标记StepExecution启动的系统时间，未启动数据为空\",\"endTime\",\"java.util.Date对象，结束时间，无论是否成功都包含该数据，如未处理完为空\",\"exitStatus\",\"ExitStatus类，记录运行结果。\",\"createTime\",\"java.util.Date,JobExecution的创建时间，某些使用execution已经创建但是并未开始运行。\",\"lastUpdate\",\"java.util.Date，最后一次更新时间\",\"executionContext\",\"批处理任务执行的所有用户数据\",\"readCount\",\"成功读取数据的次数\",\"wirteCount\",\"成功写入数据的次数\",\"commitCount\",\"成功提交数据的次数\",\"rollbackCount\",\"回归数据的次数，有业务代码触发\",\"readSkipCount\",\"当读数据发生错误时跳过处理的次数\",\"processSkipCount\",\"当处理过程发生错误，跳过处理的次数\",\"filterCount\",\"被过滤规则拦截未处理的次数\",\"writeSkipCount\",\"写数据失败，跳过处理的次数\"]},{\"header\":\"ExecutionContext\",\"slug\":\"executioncontext\",\"contents\":[\"前文已经多次提到ExecutionContext。可以简单的认为ExecutionContext提供了一个Key/Value机制，在StepExecution和JobExecution对象的任何位置都可以获取到ExecutionContext中的任何数据。最有价值的作用是记录数据的执行位置，以便发生重启时候从对应的位置继续执行：\",\"executionContext.putLong(getKey(LINES_READ_COUNT), reader.getPosition()) \",\"比如在任务中有一个名为“loadData”的Step，他的作用是从文件中读取数据写入到数据库，当第一次执行失败后，数据库中有如下数据：\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_INST_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"ATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2017-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"BATCH_STEP_EXECUTION：\",\"STEP_EXEC_ID\",\"JOB_EXEC_ID\",\"STEP_NAME\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"loadData\",\"2017-01-01 21:00\",\"2017-01-01 21:30\",\"BATCH_STEP_EXECUTION_CONTEXT：\",\"STEP_EXEC_ID\",\"SHORT_CONTEXT\",\"1\",\"{piece.count=40321}\",\"​ 在上面的例子中，Step 运行30分钟处理了40321个“pieces”，我们姑且认为“pieces”表示行间的行数（实际就是每个Step完成循环处理的个数）。这个值会在每个commit之前被更新记录在ExecutionContext中（更新需要用到StepListener后文会详细说明）。当我们再次重启这个Job时并记录在BATCH_STEP_EXECUTION_CONTEXT中的数据会加载到ExecutionContext中,这样当我们继续执行批处理任务时可以从上一次中断的位置继续处理。例如下面的代码在ItemReader中检查上次执行的结果，并从中断的位置继续执行：\",\"if (executionContext.containsKey(getKey(LINES_READ_COUNT))) { log.debug(\\\"Initializing for restart. Restart data is: \\\" + executionContext); long lineCount = executionContext.getLong(getKey(LINES_READ_COUNT)); LineReader reader = getReader(); Object record = \\\"\\\"; while (reader.getPosition() < lineCount && record != null) { record = readLine(); } } \",\"ExecutionContext是根据JobInstance进行管理的，因此只要是相同的实例都会具备相同的ExecutionContext（无论是否停止）。此外通过以下方法都可以获得一个ExecutionContext：\",\"ExecutionContext ecStep = stepExecution.getExecutionContext(); ExecutionContext ecJob = jobExecution.getExecutionContext(); \",\"但是这2个ExecutionContext并不相同，前者是在一个Step中每次Commit数据之间共享，后者是在Step与Step之间共享。\"]},{\"header\":\"JobRepository\",\"slug\":\"jobrepository\",\"contents\":[\"​ JobRepository是所有前面介绍的对象实例的持久化机制。他为JobLauncher、Job、Step的实现提供了CRUD操作。当一个Job第一次被启动时，一个JobExecution会从数据源中获取到，同时在执行的过程中StepExecution、JobExecution的实现都会记录到数据源中。挡在程序启动时使用@EnableBatchProcessing注解，JobRepository会进行自动化配置。\"]},{\"header\":\"JobLauncher\",\"slug\":\"joblauncher\",\"contents\":[\"JobLauncher为Job的启动运行提供了一个边界的入口，在启动Job的同时还可以定制JobParameters：\",\"public interface JobLauncher { public JobExecution run(Job job, JobParameters jobParameters) throws JobExecutionAlreadyRunningException, JobRestartException,JobInstanceAlreadyCompleteException,JobParametersInvalidException; } \",\"![cm9c3yeke6](img\\\\Spring Batch\\\\cm9c3yeke6.png)\"]},{\"header\":\"Spring Batch——Step控制\",\"slug\":\"spring-batch——step控制\",\"contents\":[]},{\"header\":\"Spring Batch——Job配置与运行\",\"slug\":\"spring-batch——job配置与运行\",\"contents\":[]},{\"header\":\"Spring Batch——Item概念及使用代码\",\"slug\":\"spring-batch——item概念及使用代码\",\"contents\":[\"在上文中介绍一个标准的批处理分为 Job 和 Step 。本文将结合代码介绍在Step中Reader、Processor、Writer的实际使用。\"]},{\"header\":\"Reader\",\"slug\":\"reader\",\"contents\":[\"Reader是指从各种各样的外部输入中获取数据，框架为获取各种类型的文件已经预定义了常规的Reader实现类。Reader通过ItemReader接口实现：\",\"public interface ItemReader<T> { T read() throws Exception, UnexpectedInputException, ParseException, NonTransientResourceException; } \",\"read方法的作用就是读取一条数据，数据以泛型T的实体结构返回，当read返回null时表示所有数据读取完毕。返回的数据可以是任何结构，比如文件中的一行字符串，数据库的一行数据，或者xml文件中的一系列元素，只要是一个Java对象即可。\"]},{\"header\":\"Writer\",\"slug\":\"writer\",\"contents\":[\"Writer通过ItemWriter接口实现：\",\"public interface ItemWriter<T> { void write(List<? extends T> items) throws Exception; } \",\"Writer是Reader的反向操作，是将数据写入到特定的数据源中。在上文已经介绍Writer是根据chunk属性设定的值按列表进行操作的，所以传入的是一个List结构。chunk用于表示批处理的事物分片，因此需要注意的是，在writer方法中进行完整数据写入事物操作。例如向数据库写入List中的数据，在写入完成之后再提交事物。\"]},{\"header\":\"读写的组合模式\",\"slug\":\"读写的组合模式\",\"contents\":[\"无论是读还是写，有时会需要从多个不同的来源获取文件，或者写入到不同的数据源，或者是需要在读和写之间处理一些业务。可以使用组合模式来实现这个目的：\",\"public class CompositeItemWriter<T> implements ItemWriter<T> { ItemWriter<T> itemWriter; public CompositeItemWriter(ItemWriter<T> itemWriter) { this.itemWriter = itemWriter; } public void write(List<? extends T> items) throws Exception { //Add business logic here itemWriter.write(items); } public void setDelegate(ItemWriter<T> itemWriter){ this.itemWriter = itemWriter; } } \"]},{\"header\":\"Processor\",\"slug\":\"processor\",\"contents\":[\"除了使用组合模式，直接使用Processor是一种更优雅的方法。Processor是Step中的可选项，但是批处理大部分时候都需要对数据进行处理，因此框架提供了ItemProcessor接口来满足 Processor 过程：\",\"public interface ItemProcessor<I, O> { O process(I item) throws Exception; } \",\"Processor的结构非常简单也是否易于理解。传入一个类型<I>，然后由Processor处理成为<O>。\"]},{\"header\":\"Processor链\",\"slug\":\"processor链\",\"contents\":[\"在一个Step中可以使用多个Processor来按照顺序处理业务，此时同样可以使用CompositeItem模式来实现：\",\"@Bean public CompositeItemProcessor compositeProcessor() { //创建 CompositeItemProcessor CompositeItemProcessor<Foo,Foobar> compositeProcessor = new CompositeItemProcessor<Foo,Foobar>(); List itemProcessors = new ArrayList(); //添加第一个 Processor itemProcessors.add(new FooTransformer()); //添加第二个 Processor itemProcessors.add(new BarTransformer()); //添加链表 compositeProcessor.setDelegates(itemProcessors); return processor; } \"]},{\"header\":\"过滤记录\",\"slug\":\"过滤记录\",\"contents\":[\"​ 在Reader读取数据的过程中，并不是所有的数据都可以使用，此时Processor还可以用于过滤非必要的数据，同时不会影响Step的处理过程。只要ItemProcesspr的实现类在procss方法中返回null即表示改行数据被过滤掉了。\"]},{\"header\":\"ItemStream\",\"slug\":\"itemstream\",\"contents\":[\"​ 在上文中已经提到了ItemStream。Spring Batch的每一步都是无状态的，进而Reader和Writer也是无状态的，这种方式能够很好的隔离每行数据的处理，也能将容错的范围收窄到可以空子的范围。但是这并不意味着整个批处理的过程中并不需要控制状态。例如从数据库持续读入或写入数据，每次Reader和Writer都单独去申请数据源的链接、维护数据源的状态（打开、关闭等）。因此框架提供了ItemStream接口来完善这些操作：\",\"public interface ItemStream { void open(ExecutionContext executionContext) throws ItemStreamException; void update(ExecutionContext executionContext) throws ItemStreamException; void close() throws ItemStreamException; } \"]},{\"header\":\"持久化数据\",\"slug\":\"持久化数据\",\"contents\":[\"​ 在使用Spring Batch之前需要初始化他的元数据存储（Meta-Data Schema）,也就是要将需要用到的表导入到对应的数据库中。当然，Spring Batch支持不使用任何持久化数据库，仅仅将数据放到内存中，不设置DataSource即可。\"]},{\"header\":\"初始化序列\",\"slug\":\"初始化序列\",\"contents\":[\"Spring Batch相关的工作需要使用序列SEQUENCE：\",\"CREATE SEQUENCE BATCH_STEP_EXECUTION_SEQ; CREATE SEQUENCE BATCH_JOB_EXECUTION_SEQ; CREATE SEQUENCE BATCH_JOB_SEQ; \",\"有些数据库不支持SEQUENCE，可以通过表代理，比如在MySql（InnoDB数据库）中：\",\"CREATE TABLE BATCH_STEP_EXECUTION_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_STEP_EXECUTION_SEQ values(0); CREATE TABLE BATCH_JOB_EXECUTION_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_JOB_EXECUTION_SEQ values(0); CREATE TABLE BATCH_JOB_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_JOB_SEQ values(0); \"]},{\"header\":\"关于Version字段\",\"slug\":\"关于version字段\",\"contents\":[\"某些表中都有Version字段。因为Spring的更新策略是乐观锁，因此在进行数据更新之后都会对表的Version字段进行+1处理。在内存与数据库交互的过程中，会使用采用getVersion、increaseVersion（+1）、updateDataAndVersion的过程，如果在update的时候发现Version不是预计的数值（+1），则会抛出OptimisticLockingFailureException的异常。当同一个Job在进群中不同服务上执行时，需要注意这个问题。\"]},{\"header\":\"BATCH_JOB_INSTANCE\",\"slug\":\"batch-job-instance\",\"contents\":[\"BATCH_JOB_INSTANCE用于记录JobInstance，在数据批处理概念中介绍了他的工作方式，其结构为：\",\"CREATE TABLE BATCH_JOB_INSTANCE ( JOB_INSTANCE_ID BIGINT PRIMARY KEY , VERSION BIGINT, JOB_NAME VARCHAR(100) NOT NULL , JOB_KEY VARCHAR(2500) ); \",\"字段\",\"说明\",\"JOB_INSTANCE_ID\",\"主键，主键与单个JobInstance相关。当获取到某个JobInstance实例后，通过getId方法可以获取到此数据\",\"VERSION\",\"JOB_NAME\",\"Job的名称，用于标记运行的Job，在创建Job时候指定\",\"JOB_KEY\",\"JobParameters的序列化数值。在数据批处理概念中介绍了一个JobInstance相当于Job+JobParameters。他用于标记同一个Job不同的实例\"]},{\"header\":\"BATCH_JOB_EXECUTION_PARAMS\",\"slug\":\"batch-job-execution-params\",\"contents\":[\"BATCH_JOB_EXECUTION_PARAMS对应的是JobParameters对象。其核心功能是存储Key-Value结构的各种状态数值。字段中IDENTIFYING=true用于标记那些运行过程中必须的数据（可以理解是框架需要用到的数据），为了存储key-value结构该表一个列数据格式：\",\"CREATE TABLE BATCH_JOB_EXECUTION_PARAMS ( JOB_EXECUTION_ID BIGINT NOT NULL , TYPE_CD VARCHAR(6) NOT NULL , KEY_NAME VARCHAR(100) NOT NULL , STRING_VAL VARCHAR(250) , DATE_VAL DATETIME DEFAULT NULL , LONG_VAL BIGINT , DOUBLE_VAL DOUBLE PRECISION , IDENTIFYING CHAR(1) NOT NULL , constraint JOB_EXEC_PARAMS_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ); \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"与BATCH_JOB_EXECUTION表关联的外键，详见数据批处理概念中Job、JobInstance、JobExecute的关系\",\"TYPE_CD\",\"用于标记数据的对象类型，例如 string、date、long、double，非空\",\"KEY_NAME\",\"key的值\",\"STRING_VAL\",\"string类型的数值\",\"DATE_VAL\",\"date类型的数值\",\"LONG_VAL\",\"long类型的数值\",\"DOUBLE_VAL\",\"double类型的数值\",\"IDENTIFYING\",\"标记这对key-valuse是否来自于JobInstace自身\"]},{\"header\":\"BATCH_JOB_EXECUTION\",\"slug\":\"batch-job-execution\",\"contents\":[\"关联JobExecution，每当运行一个Job都会产生一个新的JobExecution，对应的在表中都会新增一行数据。\",\"CREATE TABLE BATCH_JOB_EXECUTION ( JOB_EXECUTION_ID BIGINT PRIMARY KEY , VERSION BIGINT, JOB_INSTANCE_ID BIGINT NOT NULL, CREATE_TIME TIMESTAMP NOT NULL, START_TIME TIMESTAMP DEFAULT NULL, END_TIME TIMESTAMP DEFAULT NULL, STATUS VARCHAR(10), EXIT_CODE VARCHAR(20), EXIT_MESSAGE VARCHAR(2500), LAST_UPDATED TIMESTAMP, JOB_CONFIGURATION_LOCATION VARCHAR(2500) NULL, constraint JOB_INSTANCE_EXECUTION_FK foreign key (JOB_INSTANCE_ID) references BATCH_JOB_INSTANCE(JOB_INSTANCE_ID) ) ; \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"JobExecution的主键，JobExecution::getId方法可以获取到该值\",\"VERSION\",\"JOB_INSTANCE_ID\",\"关联到JobInstace的外键，详见数据批处理概念中Job、JobInstance、JobExecute的关系\",\"CREATE_TIME\",\"创建时间戳\",\"START_TIME\",\"开始时间戳\",\"END_TIME\",\"结束时间戳，无论成功或失败都会更新这一项数据。如果某行数据该值为空表示运行期间出现错误，并且框架无法更新该值\",\"STATUS\",\"JobExecute的运行状态:COMPLETED、STARTED或者其他状态。此数值对应Java中BatchStatus枚举值\",\"EXIT_CODE\",\"JobExecute执行完毕之后的退出返回值\",\"EXIT_MESSAGE\",\"JobExecute退出的详细内容，如果是异常退出可能会包括异常堆栈的内容\",\"LAST_UPDATED\",\"最后一次更新的时间戳\"]},{\"header\":\"BATCH_STEP_EXECUTION\",\"slug\":\"batch-step-execution\",\"contents\":[\"该表对应的是StepExecution，其结构和BATCH_JOB_EXECUTION基本相似，只是对应的对象是Step，增加了与之相对的一些字段数值：\",\"CREATE TABLE BATCH_STEP_EXECUTION ( STEP_EXECUTION_ID BIGINT PRIMARY KEY , VERSION BIGINT NOT NULL, STEP_NAME VARCHAR(100) NOT NULL, JOB_EXECUTION_ID BIGINT NOT NULL, START_TIME TIMESTAMP NOT NULL , END_TIME TIMESTAMP DEFAULT NULL, STATUS VARCHAR(10), COMMIT_COUNT BIGINT , READ_COUNT BIGINT , FILTER_COUNT BIGINT , WRITE_COUNT BIGINT , READ_SKIP_COUNT BIGINT , WRITE_SKIP_COUNT BIGINT , PROCESS_SKIP_COUNT BIGINT , ROLLBACK_COUNT BIGINT , EXIT_CODE VARCHAR(20) , EXIT_MESSAGE VARCHAR(2500) , LAST_UPDATED TIMESTAMP, constraint JOB_EXECUTION_STEP_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ) ; \",\"未填入内容部分见BATCH_JOB_EXECUTION说明。\",\"字段\",\"说明\",\"STEP_EXECUTION_ID\",\"StepExecute对应的主键\",\"VERSION\",\"STEP_NAME\",\"Step名称\",\"JOB_EXECUTION_ID\",\"关联到BATCH_JOB_EXECUTION表的外键，标记该StepExecute所属的JobExecute\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"COMMIT_COUNT\",\"执行过程中，事物提交的次数，该值与数据的规模以及chunk的设置有关\",\"READ_COUNT\",\"读取数据的次数\",\"FILTER_COUNT\",\"Processor中过滤记录的次数\",\"WRITE_COUNT\",\"吸入数据的次数\",\"READ_SKIP_COUNT\",\"读数据的跳过次数\",\"WRITE_SKIP_COUNT\",\"写数据的跳过次数\",\"PROCESS_SKIP_COUNT\",\"Processor跳过的次数\",\"ROLLBACK_COUNT\",\"回滚的次数\",\"EXIT_CODE\",\"EXIT_MESSAGE\",\"LAST_UPDATED\"]},{\"header\":\"BATCH_JOB_EXECUTION_CONTEXT\",\"slug\":\"batch-job-execution-context\",\"contents\":[\"该表会记录所有与Job相关的ExecutionContext信息。每个ExecutionContext都对应一个JobExecution，在运行的过程中它包含了所有Job范畴的状态数据，这些数据在执行失败后对于后续处理有中重大意义。\",\"CREATE TABLE BATCH_JOB_EXECUTION_CONTEXT ( JOB_EXECUTION_ID BIGINT PRIMARY KEY, SHORT_CONTEXT VARCHAR(2500) NOT NULL, SERIALIZED_CONTEXT CLOB, constraint JOB_EXEC_CTX_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ) ; \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"关联到JobExecution的外键，建立JobExecution和ExecutionContext的关系。\",\"SHORT_CONTEXT\",\"标记SERIALIZED_CONTEXT的版本号\",\"SERIALIZED_CONTEXT\",\"序列化的ExecutionContext\"]},{\"header\":\"BATCH_STEP_EXECUTION_CONTEXT\",\"slug\":\"batch-step-execution-context\",\"contents\":[\"Step中ExecutionContext相关的数据表，结构与BATCH_JOB_EXECUTION_CONTEXT完全一样。\"]},{\"header\":\"表索引建议\",\"slug\":\"表索引建议\",\"contents\":[\"上面的所有建表语句都没有提供索引，但是并不代表索引没有价值。当感觉到SQL语句的执行有效率问题时候，可以增加索引。\",\"索引带来的价值取决于SQL查询的频率以及关联关系，下面是Spring Batch框架在运行过程中会用到的一些查询条件语句，用于参考优化索引：\",\"表\",\"Where条件\",\"执行频率\",\"BATCH_JOB_INSTANCE\",\"JOB_NAME = ? and JOB_KEY = ?\",\"每次Job启动执时\",\"BATCH_JOB_EXECUTION\",\"JOB_INSTANCE_ID = ?\",\"每次Job重启时\",\"BATCH_EXECUTION_CONTEXT\",\"EXECUTION_ID = ? and KEY_NAME = ?\",\"视chunk的大小而定\",\"BATCH_STEP_EXECUTION\",\"VERSION = ?\",\"视chunk的大小而定\",\"BATCH_STEP_EXECUTION\",\"STEP_NAME = ? and JOB_EXECUTION_ID = ?\",\"每一个Step执行之前\"]},{\"header\":\"Spring Batch——文件读写\",\"slug\":\"spring-batch——文件读写\",\"contents\":[\"在上文中Job、Step都是属于框架级别的的功能，大部分时候都是提供一些配置选项给开发人员使用，而Item中的Reader、Processor和Writer是属于业务级别的，它开放了一些业务切入的接口。 但是文件的读写过程中有很多通用一致的功能Spring Batch为这些相同的功能提供了一致性实现类。\"]},{\"header\":\"扁平结构文件\",\"slug\":\"扁平结构文件\",\"contents\":[\"扁平结构文件（也称为矩阵结构文件，后文简称为文件）是最常见的一种文件类型。他通常以一行表示一条记录，字段数据之间用某种方式分割。与标准的格式数据（xml、json等）主要差别在于他没有结构性描述方案（SXD、JSON-SCHEME），进而没有结构性分割规范。因此在读写此类文件之前需要先设定好字段的分割方法。\",\"文件的字段数据分割方式通常有两种：使用分隔符或固定字段长度。前者通常使用逗号（，）之类的符号对字段数据进行划分，后者的每一列字段数据长度是固定的。 框架为文件的读取提供了FieldSet用于将文件结构中的信息映射到一个对象。FieldSet的作用是将文件的数据与类的field进行绑定（field是Java中常见的概念，不清楚的可以了解Java反射）。\"]},{\"header\":\"数据读取\",\"slug\":\"数据读取\",\"contents\":[\"Spring Batch为文件读取提供了FlatFileItemReader类，它为文件中的数据的读取和转换提供了基本功能。在FlatFileItemReader中有2个主要的功能接口，一是Resource、二是LineMapper。 Resource用于外部文件获取，详情请查看Spring核心——资源管理部分的内容，下面是一个例子：\",\"Resource resource = new FileSystemResource(\\\"resources/trades.csv\\\"); \",\"在复杂的生产环境中，文件通常由中心化、或者流程式的基础框架来管理（比如EAI）。因此文件往往需要使用FTP等方式从其他位置获取。如何迁移文件已经超出了Spring Batch框架的范围，在Spring的体系中可以参考Spring Integration项目。\",\"下面是FlatFileItemReader的属性，每一个属性都提供了Setter方法。\",\"属性名\",\"参数类型\",\"说明\",\"comments\",\"String[]\",\"指定文件中的注释前缀，用于过滤注释内容行\",\"encoding\",\"String\",\"指定文件的编码方式，默认为Charset.defaultCharset()\",\"lineMapper\",\"LineMapper\",\"利用LineMapper接口将一行字符串转换为对象\",\"linesToSkip\",\"int\",\"跳过文件开始位置的行数，用于跳过一些字段的描述行\",\"recordSeparatorPolicy\",\"RecordSeparatorPolicy\",\"用于判断数据是否结束\",\"resource\",\"Resource\",\"指定外部资源文件位置\",\"skippedLinesCallback\",\"LineCallbackHandler\",\"当配置linesToSkip，每执行一次跳过都会被回调一次，会传入跳过的行数据内容\",\"每个属性都为文件的解析提供了某方面的功能，下面是结构的说明。\"]},{\"header\":\"LineMapper\",\"slug\":\"linemapper\",\"contents\":[\"这个接口的作用是将字符串转换为对象：\",\"public interface LineMapper { T mapLine(String line, int lineNumber) throws Exception; } \",\"接口的基本处理逻辑是聚合类（FlatFileItemReader）传递一行字符串以及行号给LineMapper::mapLine，方法处理后返回一个映射的对象。\"]},{\"header\":\"LineTokenizer\",\"slug\":\"linetokenizer\",\"contents\":[\"这个接口的作用是将一行数据转换为一个FieldSet结构。对于Spring Batch而言，扁平结构文件的到Java实体的映射都通过FieldSet来控制，因此读写文件的过程需要完成字符串到FieldSet的转换：\",\"public interface LineTokenizer { FieldSet tokenize(String line); } \",\"这个接口的含义是：传递一行字符串数据，然后获取一个FieldSet。\",\"框架为LineTokenizer提供三个实现类：\",\"DelimitedLineTokenizer：利用分隔符将数据转换为FieldSet。最常见的分隔符是逗号,，类提供了分隔符的配置和解析方法。\",\"FixedLengthTokenizer：根据字段的长度来解析出FieldSet结构。必须为记录定义字段宽度。\",\"PatternMatchingCompositeLineTokenizer：使用一个匹配机制来动态决定使用哪个LineTokenizer。\"]},{\"header\":\"FieldSetMapper\",\"slug\":\"fieldsetmapper\",\"contents\":[\"该接口是将FieldSet转换为对象：\",\"public interface FieldSetMapper { T mapFieldSet(FieldSet fieldSet) throws BindException; } \",\"FieldSetMapper通常和LineTokenizer联合在一起使用：String->FieldSet->Object。\"]},{\"header\":\"DefaultLineMapper\",\"slug\":\"defaultlinemapper\",\"contents\":[\"DefaultLineMapper是LineMapper的实现，他实现了从文件到Java实体的映射：\",\"public class DefaultLineMapper implements LineMapper<>, InitializingBean { private LineTokenizer tokenizer; private FieldSetMapper fieldSetMapper; public T mapLine(String line, int lineNumber) throws Exception { return fieldSetMapper.mapFieldSet(tokenizer.tokenize(line)); } public void setLineTokenizer(LineTokenizer tokenizer) { this.tokenizer = tokenizer; } public void setFieldSetMapper(FieldSetMapper fieldSetMapper) { this.fieldSetMapper = fieldSetMapper; } } \",\"在解析文件时数据是按行解析的：\",\"传入一行字符串。\",\"LineTokenizer将字符串解析为FieldSet结构。\",\"FieldSetMapper继续解析为一个Java实体对象返回给调用者。\",\"DefaultLineMapper是框架提供的默认实现类，看似非常简单，但是利用组合模式可以扩展出很多功能。\"]},{\"header\":\"数据自动映射\",\"slug\":\"数据自动映射\",\"contents\":[\"在转换过程中如果将FieldSet的names属性与目标类的field绑定在一起，那么可以直接使用反射实现数据转换，为此框架提供了BeanWrapperFieldSetMapper来实现。\",\"DefaultLineMapper<WeatherEntity> lineMapper = new DefaultLineMapper<>(); //创建LineMapper DelimitedLineTokenizer tokenizer = new DelimitedLineTokenizer(); //创建LineTokenizer tokenizer.setNames(new String[] { \\\"siteId\\\", \\\"month\\\", \\\"type\\\", \\\"value\\\", \\\"ext\\\" }); //设置Field名称 //创建FieldSetMapper BeanWrapperFieldSetMapper<WeatherEntity> wrapperMapper = new BeanWrapperFieldSetMapper<>(); //设置实体，实体的field名称必须和tokenizer.names一致。 wrapperMapper.setTargetType(WeatherEntity.class); // 组合lineMapper lineMapper.setLineTokenizer(tokenizer); lineMapper.setFieldSetMapper(wrapperMapper); \"]},{\"header\":\"文件读取总结\",\"slug\":\"文件读取总结\",\"contents\":[\"上面提到了各种接口和实现，实际上都是围绕着FlatFileItemReader的属性在介绍，虽然内容很多但是实际上就以下几点：\",\"首先要定位文件，Spring Batch提供了Resource相关的定位方法。\",\"其次是将文件中的行字符串数据转换为对象，LineMapper的功能就是完成这个功能。\",\"框架为LineMapper提供了DefaultLineMapper作为默认实现方法，在DefaultLineMapper中需要组合使用LineTokenizer和FieldSetMapper。前者将字符串转为为一个Field，后者将Field转换为目标对象。\",\"LineTokenizer有3个实现类可供使用、FieldSetMapper有一个默认实现类BeanWrapperFieldSetMapper。\"]},{\"header\":\"文件读取可执行源码\",\"slug\":\"文件读取可执行源码\",\"contents\":[\"可执行的源码在下列地址的items子工程中：\",\"Gitee：https://gitee.com/chkui-com/spring-batch-sample\",\"Github：https://github.com/chkui/spring-batch-sample\",\"运行之前需要配置数据库链接，参看源码库中的README.md。\",\"文件读取的主要逻辑在org.chenkui.spring.batch.sample.items.FlatFileReader类：\",\"public class FlatFileReader { // FeildSet的字段名，设置字段名之后可以直接使用名字作为索引获取数据。也可以使用索引位置来获取数据 public final static String[] Tokenizer = new String[] { \\\"siteId\\\", \\\"month\\\", \\\"type\\\", \\\"value\\\", \\\"ext\\\" }; private boolean userWrapper = false; @Bean //定义FieldSetMapper用于FieldSet->WeatherEntity public FieldSetMapper<WeatherEntity> fieldSetMapper() { return new FieldSetMapper<WeatherEntity>() { @Override public WeatherEntity mapFieldSet(FieldSet fieldSet) throws BindException { if (null == fieldSet) { // fieldSet不存在则跳过该行处理 return null; } else { WeatherEntity observe = new WeatherEntity(); observe.setSiteId(fieldSet.readRawString(\\\"siteId\\\")); //Setter return observe; } } }; } @Bean // 配置 Reader public ItemReader<WeatherEntity> flatFileReader( @Qualifier(\\\"fieldSetMapper\\\") FieldSetMapper<WeatherEntity> fieldSetMapper) { FlatFileItemReader<WeatherEntity> reader = new FlatFileItemReader<>(); // 读取资源文件 reader.setResource(new FileSystemResource(\\\"src/main/resources/data.csv\\\")); // 初始化 LineMapper实现类 DefaultLineMapper<WeatherEntity> lineMapper = new DefaultLineMapper<>(); // 创建LineTokenizer接口实现 DelimitedLineTokenizer tokenizer = new DelimitedLineTokenizer(); // 设定每个字段的名称，如果不设置需要使用索引获取值 tokenizer.setNames(Tokenizer); // 设置tokenizer工具 lineMapper.setLineTokenizer(tokenizer); //使用 BeanWrapperFieldSetMapper 使用反射直接转换 if (userWrapper) { BeanWrapperFieldSetMapper<WeatherEntity> wrapperMapper = new BeanWrapperFieldSetMapper<>(); wrapperMapper.setTargetType(WeatherEntity.class); fieldSetMapper = wrapperMapper; } lineMapper.setFieldSetMapper(fieldSetMapper); reader.setLineMapper(lineMapper); // 跳过的初始行，用于过滤字段行 reader.setLinesToSkip(1); reader.open(new ExecutionContext()); return reader; } } \"]},{\"header\":\"按字段长度格读取文件\",\"slug\":\"按字段长度格读取文件\",\"contents\":[\"除了按照分隔符，有些文件可以字段数据的占位长度来提取数据。按照前面介绍的过程，实际上只要修改LineTokenizer接口即可，框架提供了FixedLengthTokenizer类：\",\"@Bean public FixedLengthTokenizer fixedLengthTokenizer() { FixedLengthTokenizer tokenizer = new FixedLengthTokenizer(); tokenizer.setNames(\\\"ISIN\\\", \\\"Quantity\\\", \\\"Price\\\", \\\"Customer\\\"); //Range用于设定数据的长度。 tokenizer.setColumns(new Range(1-12), new Range(13-15), new Range(16-20), new Range(21-29)); return tokenizer; } \"]},{\"header\":\"写入扁平结构文件\",\"slug\":\"写入扁平结构文件\",\"contents\":[\"将数据写入到文件与读取的过程正好相反：将对象转换为字符串。\"]},{\"header\":\"LineAggregator\",\"slug\":\"lineaggregator\",\"contents\":[\"与LineMapper相对应的是LineAggregator，他的功能是将实体转换为字符串：\",\"public interface LineAggregator<T> { public String aggregate(T item); } \"]},{\"header\":\"PassThroughLineAggregator\",\"slug\":\"passthroughlineaggregator\",\"contents\":[\"框架为LineAggregator接口提供了一个非常简单的实现类——PassThroughLineAggregator，其唯一实现就是使用对象的toString方法：\",\"public class PassThroughLineAggregator<T> implements LineAggregator<T> { public String aggregate(T item) { return item.toString(); } } \"]},{\"header\":\"DelimitedLineAggregator\",\"slug\":\"delimitedlineaggregator\",\"contents\":[\"LineAggregator的另外一个实现类是DelimitedLineAggregator。与PassThroughLineAggregator简单直接使用toString方法不同的是，DelimitedLineAggregator需要一个转换接口FieldExtractor：\",\"DelimitedLineAggregator<CustomerCredit> lineAggregator = new DelimitedLineAggregator<>(); lineAggregator.setDelimiter(\\\",\\\"); lineAggregator.setFieldExtractor(fieldExtractor); \"]},{\"header\":\"FieldExtractor\",\"slug\":\"fieldextractor\",\"contents\":[\"FieldExtractor用于实体类到collection结构的转换。它可以和LineTokenizer进行类比，前者是将实体类转换为扁平结构的数据，后者是将String转换为一个FieldSet结构。\",\"public interface FieldExtractor<T> { Object[] extract(T item); } \",\"框架为FieldExtractor接口提供了一个基于反射的实现类BeanWrapperFieldExtractor，其过程就是将实体对象转换为列表：\",\"BeanWrapperFieldExtractor<CustomerCredit> fieldExtractor = new BeanWrapperFieldExtractor<>(); fieldExtractor.setNames(new String[] {\\\"field1\\\", \\\"field2\\\"}); \",\"setName方法用于指定要转换的field列表。\"]},{\"header\":\"输出文件处理\",\"slug\":\"输出文件处理\",\"contents\":[\"​ 文件读取的逻辑非常简单：文件存在打开文件并写入数据，当文件不存在抛出异常。但是写入文件明显不能这么简单粗暴。新建一个JobInstance时最直观的操作是：存在同名文件就抛出异常，不存在则创建文件并写入数据。但是这样做显然有很大的问题，当批处理过程中出现问题需要restart，此时并不会从头开始处理所有的数据，而是要求文件存在并接着继续写入。为了确保这个过程 FlatFileItemWriter默认会在新 JobInstance运行时删除已有文件，而运行重启时继续在文件末尾写入。 FlatFileItemWriter可以使用 shouldDeleteIfExists、 appendAllowed、 shouldDeleteIfEmpty来有针对性的控制文件。\"]},{\"header\":\"文件写入可执源码\",\"slug\":\"文件写入可执源码\",\"contents\":[\"文件写入主要代码在org.chenkui.spring.batch.sample.items.FlatFileWriter：\",\"public class FlatFileWriter { private boolean useBuilder = true; @Bean public ItemWriter<MaxTemperatureEntiry> flatFileWriter() { BeanWrapperFieldExtractor<MaxTemperatureEntiry> fieldExtractor = new BeanWrapperFieldExtractor<>(); fieldExtractor.setNames(new String[] { \\\"siteId\\\", \\\"date\\\", \\\"temperature\\\" }); //设置映射field fieldExtractor.afterPropertiesSet(); //参数检查 DelimitedLineAggregator<MaxTemperatureEntiry> lineAggregator = new DelimitedLineAggregator<>(); lineAggregator.setDelimiter(\\\",\\\"); //设置输出分隔符 lineAggregator.setFieldExtractor(fieldExtractor); //设置FieldExtractor处理器 FlatFileItemWriter<MaxTemperatureEntiry> fileWriter = new FlatFileItemWriter<>(); fileWriter.setLineAggregator(lineAggregator); fileWriter.setResource(new FileSystemResource(\\\"src/main/resources/out-data.csv\\\")); //设置输出文件位置 fileWriter.setName(\\\"outpufData\\\"); if (useBuilder) {//使用builder方式创建 fileWriter = new FlatFileItemWriterBuilder<MaxTemperatureEntiry>().name(\\\"outpufData\\\") .resource(new FileSystemResource(\\\"src/main/resources/out-data.csv\\\")).lineAggregator(lineAggregator) .build(); } return fileWriter; } } \",\"文件的写入过程与读取过程完全对称相反：先用FieldExtractor将对象转换为一个collection结构（列表），然后用lineAggregator将collection转化为带分隔符的字符串。\"]},{\"header\":\"代码说明\",\"slug\":\"代码说明\",\"contents\":[\"代码中的测试数据来自数据分析交流项目bi-process-example，是NOAA的2015年全球天气监控数据。为了便于源码存储进行了大量的删减，原始数据有百万条，如有需要使用下列方式下载： curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2015.csv.gz #数据文件 curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt # 文件结构及类型说明\",\"代码实现了读取文件、处理文件、写入文件的整个过程。处理文件的过程是只获取监控的最高温度信息（Type=TMAX），其他都过滤。\",\"本案例的代码使用org.chenkui.spring.batch.sample.flatfile.FlatFileItemApplication::main方法运行，使用的是Command Runner的方式执行（运行方式的说明见Item概念及使用代码的命令行方式运行、Java内嵌运行）。\"]},{\"header\":\"Spring Batch——数据库批数据读写\",\"slug\":\"spring-batch——数据库批数据读写\",\"contents\":[\"本文将接着前面的内容说明数据库如何进行批处理读写。\"]},{\"header\":\"数据读取\",\"slug\":\"数据读取-1\",\"contents\":[\"数据库是绝大部分系统要用到的数据存储工具，因此针对数据库执行批量数据处理任务也是很常见的需求。数据的批量处理与常规业务开发不同，如果一次性读取百万条，对于任何系统而言肯定都是不可取的。为了解决这个问题Spring Batch提供了2套数据读取方案：\",\"基于游标读取数据\",\"基于分页读取数据\"]},{\"header\":\"游标读取数据\",\"slug\":\"游标读取数据\",\"contents\":[\"对于有经验大数据工程师而言数据库游标的操作应该是非常熟悉的，因为这是从数据库读取数据流标准方法，而且在Java中也封装了ResultSet这种面向游标操作的数据结构。\",\"ResultSet一直都会指向结果集中的某一行数据，使用next方法可以让游标跳转到下一行数据。Spring Batch同样使用这个特性来控制数据的读取：\",\"在初始化时打开游标。\",\"每一次调用ItemReader::read方法就从ResultSet获取一行数据并执行next。\",\"返回可用于数据处理的映射结构（map、dict）。\",\"在一切都执行完毕之后，框架会使用回调过程调用ResultSet::close来关闭游标。由于所有的业务过程都绑定在一个事物之上，所以知道到Step执行完毕或异常退出调用执行close。下图展示了数据读取的过程：\",\"SQL语句的查询结果称为数据集（对于大部分数据库而言，其SQL执行结果会产生临时的表空间索引来存放数据集）。游标开始会停滞在ID=2的位置，一次ItemReader执行完毕后会产生对应的实体FOO2，然后游标下移直到最后的ID=6。最后关闭游标。\"]},{\"header\":\"JdbcCursorItemReader\",\"slug\":\"jdbccursoritemreader\",\"contents\":[\"JdbcCursorItemReader是使用游标读取数据集的ItemReader实现类之一。它使用JdbcTemplate中的DataSource控制ResultSet,其过程是将ResultSet的每行数据转换为所需要的实体类。\",\"JdbcCursorItemReader的执行过程有三步：\",\"通过DataSource创建JdbcTemplate。\",\"设定数据集的SQL语句。\",\"创建ResultSet到实体类的映射。 大致如下：\",\"//随风溜达的向日葵 chkui.com JdbcCursorItemReader itemReader = new JdbcCursorItemReader(); itemReader.setDataSource(dataSource); itemReader.setSql(\\\"SELECT ID, NAME, CREDIT from CUSTOMER\\\"); itemReader.setRowMapper(new CustomerCreditRowMapper()); \",\"除了上面的代码，JdbcCursorItemReader还有其他属性：\",\"属性名称\",\"说明\",\"ignoreWarnings\",\"标记当执行SQL语句出现警告时，是输出日志还是抛出异常，默认为true——输出日志\",\"fetchSize\",\"预通知JDBC驱动全量数据的个数\",\"maxRows\",\"设置ResultSet从数据库中一次读取记录的上限\",\"queryTimeout\",\"设置执行SQL语句的等待超时时间，单位秒。当超过这个时间会抛出DataAccessException\",\"verifyCursorPosition\",\"对游标位置进行校验。由于在RowMapper::mapRow方法中ResultSet是直接暴露给使用者的，因此有可能在业务代码层面调用了ResultSet::next方法。将这个属性设置为true,在框架中会有一个位置计数器与ResultSet保持一致，当执行完Reader后位置不一致会抛出异常。\",\"saveState\",\"标记读取的状态是否被存放到ExecutionContext中。默认为true\",\"driverSupportsAbsolute\",\"告诉框架是指直接使用ResultSet::absolute方法来指定游标位置，使用这个属性需要数据库驱动支持。建议在支持absolute特性的数据库上开启这个特性，能够明显的提升性能。默认为false\",\"setUseSharedExtendedConnection\",\"标记读取数据的游标是否与Step其他过程绑定成同一个事物。默认为false,表示读取数据的游标是单独建立连接的，具有自身独立的事物。如果设定为true需要用ExtendedConnectionDataSourceProxy包装DataSource用于管理事物过程。此时游标的创建标记为'READ_ONLY'、'HOLD_CURSORS_OVER_COMMIT'。需要注意的是该属性需要数据库支持3.0以上的JDBC驱动。\",\"执行JdbcCursorItemReader的代码在org.chenkui.spring.batch.sample.items.JdbcReader。启动位置是org.chenkui.spring.batch.sample.database.cursor.JdbcCurosrApplication。\",\"在运行代码之前请先在数据库中执行以下DDL语句，并添加部分测试数据。\",\"CREATE TABLE `tmp_test_weather` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `siteid` varchar(64) NOT NULL COMMENT '业务主键', `month` varchar(64) NOT NULL COMMENT '日期', `type` varchar(64) NOT NULL COMMENT '气象类型', `value` int(11) NOT NULL COMMENT '值', `ext` varchar(255) DEFAULT NULL COMMENT '扩展数据', PRIMARY KEY (`id`) ) ; \",\"运行代码：\",\"//随风溜达的向日葵 chkui.com public class JdbcReader { @Bean public RowMapper<WeatherEntity> weatherEntityRowMapper() { return new RowMapper<WeatherEntity>() { public static final String SITEID_COLUMN = \\\"siteId\\\"; // 设置映射字段 public static final String MONTH_COLUMN = \\\"month\\\"; public static final String TYPE_COLUMN = \\\"type\\\"; public static final String VALUE_COLUMN = \\\"value\\\"; public static final String EXT_COLUMN = \\\"ext\\\"; @Override // 数据转换 public WeatherEntity mapRow(ResultSet resultSet, int rowNum) throws SQLException { WeatherEntity weatherEntity = new WeatherEntity(); weatherEntity.setSiteId(resultSet.getString(SITEID_COLUMN)); weatherEntity.setMonth(resultSet.getString(MONTH_COLUMN)); weatherEntity.setType(WeatherEntity.Type.valueOf(resultSet.getString(TYPE_COLUMN))); weatherEntity.setValue(resultSet.getInt(VALUE_COLUMN)); weatherEntity.setExt(resultSet.getString(EXT_COLUMN)); return weatherEntity; } }; } @Bean public ItemReader<WeatherEntity> jdbcCursorItemReader( @Qualifier(\\\"weatherEntityRowMapper\\\") RowMapper<WeatherEntity> rowMapper, DataSource datasource) { JdbcCursorItemReader<WeatherEntity> itemReader = new JdbcCursorItemReader<>(); itemReader.setDataSource(datasource); //设置DataSource //设置读取的SQL itemReader.setSql(\\\"SELECT siteId, month, type, value, ext from TMP_TEST_WEATHER\\\"); itemReader.setRowMapper(rowMapper); //设置转换 return itemReader; } } \"]},{\"header\":\"HibernateCursorItemReader\",\"slug\":\"hibernatecursoritemreader\",\"contents\":[\"在Java体系中数据库操作常见的规范有JPA或ORM，Spring Batch提供了HibernateCursorItemReader来实现HibernateTemplate,它可以通过Hibernate框架进行游标的控制。\",\"需要注意的是：使用Hibernate框架来处理批量数据到目前为止一直都有争议，核心原因是Hibernate最初是为在线联机事物型系统开发的。不过这并不意味着不能使用它来处理批数据，解决此问题就是让Hibernate使用StatelessSession用来保持游标，而不是standard session一次读写，这将导致Hibernate的缓存机制和数据脏读检查失效，进而影响批处理的过程。关于Hibernate的状态控制机制请阅读官方文档。\",\"HibernateCursorItemReader使用过程与JdbcCursorItemReader没多大差异都是逐条读取数据然后控制状态链接关闭。只不过他提供了Hibernate所使用的HSQL方案。\",\"@Bean public ItemReader<WeatherEntity> hibernateCursorItemReader(SessionFactory sessionFactory) { HibernateCursorItemReader<WeatherEntity> itemReader = new HibernateCursorItemReader<>(); itemReader.setName(\\\"hibernateCursorItemReader\\\"); itemReader.setQueryString(\\\"from WeatherEntity tmp_test_weather\\\"); itemReader.setSessionFactory(sessionFactory); return itemReader; } \",\"或\",\"public ItemReader<WeatherEntity> hibernateCursorItemReader(SessionFactory sessionFactory) { return new HibernateCursorItemReaderBuilder<CustomerCredit>() .name(\\\"creditReader\\\") .sessionFactory(sessionFactory) .queryString(\\\"from CustomerCredit\\\") .build(); } \",\"如果没有特别的需要，不推荐使用Hibernate。\"]},{\"header\":\"StoredProcedureItemReader\",\"slug\":\"storedprocedureitemreader\",\"contents\":[\"存储过程是在同一个数据库中处理大量数据的常用方法。StoredProcedureItemReader的执行过程和JdbcCursorItemReader一致，但是底层逻辑是先执行存储过程，然后返回存储过程执行结果游标。不同的数据库存储过程游标返回会有一些差异：\",\"作为一个ResultSet返回。（SQL Server Sybase, DB2, Derby以及MySQL）\",\"参数返回一个 ref-cursor实例。比如Oracle、PostgreSQL数据库，这类数据库存储过程是不会直接return任何内容的，需要从传参获取。\",\"返回存储过程调用后的返回值。\",\"针对以上3个类型，配置上有一些差异：\",\"//随风溜达的向日葵 chkui.com @Bean public StoredProcedureItemReader reader(DataSource dataSource) { StoredProcedureItemReader reader = new StoredProcedureItemReader(); reader.setDataSource(dataSource); reader.setProcedureName(\\\"sp_processor_weather\\\"); reader.setRowMapper(new weatherEntityRowMapper()); //第二种类型需要指定ref-cursor的参数位置 reader.setRefCursorPosition(1); //第三种类型需要明确的告知reader通过返回获取 reader.setFunction(true); return reader; } \",\"使用存储过程处理数据的好处是可以实现针对库内的数据进行合并、分割、排序等处理。如果数据在同一个数据库，性能也明显好于通过Java处理。\"]},{\"header\":\"分页读取数据\",\"slug\":\"分页读取数据\",\"contents\":[\"相对于游标，还有一个办法是进行分页查询。分页查询意味着再进行批处理的过程中同一个SQL会多次执行。在联机型事物系统中分页查询常用于列表功能，每一次查询需要指定开始位置和结束位置。\"]},{\"header\":\"JdbcPagingItemReader\",\"slug\":\"jdbcpagingitemreader\",\"contents\":[\"分页查询的默认实现类是JdbcPagingItemReader，它的核心功能是用分页器PagingQueryProvider进行分页控制。由于不同的数据库分页方法差别很大，所以针对不同的数据库有不同的实现类。框架提供了SqlPagingQueryProviderFactoryBean用于检查当前数据库并自动注入对应的PagingQueryProvider。\",\"JdbcPagingItemReader会从数据库中一次性读取一整页的数据，但是调用Reader的时候还是会一行一行的返回数据。框架会自行根据运行情况确定什么时候需要执行下一个分页的查询。\"]},{\"header\":\"分页读取数据执行源码\",\"slug\":\"分页读取数据执行源码\",\"contents\":[\"Gitee：https://gitee.com/chkui-com/spring-batch-sample\",\"Github：https://github.com/chkui/spring-batch-sample\",\"执行JdbcPagingItemReader的代码在org.chenkui.spring.batch.sample.items.pageReader。启动位置是org.chenkui.spring.batch.sample.database.paging.JdbcPagingApplication：\",\"//随风溜达的向日葵 chkui.com public class pageReader { final private boolean wrapperBuilder = false; @Bean //设置 queryProvider public SqlPagingQueryProviderFactoryBean queryProvider(DataSource dataSource) { SqlPagingQueryProviderFactoryBean provider = new SqlPagingQueryProviderFactoryBean(); provider.setDataSource(dataSource); provider.setSelectClause(\\\"select id, siteid, month, type, value, ext\\\"); provider.setFromClause(\\\"from tmp_test_weather\\\"); provider.setWhereClause(\\\"where id>:start\\\"); provider.setSortKey(\\\"id\\\"); return provider; } @Bean public ItemReader<WeatherEntity> jdbcPagingItemReader(DataSource dataSource, PagingQueryProvider queryProvider, RowMapper<WeatherEntity> rowMapper) { Map<String, Object> parameterValues = new HashMap<>(); parameterValues.put(\\\"start\\\", \\\"1\\\"); JdbcPagingItemReader<WeatherEntity> itemReader; if (wrapperBuilder) { itemReader = new JdbcPagingItemReaderBuilder<WeatherEntity>() .name(\\\"creditReader\\\") .dataSource(dataSource) .queryProvider(queryProvider) .parameterValues(parameterValues) .rowMapper(rowMapper) .pageSize(1000) .build(); } else { itemReader = new JdbcPagingItemReader<>(); itemReader.setName(\\\"weatherEntityJdbcPagingItemReader\\\"); itemReader.setDataSource(dataSource); itemReader.setQueryProvider(queryProvider); itemReader.setParameterValues(parameterValues); itemReader.setRowMapper(rowMapper); itemReader.setPageSize(1000); } return itemReader; } } \"]},{\"header\":\"数据写入\",\"slug\":\"数据写入\",\"contents\":[\"Spring Batch为不同类型的文件的写入提供了多个实现类，但并没有为数据库的写入提供任何实现类，而是交由开发者自己去实现接口。理由是：\",\"数据库的写入与文件写入有巨大的差别。对于一个Step而言，在写入一份文件时需要保持对文件的打开状态从而能够高效的向队尾添加数据。如果每次都重新打开文件，从开始位置移动到队尾会耗费大量的时间（很多文件流无法在open时就知道长度）。当整个Step结束时才能关闭文件的打开状态，框架提供的文件读写类都实现了这个控制过程。\",\"另外无论使用何种方式将数据写入文件都是\\\"逐行进行\\\"的（流数据写入、字符串逐行写入）。因此当数据写入与整个Step绑定为事物时还需要实现一个控制过程是：在写入数据的过程中出现异常时要擦除本次事物已经写入的数据，这样才能和整个Step的状态保持一致。框架中的类同样实现了这个过程。\",\"但是向数据库写入数据并不需要类似于文件的尾部写入控制，因为数据库的各种链接池本身就保证了链接->写入->释放的高效执行，也不存在向队尾添加数据的问题。而且几乎所有的数据库驱动都提供了事物能力，在任何时候出现异常都会自动回退，不存在擦除数据的问题。\",\"因此，对于数据库的写入操作只要按照常规的批量数据写入的方式即可，开发者使用任何工具都可以完成这个过程。\"]},{\"header\":\"写入数据一个简单的实现\",\"slug\":\"写入数据一个简单的实现\",\"contents\":[\"实现数据写入方法很多，这和常规的联机事务系统没任何区别。下面直接用JdbcTemplate实现了一个简单的数据库写入过程。\",\"执行数据库写入的核心代码在org.chenkui.spring.batch.sample.items.JdbcWriter。启动位置是org.chenkui.spring.batch.sample.database.output.JdbcWriterApplication。\",\"//随风溜达的向日葵 chkui.com public class JdbcWriter { @Bean public ItemWriter<WeatherEntity> jdbcBatchWriter(JdbcTemplate template) { return new ItemWriter<WeatherEntity>() { final private static String INSERt_SQL = \\\"INSERT INTO tmp_test_weather(siteid, month, type, value, ext) VALUES(?,?,?,?,?)\\\"; @Override public void write(List<? extends WeatherEntity> items) throws Exception { List<Object[]> batchArgs = new ArrayList<>(); for (WeatherEntity entity : items) { Object[] objects = new Object[5]; objects[0] = entity.getSiteId(); objects[1] = entity.getMonth(); objects[2] = entity.getType().name(); objects[3] = entity.getValue(); objects[4] = entity.getExt(); batchArgs.add(objects); } template.batchUpdate(INSERt_SQL, batchArgs); } }; } } \"]},{\"header\":\"组合使用案例\",\"slug\":\"组合使用案例\",\"contents\":[\"下面是一些组合使用过程，简单实现了文件到数据库、数据库到文件的过程。文件读写的过程已经在文件读写中介绍过，这里会重复使用之前介绍的文件读写的功能。\",\"下面的案例是将data.csv中的数据写入到数据库，然后再将数据写入到out-data.csv。案例组合使用已有的item完成任务：flatFileReader、jdbcBatchWriter、jdbcCursorItemReader、simpleProcessor、flatFileWriter。这种Reader、Processor、Writer组合的方式也是完成一个批处理工程的常见开发方式。\",\"案例的运行代码在org.chenkui.spring.batch.sample.database.complex包中，使用了2个Step来完成任务，一个将数据读取到数据库，一个将数据进行过滤，然后再写入到文件：\",\"//随风溜达的向日葵 chkui.com public class FileComplexProcessConfig { @Bean // 配置Step1 public Step file2DatabaseStep(StepBuilderFactory builder, @Qualifier(\\\"flatFileReader\\\") ItemReader<WeatherEntity> reader, @Qualifier(\\\"jdbcBatchWriter\\\") ItemWriter<WeatherEntity> writer) { return builder.get(\\\"file2DatabaseStep\\\") // 创建 .<WeatherEntity, WeatherEntity>chunk(50) // 分片 .reader(reader) // 读取 .writer(writer) // 写入 .faultTolerant() // 开启容错处理 .skipLimit(20) // 跳过设置 .skip(Exception.class) // 跳过异常 .build(); } @Bean // 配置Step2 public Step database2FileStep(StepBuilderFactory builder, @Qualifier(\\\"jdbcCursorItemReader\\\") ItemReader<WeatherEntity> reader, @Qualifier(\\\"simpleProcessor\\\") ItemProcessor<WeatherEntity, MaxTemperatureEntiry> processor, @Qualifier(\\\"flatFileWriter\\\") ItemWriter<MaxTemperatureEntiry> writer) { return builder.get(\\\"database2FileStep\\\") // 创建 .<WeatherEntity, MaxTemperatureEntiry>chunk(50) // 分片 .reader(reader) // 读取 .processor(processor) // .writer(writer) // 写入 .faultTolerant() // 开启容错处理 .skipLimit(20) // 跳过设置 .skip(Exception.class) // 跳过异常 .build(); } @Bean public Job file2DatabaseJob(@Qualifier(\\\"file2DatabaseStep\\\") Step step2Database, @Qualifier(\\\"database2FileStep\\\") Step step2File, JobBuilderFactory builder) { return builder.get(\\\"File2Database\\\").start(step2Database).next(step2File).build(); } } \"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"Spring Batch(1)——数据批处理概念 (tencent.com)\",\"Spring Batch中@StepScope的适用范围及理解_lovepeacee的博客-CSDN博客\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatchJob.html\":{\"title\":\"Spring Batch——Job配置与运行\",\"contents\":[{\"header\":\"Spring Batch——Job配置与运行\",\"slug\":\"spring-batch——job配置与运行\",\"contents\":[\"​ 在上文中介绍了批处理的概念以及Spring Batch相关的使用场景，后续将会陆续说明在代码层面如何使用。\"]},{\"header\":\"引入\",\"slug\":\"引入\",\"contents\":[\"​ Spring batch的引入非常简单，只需要引入Spring Framework、Datasource以及Spring Batch。在Spring Boot体系下只需引入spring-boot-starter-batch 即可。他已经涵盖了以上所有内容。\"]},{\"header\":\"Job配置\",\"slug\":\"job配置\",\"contents\":[\"Job接口有多种多样的实现类，通常我们使用configuration类来构建获取一个Job：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") //Job名称 .start(playerLoad()) //Job Step .next(gameLoad()) //Job Step .next(playerSummarization()) //Job Step .end() .build(); } \",\"上面的代码定义了一个Job实例，并且在这个实例中包含了三个Step实例\"]},{\"header\":\"重启（启动）配置\",\"slug\":\"重启-启动-配置\",\"contents\":[\"​ 批处理的一个核心问题是需要定义重启（启动）时的一些行为。当指定的JobInstance 被JobExecution 执行时候即认为某个Job已经重启（启动）。理想状态下，所有的任务都应该可以从它们之前中断的位置启动，但是某些情况下这样做是无法实现的。开发人员可以关闭重启机制或认为每次启动都是新的JobInstance ：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") .preventRestart() //防止重启 ... .build(); } \"]},{\"header\":\"监听Job Execution\",\"slug\":\"监听job-execution\",\"contents\":[\"当任务执行完毕或开始执行时，需要执行一些处理工作。这个时候可以使用JobExecutionListener：\",\"public interface JobExecutionListener { void beforeJob(JobExecution jobExecution); void afterJob(JobExecution jobExecution); } \",\"添加方式：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") .listener(sampleListener()) //JobExecutionListener的实现类 ... .build(); } \",\"需要注意的是afterJob方法无论批处理任务成功还是失败都会被执行，所以增加以下判断：\",\"public void afterJob(JobExecution jobExecution){ if( jobExecution.getStatus() == BatchStatus.COMPLETED ){ //job success } else if(jobExecution.getStatus() == BatchStatus.FAILED){ //job failure } } \",\"除了直接实现接口还可以用 @BeforeJob 和 @AfterJob 注解。\"]},{\"header\":\"Java配置\",\"slug\":\"java配置\",\"contents\":[\"​ 在Spring Batch 2.2.0版本之后（Spring 3.0+）支持纯Java配置。其核心是@EnableBatchProcessing 注解和两个构造器。@EnableBatchProcessing的作用类似于Spring中的其他@Enable*,使用@EnableBatchProcessing之后会提供一个基本的配置用于执行批处理任务。\",\"​ 对应的会有一系列StepScope实例被注入到Ioc容器中：JobRepository、JobLauncher、JobRegistry、PlatformTransactionManager、JobBuilderFactory以及StepBuilderFactory。\",\"​ 配置的核心接口是BatchConfigurer，默认情况下需要在容器中指定DataSource，该数据源用于JobRepository相关的表。开发的过程中可以使用自定义的BatchConfigurer实现来提供以上所有的Bean。通常情况下可以扩展重载DefaultBatchConfigurer类中的Getter方法用于实现部分自定义功能：\",\"@Bean public BatchConfigurer batchConfigurer() { return new DefaultBatchConfigurer() { @Override public PlatformTransactionManager getTransactionManager() { return new MyTransactionManager(); } }; } \",\"使用了@EnableBatchProcessing之后开发人员可以使用以下的方法来配置一个Job：\",\"@Configuration @EnableBatchProcessing @Import(DataSourceConfiguration.class) public class AppConfig { @Autowired private JobBuilderFactory jobs; @Autowired private StepBuilderFactory steps; @Bean public Job job(@Qualifier(\\\"step1\\\") Step step1,@Qualifier(\\\"step2\\\") Step step2) { return jobs.get(\\\"myJob\\\").start(step1).next(step2).build(); } @Bean protected Step step1(ItemReader<Person> reader, ItemProcessor<Person, Person> processor, ItemWriter<Person> writer) { return steps.get(\\\"step1\\\") .<Person, Person> chunk(10) .reader(reader) .processor(processor) .writer(writer) .build(); } @Bean protected Step step2(Tasklet tasklet) { return steps.get(\\\"step2\\\") .tasklet(tasklet) .build(); } } \"]},{\"header\":\"JobRepository配置\",\"slug\":\"jobrepository配置\",\"contents\":[\"​ 一旦使用了@EnableBatchProcessing 注解，JobRepository即会被注入到IoCs容器中并自动使用容器中的DataSource。JobRepository用于处理批处理表的CURD，整个Spring Batch的运行都会使用到它。除了使用容器中默认的DataSoruce以及其他组件，还可以在BatchConfigurer中进行配置：\",\"@Override protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setIsolationLevelForCreate(\\\"ISOLATION_SERIALIZABLE\\\"); factory.setTablePrefix(\\\"BATCH_\\\"); factory.setMaxVarCharLength(1000); return factory.getObject(); } \",\"在代码中可以看到，设置JobRepository需要DataSource和TransactionManager，如果没有指定将会使用容器中的默认配置。\"]},{\"header\":\"JobRepository的事物配置\",\"slug\":\"jobrepository的事物配置\",\"contents\":[\"​ 默认情况下框架为JobRepository提供了默认PlatformTransactionManager事物管理。它用于确保批处理执行过程中的元数据正确的写入到指定数据源中。如果缺乏事物，那么框架产生元数据就无法和整个处理过程完全契合。\",\"​ 如下图，在BatchConfigurer中的setIsolationLevelForCreate方法中可以指定事物的隔离等级：\",\"protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setIsolationLevelForCreate(\\\"ISOLATION_REPEATABLE_READ\\\"); return factory.getObject(); } \",\"setIsolationLevelForCreate方法支持2个值：ISOLATION_SERIALIZABLE 、ISOLATION_REPEATABLE_READ ，前者是默认配置，类似于@Transactional(isolation = Isolation.SERIALIZABLE)，表示查询和写入都是一次事物，会对事物进行严格的锁定，当事物完成提交后才能进行其他的读写操作，容易死锁。后者是读事物开放，写事物锁定。任何时候都可以快速的读取数据，但是写入事物有严格的事物机制。当一个事物挂起某些记录时，其他写操作必须排队。\"]},{\"header\":\"修改表名称\",\"slug\":\"修改表名称\",\"contents\":[\"默认情况下，JobRepository管理的表都以*BATCH_*开头。需要时可以修改前缀：\",\"// This would reside in your BatchConfigurer implementation @Override protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setTablePrefix(\\\"SYSTEM.TEST_\\\"); //修改前缀 return factory.getObject(); } \"]},{\"header\":\"内存级存储\",\"slug\":\"内存级存储\",\"contents\":[\"​ Spring Batch支持将运行时的状态数据（元数据）仅保存在内存中。重载JobRepository不设置DataSource 即可：\",\"@Override protected JobRepository createJobRepository() throws Exception { MapJobRepositoryFactoryBean factory = new MapJobRepositoryFactoryBean(); factory.setTransactionManager(transactionManager); return factory.getObject(); } \",\"需要注意的是，内存级存储无法满足分布式系统。\"]},{\"header\":\"JobLauncher配置\",\"slug\":\"joblauncher配置\",\"contents\":[\"启用了@EnableBatchProcessing 之后JobLauncher 会自动注入到容器中以供使用。此外可以自行进行配置：\",\"@Override protected JobLauncher createJobLauncher() throws Exception { SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository); jobLauncher.afterPropertiesSet(); return jobLauncher; } \",\"JobLauncher 唯一的必要依赖只有JobRepository。如下图，Job的执行通常是一个同步过程：\",\"![bpq56xfz43](img/Spring Batch Job/bpq56xfz43.png)\",\"可以通过修改TaskExecutor来指定Job的执行过程：\",\"@Bean public JobLauncher jobLauncher() { SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository()); jobLauncher.setTaskExecutor(new SimpleAsyncTaskExecutor()); //转换为异步任务 jobLauncher.afterPropertiesSet(); return jobLauncher; } \",\"这样执行过程变为：\"]},{\"header\":\"运行一个Job\",\"slug\":\"运行一个job\",\"contents\":[\"以一个Http为例：\",\"@Controller public class JobLauncherController { @Autowired JobLauncher jobLauncher; @Autowired Job job; @RequestMapping(\\\"/jobLauncher.html\\\") public void handle() throws Exception{ jobLauncher.run(job, new JobParameters()); } } \",\"单单是配置好Job是肯定无法执行的，还需要对Step进行配置。后面会陆续介绍。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatchStep.html\":{\"title\":\"Spring Batch——Step控制\",\"contents\":[{\"header\":\"Spring Batch——Step控制\",\"slug\":\"spring-batch——step控制\",\"contents\":[\"​ 批处理任务的主要业务逻辑都是在Step中去完成的。可以将Job理解为运行Step的框架，而Step理解为业务功能。\"]},{\"header\":\"Step配置\",\"slug\":\"step配置\",\"contents\":[\"​ Step是Job中的工作单元，每一个Step涵盖了单行记录的处理闭环。下图是一个Step的简要结构：\",\"​ 一个Step通常涵盖三个部分：读数据（Reader）、处理数据（Processor）和写数据（Writer）。但是并不是所有的Step 都需要自身来完成数据的处理，比如存储过程等方式是通过外部功能来完成，因此Spring Batch提供了2种Step的处理方式：1）面向分片的ChunkStep，2）面向过程的TaskletStep。但是基本上大部分情况下都是使用面向分片的方式来解决问题。\"]},{\"header\":\"面向分片的处理过程\",\"slug\":\"面向分片的处理过程\",\"contents\":[\"​ 在Step中数据是按记录（按行）处理的，但是每条记录处理完毕之后马上提交事物反而会导致IO的巨大压力。因此Spring Batch提供了数据处理的分片功能。设置了分片之后，一次工作会从Read开始，然后交由给Processor处理。处理完毕后会进行聚合，待聚合到一定的数量的数据之后一次性调用Write将数据提交到物理数据库。其过程大致为：\",\"在Spring Batch中所谓的事物和数据事物的概念一样，就是一次性提交多少数据。如果在聚合数据期间出现任何错误，所有的这些数据都将不执行写入。\"]},{\"header\":\"分区\",\"slug\":\"分区\",\"contents\":[\"Spring Batch也为Step的分区执行和远程执行提供了一个SPI(服务提供者接口)。在这种情况下,远端的执行程序只是一些简单的Step实例,配置和使用方式都和本机处理一样容易。下面是一幅实际的模型示意图:\",\"在左侧执行的作业(Job)是串行的Steps,而中间的那一个Step被标记为 Master。图中的 Slave 都是一个Step的相同实例,对于作业来说,这些Slave的执行结果实际上等价于就是Master的结果。Slaves通常是远程服务,但也有可能是本地执行的其他线程。在此模式中,Master发送给Slave的消息不需要持久化(durable) ,也不要求保证交付: 对每个作业执行步骤来说,保存在 JobRepository 中的Spring Batch元信息将确保每个Slave都会且仅会被执行一次。\",\"Spring Batch的SPI由Step的一个专门的实现( PartitionStep),以及需要由特定环境实现的两个策略接口组成。这两个策略接口分别是 PartitionHandler 和 StepExecutionSplitter,他们的角色如下面的序列图所示:此时在右边的Step就是“远程”Slave。\"]},{\"header\":\"分区处理器(PartitionHandler)\",\"slug\":\"分区处理器-partitionhandler\",\"contents\":[\"PartitionHandler组件知道远程网格环境的组织结构。 它可以发送StepExecution请求给远端Steps,采用某种具体的数据格式,例如DTO.它不需要知道如何分割输入数据,或者如何聚合多个步骤执行的结果。一般来说它可能也不需要了解弹性或故障转移,因为在许多情况下这些都是结构的特性,无论如何Spring Batch总是提供了独立于结构的可重启能力: 一个失败的作业总是会被重新启动,并且只会重新执行失败的步骤。\",\"PartitionHandler接口可以有各种结构的实现类: 如简单RMI远程方法调用,EJB远程调用,自定义web服务、JMS、Java Spaces, 共享内存网格(如Terracotta或Coherence)、网格执行结构(如GridGain)。Spring Batch自身不包含任何专有网格或远程结构的实现。\",\"但是 Spring Batch也提供了一个有用的PartitionHandler实现，在本地分开的线程中执行Steps,该实现类名为 TaskExecutorPartitionHandler,并且他是上面的XML配置中的默认处理器。还可以像下面这样明确地指定:\",\"<step id=\\\"step1.master\\\"> <partition step=\\\"step1\\\" handler=\\\"handler\\\"/> </step> <bean class=\\\"org.spr...TaskExecutorPartitionHandler\\\"> <property name=\\\"taskExecutor\\\" ref=\\\"taskExecutor\\\"/> <property name=\\\"step\\\" ref=\\\"step1\\\" /> <property name=\\\"gridSize\\\" value=\\\"10\\\" /> </bean> \",\"gridSize决定要创建的独立的step执行的数量,所以它可以配置为TaskExecutor中线程池的大小,或者也可以设置得比可用的线程数稍大一点,在这种情况下,执行块变得更小一些。\",\"TaskExecutorPartitionHandler 对于IO密集型步骤非常给力,比如要拷贝大量的文件,或复制文件系统到内容管理系统时。它还可用于远程执行的实现,通过为远程调用提供一个代理的步骤实现(例如使用Spring Remoting)，如下面代码注入到 Spring 容器当中使用\",\"@Bean public PartitionHandler suyqrPartitionHandler(){ // 分格，多线程处理 TaskExecutorPartitionHandler handler = new TaskExecutorPartitionHandler(); handler.setGridSize(10); handler.setStep(suyqrSlaveStep()); handler.setTaskExecutor(taskExecutor); try { handler.afterPropertiesSet(); } catch (Exception e) { e.printStackTrace(); } return handler; } \"]},{\"header\":\"分割器(Partitioner)\",\"slug\":\"分割器-partitioner\",\"contents\":[\"分割器有一个简单的职责: 仅为新的step实例生成执行环境(contexts),作为输入参数(这样重启时就不需要考虑)。 该接口只有一个方法:\",\"public interface Partitioner { Map<String, ExecutionContext> partition(int gridSize); } \",\"这个方法的返回值是一个Map对象,将每个Step执行分配的唯一名称(Map泛型中的 String),和与其相关的输入参数以ExecutionContext 的形式做一个映射。Step执行的名称( Partitioner接口返回的 Map 中的 key)在整个作业的执行过程中需要保持唯一,除此之外没有其他具体要求。简单说，实现 Partitioner 接口实现\"]},{\"header\":\"面向对象配置Step\",\"slug\":\"面向对象配置step\",\"contents\":[\"@Bean public Job sampleJob(JobRepository jobRepository, Step sampleStep) { return this.jobBuilderFactory.get(\\\"sampleJob\\\") .repository(jobRepository) .start(sampleStep) .build(); } @Bean public Step sampleStep(PlatformTransactionManager transactionManager) { return this.stepBuilderFactory.get(\\\"sampleStep\\\") .transactionManager(transactionManager) .<String, String>chunk(10) //分片配置 .reader(itemReader()) //reader配置 .writer(itemWriter()) //write配置 .build(); } \",\"观察sampleStep方法：\",\"reader: 使用ItemReader提供读数据的方法。\",\"write：ItemWrite提供写数据的方法。\",\"transactionManager：使用默认的 PlatformTransactionManager 对事物进行管理。当配置好事物之后Spring Batch会自动对事物进行管理，无需开发人员显示操作。\",\"chunk：指定一次性数据提交的记录数，因为任务是基于Step分次处理的，当累计到chunk配置的次数则进行一次提交。提交的内容除了业务数据，还有批处理任务运行相关的元数据。\",\"是否使用ItemProcessor是一个可选项。如果没有Processor可以将数据视为读取并直接写入。\"]},{\"header\":\"提交间隔\",\"slug\":\"提交间隔\",\"contents\":[\"​ Step使用PlatformTransactionManager管理事物。每次事物提交的间隔根据chunk方法中配置的数据执行。如果设置为1，那么在每一条数据处理完之后都会调用ItemWrite进行提交。提交间隔设置太小，那么会浪费需要多不必要的资源，提交间隔设置的太长，会导致事物链太长占用空间，并且出现失败会导致大量数据回滚。因此设定一个合理的间隔是非常必要的，这需要根据实际业务情况、性能要求、以及数据安全程度来设定。如果没有明确的评估目标，设置为10~20较为合适。\"]},{\"header\":\"配置Step重启\",\"slug\":\"配置step重启\",\"contents\":[\"前文介绍了Job的重启，但是每次重启对Step也是有很大的影响的，因此需要特定的配置。\"]},{\"header\":\"限定重启次数\",\"slug\":\"限定重启次数\",\"contents\":[\"​ 某些Step可能用于处理一些先决的任务，所以当Job再次重启时这Step就没必要再执行，可以通过设置startLimit来限定某个Step重启的次数。当设置为1时候表示仅仅运行一次，而出现重启时将不再执行：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(itemReader()) .writer(itemWriter()) .startLimit(1) .build(); } \"]},{\"header\":\"重启已经完成任务的Step\",\"slug\":\"重启已经完成任务的step\",\"contents\":[\"​ 在单个JobInstance的上下文中，如果某个Step已经处理完毕（COMPLETED）那么在默认情况下重启之后这个Step并不会再执行。可以通过设置allow-start-if-complete为true告知框架每次重启该Step都要执行：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(itemReader()) .writer(itemWriter()) .allowStartIfComplete(true) .build(); } \"]},{\"header\":\"配置略过逻辑\",\"slug\":\"配置略过逻辑\",\"contents\":[\"​ 某些时候在任务处理单个记录时中出现失败并不应该停止任务，而应该跳过继续处理下一条数据。是否跳过需要根据业务来判定，因此框架提供了跳过机制交给开发人员使用。如何配置跳过机制：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(flatFileItemReader()) .writer(itemWriter()) .faultTolerant() .skipLimit(10) .skip(FlatFileParseException.class) .build(); } \",\"代码的含义是当处理过程中抛出FlatFileParseException异常时就跳过该条记录的处理。skip-limit（skipLimit方法）配置的参数表示当跳过的次数超过数值时则会导致整个Step失败，从而停止继续运行。还可以通过反向配置的方式来忽略某些异常：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(flatFileItemReader()) .writer(itemWriter()) .faultTolerant() .skipLimit(10) .skip(Exception.class) .noSkip(FileNotFoundException.class) .build(); } \",\"skip表示要当捕捉到Exception异常就跳过。但是Exception有很多继承类，此时可以使用noSkip方法指定某些异常不能跳过。\"]},{\"header\":\"设置重试逻辑\",\"slug\":\"设置重试逻辑\",\"contents\":[\"当处理记录出个异常之后并不希望他立即跳过或者停止运行，而是希望可以多次尝试执行直到失败：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .faultTolerant() .retryLimit(3) .retry(DeadlockLoserDataAccessException.class) .build(); } \",\"retry(DeadlockLoserDataAccessException.class)表示只有捕捉到该异常才会重试，retryLimit(3)表示最多重试3次，faultTolerant()表示启用对应的容错功能。\"]},{\"header\":\"Step 中的事务\",\"slug\":\"step-中的事务\",\"contents\":[]},{\"header\":\"事物回滚控制\",\"slug\":\"事物回滚控制\",\"contents\":[\"​ 默认情况下，无论是设置了重试（retry）还是跳过（skip），只要从Writer抛出一个异常都会导致事物回滚。如果配置了skip机制，那么在Reader中抛出的异常不会导致回滚。有些从Writer抛出一个异常并不需要回滚数据，noRollback属性为Step提供了不必进行事物回滚的异常配置：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .faultTolerant() //不必回滚的异常 .noRollback(ValidationException.class) .build(); } \"]},{\"header\":\"事物数据读取的缓存\",\"slug\":\"事物数据读取的缓存\",\"contents\":[\"​ 一次Setp分为Reader、Processor 和Writer 三个阶段，这些阶段统称为Item。默认情况下如果错误不是发生在Reader阶段，那么没必要再去重新读取一次数据。但是某些场景下需要Reader部分也需要重新执行，比如Reader是从一个JMS队列中消费消息，当发生回滚的时候消息也会在队列上重放，因此也要将Reader纳入到回滚的事物中，根据这个场景可以使用readerIsTransactionalQueue 来配置数据重读：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .readerIsTransactionalQueue() //数据重读 .build(); } \"]},{\"header\":\"事物属性\",\"slug\":\"事物属性\",\"contents\":[\"事物的属性包括隔离等级（isolation）、传播方式（propagation）以及过期时间（timeout）。关于事物的控制详见Spring Data Access的说明，下面是相关配置的方法：\",\"@Bean public Step step1() { //配置事物属性 DefaultTransactionAttribute attribute = new DefaultTransactionAttribute(); attribute.setPropagationBehavior(Propagation.REQUIRED.value()); attribute.setIsolationLevel(Isolation.DEFAULT.value()); attribute.setTimeout(30); return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .transactionAttribute(attribute) //设置事物属性 .build(); } \"]},{\"header\":\"向Step注册 ItemStream\",\"slug\":\"向step注册-itemstream\",\"contents\":[\"​ ItemStream是用于每一个阶段（Reader、Processor、Writer）的”生命周期回调数据处理器“，后续的文章会详细介绍ItemStream。在4.×版本之后默认注入注册了通用的ItemStream。\",\"有2种方式将ItemStream注册到Step中，一是使用stream方法：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(compositeItemWriter()) .stream(fileItemWriter1()) .stream(fileItemWriter2()) .build(); } \",\"二是使用相关方法的代理：\",\"@Bean public CompositeItemWriter compositeItemWriter() { List<ItemWriter> writers = new ArrayList<>(2); writers.add(fileItemWriter1()); writers.add(fileItemWriter2()); CompositeItemWriter itemWriter = new CompositeItemWriter(); itemWriter.setDelegates(writers); return itemWriter; } \"]},{\"header\":\"StepExecution拦截器\",\"slug\":\"stepexecution拦截器\",\"contents\":[\"在Step执行的过程中会产生各种各样的事件，开发人员可以利用各种Listener接口对Step及Item进行监听。通常在创建一个Step的时候添加拦截器：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(reader()) .writer(writer()) .listener(chunkListener()) //添加拦截器 .build(); } \",\"Spring Batch提供了多个接口以满足不同事件的监听。\"]},{\"header\":\"StepExecutionListener\",\"slug\":\"stepexecutionlistener\",\"contents\":[\"StepExecutionListener可以看做一个通用的Step拦截器，他的作用是在Step开始之前和结束之后进行拦截处理：\",\"public interface StepExecutionListener extends StepListener { void beforeStep(StepExecution stepExecution); //Step执行之前 ExitStatus afterStep(StepExecution stepExecution); //Step执行完毕之后 } \",\"在结束的时候开发人员可以自己定义返回的ExitStatus，用于配合流程控制（见后文）实现对整个Step执行过程的控制。\"]},{\"header\":\"ChunkListener\",\"slug\":\"chunklistener\",\"contents\":[\"ChunkListener是在数据事物发生的两端被触发。chunk的配置决定了处理多少项记录才进行一次事物提交，ChunkListener的作用就是对一次事物开始之后或事物提交之后进行拦截：\",\"public interface ChunkListener extends StepListener { void beforeChunk(ChunkContext context); //事物开始之后，ItemReader调用之前 void afterChunk(ChunkContext context); //事物提交之后 void afterChunkError(ChunkContext context); //事物回滚之后 } \",\"如果没有设定chunk也可以使用ChunkListener，它会被TaskletStep调用（TaskletStep见后文）。\"]},{\"header\":\"ItemReadListener\",\"slug\":\"itemreadlistener\",\"contents\":[\"该接口用于对Reader相关的事件进行监控：\",\"public interface ItemReadListener<T> extends StepListener { void beforeRead(); void afterRead(T item); void onReadError(Exception ex); } \",\"beforeRead在每次Reader调用之前被调用，afterRead在每次Reader成功返回之后被调用，而onReadError会在出现异常之后被调用，可以将其用于记录异常日志。\"]},{\"header\":\"ItemProcessListener\",\"slug\":\"itemprocesslistener\",\"contents\":[\"ItemProcessListener和ItemReadListener类似，是围绕着ItemProcessor进行处理的：\",\"public interface ItemProcessListener<T, S> extends StepListener { void beforeProcess(T item); //processor执行之前 void afterProcess(T item, S result); //processor直线成功之后 void onProcessError(T item, Exception e); //processor执行出现异常 } \"]},{\"header\":\"ItemWriteListener\",\"slug\":\"itemwritelistener\",\"contents\":[\"ItemWriteListener的功能和ItemReadListener、ItemReadListener类似，但是需要注意的是它接收和处理的数据对象是一个List。List的长度与chunk配置相关。\",\"public interface ItemWriteListener<S> extends StepListener { void beforeWrite(List<? extends S> items); void afterWrite(List<? extends S> items); void onWriteError(Exception exception, List<? extends S> items); } \"]},{\"header\":\"SkipListener\",\"slug\":\"skiplistener\",\"contents\":[\"ItemReadListener、ItemProcessListener和ItemWriteListener都提供了错误拦截处理的机制，但是没有处理跳过（skip）的数据记录。因此框架提供了SkipListener来专门处理那么被跳过的记录：\",\"public interface SkipListener<T,S> extends StepListener { void onSkipInRead(Throwable t); //Read期间导致跳过的异常 void onSkipInProcess(T item, Throwable t); //Process期间导致跳过的异常 void onSkipInWrite(S item, Throwable t); //Write期间导致跳过的异常 } \",\"SkipListener的价值是可以将那些未能成功处理的记录在某个位置保存下来，然后交给其他批处理进一步解决，或者人工来处理。Spring Batch保证以下2个特征：\",\"跳过的元素只会出现一次。\",\"SkipListener始终在事物提交之前被调用，这样可以保证监听器使用的事物资源不会被业务事物影响。\"]},{\"header\":\"TaskletStep\",\"slug\":\"taskletstep\",\"contents\":[\"​ 面向分片（Chunk-oriented processing ）的过程并不是Step的唯一执行方式。比如用数据库的存储过程来处理数据，这个时候使用标准的Reader、Processor、Writer会很奇怪，针对这些情况框架提供了TaskletStep。 TaskletStep是一个非常简单的接口，仅有一个方法——execute。TaskletStep会反复的调用这个方法直到获取一个RepeatStatus.FINISHED返回或者抛出一个异常。所有的Tasklet调用都会包装在一个事物中。\",\"注册一个TaskletStep非常简单，只要添加一个实现了Tasklet接口的类即可：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .tasklet(myTasklet()) //注入Tasklet的实现 .build(); } \",\"TaskletStep还支持适配器处理等，详见官网说明。\"]},{\"header\":\"控制Step执行流程\",\"slug\":\"控制step执行流程\",\"contents\":[]},{\"header\":\"顺序执行\",\"slug\":\"顺序执行\",\"contents\":[\"顺序执行通过next方法来标记：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(stepA()) .next(stepB()) //顺序执行 .next(stepC()) .build(); } \"]},{\"header\":\"条件执行\",\"slug\":\"条件执行\",\"contents\":[\"在顺序执行的过程中，在整个执行链条中有一个Step执行失败则整个Job就会停止。但是通过条件执行，可以指定各种情况下的执行分。为了实现更加复杂的控制，可以通过Step执行后的退出命名来定义条件分之。先看一个简单的代码：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(stepA()) //启动时执行的step .on(\\\"*\\\").to(stepB()) //默认跳转到stepB //当返回的ExitStatus为\\\"FAILED\\\"时，执行。 .from(stepA()).on(\\\"FAILED\\\").to(stepC()) .end() .build(); } \",\"这里使用来表示默认处理，是一个通配符表示处理任意字符串，对应的还可以使用?表示匹配任意字符。在上文中介绍了Step的退出都会有ExitStatus，命名都来源于它。下面是一个更加全面的代码。\",\"配置拦截器处理ExitCode：\",\"public class SkipCheckingListener extends StepExecutionListenerSupport { public ExitStatus afterStep(StepExecution stepExecution) { String exitCode = stepExecution.getExitStatus().getExitCode(); if (!exitCode.equals(ExitStatus.FAILED.getExitCode()) && //当Skip的Item大于0时，则指定ExitStatus的内容 stepExecution.getSkipCount() > 0) { return new ExitStatus(\\\"COMPLETED WITH SKIPS\\\"); } else { return null; } } } \",\"拦截器指示当有一个以上被跳过的记录时，返回的ExitStatus为\\\"COMPLETED WITH SKIPS\\\"。对应的控制流程：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()).on(\\\"FAILED\\\").end() //执行失败直接退出 //有跳过元素执行 errorPrint1() .from(step1()).on(\\\"COMPLETED WITH SKIPS\\\").to(errorPrint1()) .from(step1()).on(\\\"*\\\").to(step2()) //默认（成功）情况下执行 Step2 .end() .build(); } \"]},{\"header\":\"Step的停机退出机制\",\"slug\":\"step的停机退出机制\",\"contents\":[\"​ Spring Batch为Job提供了三种退出机制，这些机制为批处理的执行提供了丰富的控制方法。在介绍退出机制之前需要回顾一下 数据批处理概念一文中关于StepExecution的内容。在StepExecution中有2个表示状态的值，一个名为status，另外一个名为exitStatus。前者也被称为BatchStatus。\",\"​ 前面以及介绍了ExitStatus的使用，他可以控制Step执行链条的条件执行过程。除此之外BatchStatus也会参与到过程的控制。\"]},{\"header\":\"End退出\",\"slug\":\"end退出\",\"contents\":[\"默认情况下（没有使用end、fail方法结束），Job要顺序执行直到退出，这个退出称为end。这个时候，BatchStatus=COMPLETED、ExitStatus=COMPLETED，表示成功执行。\",\"除了Step链式处理自然退出，也可以显示调用end来退出系统。看下面的例子：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) //启动 .next(step2()) //顺序执行 .on(\\\"FAILED\\\").end() .from(step2()).on(\\\"*\\\").to(step3()) //条件执行 .end() .build(); } \",\"上面的代码，step1到step2是顺序执行，当step2的exitStatus返回\\\"FAILED\\\"时则直接End退出。其他情况执行Step3。\"]},{\"header\":\"Fail退出\",\"slug\":\"fail退出\",\"contents\":[\"除了end还可以使用fail退出，这个时候，BatchStatus=FAILED、ExitStatus=EARLY TERMINATION，表示执行失败。这个状态与End最大的区别是Job会尝试重启执行新的JobExecution。看下面代码的例子：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) //执行step1 .next(step2()).on(\\\"FAILED\\\").fail() //step2的ExitStatus=FAILED 执行fail .from(step2()).on(\\\"*\\\").to(step3()) //否则执行step3 .end() .build(); } \"]},{\"header\":\"在指定的节点中断\",\"slug\":\"在指定的节点中断\",\"contents\":[\"Spring Batch还支持在指定的节点退出，退出后下次重启会从中断的点继续执行。中断的作用是某些批处理到某个步骤后需要人工干预，当干预完之后又接着处理：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") //如果step1的ExitStatus=COMPLETED则在step2中断 .start(step1()).on(\\\"COMPLETED\\\").stopAndRestart(step2()) //否则直接退出批处理 .end() .build(); } \"]},{\"header\":\"程序化流程的分支\",\"slug\":\"程序化流程的分支\",\"contents\":[\"可以直接进行编码来控制Step之间的扭转，Spring Batch提供了JobExecutionDecider接口来协助分支管理：\",\"public class MyDecider implements JobExecutionDecider { public FlowExecutionStatus decide(JobExecution jobExecution, StepExecution stepExecution) { String status; if (someCondition()) { status = \\\"FAILED\\\"; } else { status = \\\"COMPLETED\\\"; } return new FlowExecutionStatus(status); } } \",\"接着将MyDecider作为过滤器添加到配置过程中：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) .next(decider()).on(\\\"FAILED\\\").to(step2()) .from(decider()).on(\\\"COMPLETED\\\").to(step3()) .end() .build(); } \"]},{\"header\":\"流程分裂\",\"slug\":\"流程分裂\",\"contents\":[\"在线性处理过程中，流程都是一个接着一个执行的。但是为了满足某些特殊的需要，Spring Batch提供了执行的过程分裂并行Step的方法。参看下面的Job配置：\",\"@Bean public Job job() { Flow flow1 = new FlowBuilder<SimpleFlow>(\\\"flow1\\\") .start(step1()) .next(step2()) .build();//并行流程1 Flow flow2 = new FlowBuilder<SimpleFlow>(\\\"flow2\\\") .start(step3()) .build();//并行流程2 return this.jobBuilderFactory.get(\\\"job\\\") .start(flow1) .split(new SimpleAsyncTaskExecutor()) //创建一个异步执行任务 .add(flow2) .next(step4()) //2个分支执行完毕之后再执行step4。 .end() .build(); } \",\"这里表示flow1和flow2会并行执行，待2者执行成功后执行step4。\"]},{\"header\":\"数据绑定\",\"slug\":\"数据绑定\",\"contents\":[\"在Job或Step的任何位置，都可以获取到统一配置的数据。比如使用标准的Spring Framework方式：\",\"@Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"${input.file.name}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"当我们通过配置文件（application.properties中 input.file.name=filepath）或者jvm参数（-Dinput.file.name=filepath）指定某些数据时，都可以通过这种方式获取到对应的配置参数。\",\"此外，也可以从JobParameters从获取到Job运行的上下文参数：\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobParameters['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"无论是JobExecution还是StepExecution，其中的内容都可以通过这种方式去获取参数，例如：\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobExecutionContext['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"或者\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{stepExecutionContext['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \"]},{\"header\":\"Step Scope\",\"slug\":\"step-scope\",\"contents\":[\"注意看上面的代码例子，都有一个@StepScope注解。这是为了进行后期绑定进行的标识。因为在Spring的IoCs容器进行初始化的阶段并没有任何的*Execution在执行，进而也不存在任何*ExecutionContext，所以这个时候根本无法注入标记的数据。所以需要使用注解显式的告诉容器直到Step执行的阶段才初始化这个@Bean。\"]},{\"header\":\"Job Scope\",\"slug\":\"job-scope\",\"contents\":[\"Job Scope的概念和 Step Scope类似，都是用于标识在到了某个执行时间段再添加和注入Bean。@JobScope用于告知框架知道JobInstance存在时候才初始化对应的@Bean：\",\"@JobScope @Bean // 初始化获取 jobParameters中的参数 public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobParameters[input]}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"@JobScope @Bean // 初始化获取jobExecutionContext中的参数 public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobExecutionContext中的参数['input.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring.html\":{\"title\":\"Spring 小技巧\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"收集 Spring 未归类的小知识\"]},{\"header\":\"Spring 依赖注入\",\"slug\":\"spring-依赖注入\",\"contents\":[]},{\"header\":\"注入类\",\"slug\":\"注入类\",\"contents\":[\"Spring 两种方式可以注入类：\",\"在私有属性上加注解 @Autowired\",\"@Autowired private ITestService testService; \",\"@Autowired 还支持构造方法注入，在构造方法上加上该注解，就可，支持多个参数构造注入，Spring 官方更推荐。\",\"@Autowired public DataRecordServiceImplTest(IDataRecordService recordService){ this.recordService = recordService; } \"]},{\"header\":\"注入父类\",\"slug\":\"注入父类\",\"contents\":[\"当需要注入某个类的所有子类时，可以通过Map方式注入\",\"@Autowired private Map<String, IParentService> dictServiceMap; \",\"这样就能注入 IParentService 的所有子类\",\"这里 key 不会和子类名称完全一致，是截取前n个字符。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Java\",\"Spring\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81.html\":{\"title\":\"Spring 数据验证\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"​ Spring Validation 是对 Hibernate Validation进行了二次封装，在Spring MVC 模块中添加了自动校验，并将校验信息封装进了特定的类中。通过框架，可以轻松完成Spring 的校验。\"]},{\"header\":\"准备工作\",\"slug\":\"准备工作\",\"contents\":[\"使用 Spring Validation，需要导入下面这个依赖包\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-validation</artifactId> <version>${spring-boot.version}</version> </dependency> \"]},{\"header\":\"使用\",\"slug\":\"使用\",\"contents\":[]},{\"header\":\"默认校验\",\"slug\":\"默认校验\",\"contents\":[\"如果使用默认注解进行校验，无需使用其他配置，下面是常用的校验注解：\",\"注解\",\"说明\",\"@Null\",\"被注释的元素必须为null\",\"@NotNull\",\"被注释的元素必须不为null\",\"@AssertTrue\",\"被注释的元素必须为true\",\"@AssertFalse\",\"被注释的元素必须为false\",\"@Min(value)\",\"被注释的元素必须是一个数字，其值必须大于等于指定的最小值\",\"@Max(value)\",\"被注释的元素必须是一个数字，其值必须小于等于指定的最大值\",\"@DecimalMin(value)\",\"被注释的元素必须是一个数字，其值必须大于等于指定的最小值\",\"@DecimalMax(value)\",\"被注释的元素必须是一个数字，其值必须小于等于指定的最大值\",\"@Size(max, min)\",\"被注释的元素的大小必须在指定的范围内\",\"@Digits (integer, fraction)\",\"被注释的元素必须是一个数字，其值必须在可接受的范围内\",\"@Past\",\"被注释的元素必须是一个过去的日期\",\"@Future\",\"被注释的元素必须是一个将来的日期\",\"@Pattern(value)\",\"被注释的元素必须符合指定的正则表达式\",\"@Email\",\"被注释的元素必须是电子邮箱地址\",\"@Length\",\"被注释的字符串的大小必须在指定的范围内\",\"@NotEmpty\",\"被注释的字符串的必须非空\",\"@Range\",\"被注释的元素必须在合适的范围内\",\"使用内置默认注解校验，需要在对应属性上加上注解，同时在需要校验的接口上使用Valid 注解，在接口所在类加上Validated 注解。\",\"import javax.validation.Valid; import org.springframework.validation.annotation.Validated; @Validated public interface IPersonService { void savePerson(@Valid Person person); } \",\"内置校验注解支持对返回的消息进行自定义 message 返回错误信息，帮助异常捕捉更好的获取信息。多重校验还可以加不同注解来实现。\",\"@Data public class Person { @NotNull(message = \\\"身份证不能为空\\\") @Size(message = \\\"身份证不正确\\\") private String idCard; @Size(min = 2) private String name; private String phone; @NotNull @Email private String email; private Date birthDate; /** * true 男性， false 女性 */ private boolean sex; } \",\"如果有多个参数需要校验，形式可以如下，即一个校验类对应一个校验结果。\",\"void funct(@Validated Person person, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult); \"]},{\"header\":\"自定义校验\",\"slug\":\"自定义校验\",\"contents\":[\"Spring Validation 自定义校验需要一些配置，创建自己的校验注解和校验实现\",\"添加自己的校验注解：\",\"@Documented //此处填写校验实现类 @Constraint(validatedBy = ValidPersonValidator.class) @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) public @interface ValidPerson { // 默认返回信息 String message() default \\\"\\\"; // 必要信息 Class<?>[] groups() default {}; Class<? extends Payload>[] payload() default {}; } \",\"添加自己的校验实现\",\"// ValidPerson为校验注解 // Person需要校验的类 public class ValidPersonValidator implements ConstraintValidator<ValidPerson, Person> { @SneakyThrows @Override public boolean isValid(Person person, ConstraintValidatorContext context){ String msg = \\\"\\\"; if(person.getIdCard() == null || person.getIdCard().length() != 18){ msg = \\\"身份证有误\\\"; // 取消多余message显示 context.disableDefaultConstraintViolation(); // 自定义返回msg context.buildConstraintViolationWithTemplate(msg).addConstraintViolation(); return false; } return true; } } \"]}],\"customFields\":{\"1\":[\"Java\",\"Spring\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring%E7%8A%B6%E6%80%81%E6%9C%BA.html\":{\"title\":\"Spring 状态机\",\"contents\":[{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"Spring 状态机官方文档 (spring.io)\",\"Spring Statemachine 状态机初探 - 简书 (jianshu.com)\",\"Spring StateMachine - 代码天地 (codetd.com)\",\"状态机这一概念比 Java 出现的都要早，Spring 在这一模型的基础上，做出 Spring 状态机 框架。\"]},{\"header\":\"有限状态机\",\"slug\":\"有限状态机\",\"contents\":[\"有限状态机：简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。应用FSM模型可以帮助对象生命周期的状态的顺序以及导致状态变化的事件进行管理。将状态和事件控制从不同的业务Service方法的if else中抽离出来。FSM的应用范围很广，对于有复杂状态流，扩展性要求比较高的场景都可以使用该模型。 下面是状态机模型中的4个要素，即现态、条件、动作、次态。 \",\"现态：是指当前所处的状态。\",\"条件：又称为“事件”。当一个条件被满足，将会触发一个动作，或者执行一次状态的迁移。\",\"动作：条件满足后执行的动作。动作执行完毕后，可以迁移到新的状态，也可以仍旧保持原状态。动作不是必需的，当条件满足后，也可以不执行任何动作，直接迁移到新状态。\",\"次态：条件满足后要迁往的新状态。“次态”是相对于“现态”而言的，“次态”一旦被激活，就转变成新的“现态”了。\"]},{\"header\":\"状态机 DEMO\",\"slug\":\"状态机-demo\",\"contents\":[\"使用 Spring 状态机要先导入Starter包\",\"<!--spring statemachine--> <dependency> <groupId>org.springframework.statemachine</groupId> <artifactId>spring-statemachine-core</artifactId> <version>${state-machine.version}</version> </dependency> \",\"编写相关 Java 配置文件\",\"@Configuration // 开启状态机 @EnableStateMachine(name = \\\"Order\\\") public class StateMachineConfig extends EnumStateMachineConfigurerAdapter<OrderState, OrderEvent> { // 状态机初始化 @Override public void configure(StateMachineStateConfigurer<OrderState, OrderEvent> states) throws Exception { states.withStates().initial(UNCREATE).states(EnumSet.allOf(OrderState.class)); } // 编写状态机触发流程 @Override public void configure(StateMachineTransitionConfigurer<OrderState, OrderEvent> transitions) throws Exception { transitions.withExternal().source(UNCREATE).target(CREATE).event(CREATE_ORDER).and() .withExternal().source(CREATE).target(PAIED).event(PAY).and() .withExternal().source(PAIED).target(HARVESTED).event(SEND).and() .withExternal().source(HARVESTED).target(FINISH).event(CONFIRM); } } \",\"状态机流程的相关写法：\",\"withExternal 是当source和target不同时的写法，比如付款成功后状态发生的变化。\",\"withInternal 当source和target相同时的串联写法，比如付款失败后都是待付款状态。\",\"withExterna l的source和target用于执行前后状态、event为触发的事件、guard判断是否执行action。同时满足source、target、event、guard的条件后执行最后的action。\",\"withChoice 当执行一个动作，可能导致多种结果时，可以选择使用choice+guard来跳转\",\"withChoice根据guard的判断结果执行first/then的逻辑。\",\"withChoice不需要发送事件来进行触发。\",\"这里还需要使用两个枚举来表示状态和触发事件\",\"/** * 订单状态 */ public enum OrderState { // 未创建 UNCREATE, // 创建,待支付 CREATE, // 已支付，待发货 PAIED, // 待收货 HARVESTED, // 完成 FINISH; } \",\"/** * 订单事件 */ public enum OrderEvent { // 创建订单 CREATE_ORDER, // 支付 PAY, // 发货 SEND, // 确认收货 CONFIRM; } \"]},{\"header\":\"简单状态机\",\"slug\":\"简单状态机\",\"contents\":[\"要保持状态机能够恢复读取，需要将状态机持久化\",\"/** * 状态机持久化 */ @Component public class OrderStateMachinePersist implements StateMachinePersist<OrderState, OrderEvent, OrderState> { @Override public void write(StateMachineContext<OrderState, OrderEvent> stateMachineContext, OrderState orderState) throws Exception { // 默认不持久化 } @Override public StateMachineContext<OrderState, OrderEvent> read(OrderState currentState) throws Exception { return new DefaultStateMachineContext<>(currentState, null, null, null); } } \",\"/** * 注入状态机状态持久化到 Spring容器 */ @Configuration public class Config { @Autowired private OrderStateMachinePersist stateMachinePersist; @Bean public StateMachinePersister<OrderState, OrderEvent, OrderState> getPersist(){ return new DefaultStateMachinePersister<>(stateMachinePersist); } } \",\"在简单状态流当中，我们设定一个简单的订单场景，订单的状态由状态机管理\",\"然后编写业务流程，运行代码，即完成这样一个简单的 Spring 状态机\",\"@Slf4j @Service public class OrderService { @Resource private StateMachine<OrderState, OrderEvent> stateMachine; @Autowired private StateMachinePersister<OrderState, OrderEvent, OrderState> machinePersister; // 创建订单 public void createOrder(){ Message message = MessageBuilder.withPayload(OrderEvent.CREATE_ORDER).setHeader(\\\"order\\\", \\\"1\\\").build(); sendEvent(message, OrderState.UNCREATE); } // 支付 public void payed(){ Message message = MessageBuilder.withPayload(OrderEvent.PAY).setHeader(\\\"order\\\", \\\"1\\\").build(); sendEvent(message, OrderState.CREATE); } @SneakyThrows private void sendEvent(Message message, OrderState currentState){ stateMachine.start(); // 恢复状态机到当前订单状态，可由我们控制（该方法实际是重新设置状态机实例状态） machinePersister.restore(stateMachine, currentState); log.info(\\\"事件前状态:{}\\\", stateMachine.getState().getId()); stateMachine.sendEvent(message); log.info(\\\"事件后状态:{}\\\", stateMachine.getState().getId()); stateMachine.stop(); } } \"]},{\"header\":\"触发事件\",\"slug\":\"触发事件\",\"contents\":[\"​ 前面提到如果让状态机状态发生变化，实际业务，状态的变更，也会涉及一系列数据的变更， Spring 状态机提供状态变更监听，来触发我们对应的事件。\",\"@Service // 监听绑定对应的状态机 @WithStateMachine(name = \\\"Order\\\") public class OrderStateService { @SneakyThrows // 监听对应注解 @OnTransition(source = \\\"UNCREATE\\\", target = \\\"CREATE\\\") public void createOrder(Message<OrderEvent> message){ log.info(\\\"创建订单：{}\\\", message.getPayload()); } } \",\"@OnTransition 注解能够监听状态变化，触发对应事件，通过 Message 参数，可以获取状态变更传递过来的值。\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/web%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98.html\":{\"title\":\"Web序列化器加载顺序问题\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在项目中，配置多序列化转换，有多种方式实现，使用不同方式的Spring 底层实现，导致序列化器的选择结果有很大区别，我遇到问题，导致fastjson序列化没有正确的加载，也没有走jackjson的自定义序列化\",\"我的fastjson和jackjson都做了自定义处理，jackjson优先，fastjson其次\",\"开始是这样，我的WebMvc配置继承了 WebMvcConfigurationSupport 类，但是一些需要，我需要切换为实现 WebMvcConfigurer 接口，进行编写代码时，实际并没有走我的自定义序列化方式，jackjson和fastjson，都没有被调用。\",\"通过调试，我找到了下面这段代码\",\"package org.springframework.web.servlet.mvc.method.annotation; ... protected <T> void writeWithMessageConverters(@Nullable T value, MethodParameter returnType, ServletServerHttpRequest inputMessage, ServletServerHttpResponse outputMessage) throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException { ... for (HttpMessageConverter<?> converter : this.messageConverters) { GenericHttpMessageConverter genericConverter = (converter instanceof GenericHttpMessageConverter ? (GenericHttpMessageConverter<?>) converter : null); if (genericConverter != null ? ((GenericHttpMessageConverter) converter).canWrite(targetType, valueType, selectedMediaType) : converter.canWrite(valueType, selectedMediaType)) { body = getAdvice().beforeBodyWrite(body, returnType, selectedMediaType, (Class<? extends HttpMessageConverter<?>>) converter.getClass(), inputMessage, outputMessage); if (body != null) { Object theBody = body; LogFormatUtils.traceDebug(logger, traceOn -> \\\"Writing [\\\" + LogFormatUtils.formatValue(theBody, !traceOn) + \\\"]\\\"); addContentDispositionHeader(inputMessage, outputMessage); if (genericConverter != null) { genericConverter.write(body, targetType, selectedMediaType, outputMessage); } else { ((HttpMessageConverter) converter).write(body, selectedMediaType, outputMessage); } } ... ... \",\"这段代码，是调用序列化转换，输出序列化结果的地方，断点这儿发现默认使用的是 MappingJackson2HttpMessageConverter 序列化转换器，这就奇怪了，为什么继承WebMvcConfigurationSupport 类并没有使用这个默认的序列化器。\",\"通过监听一个List<HttpMessageConverter<?>> converters 属性发现，实现 WebMvcConfigurer 接口，会多加载很多自带的序列化转换器，而且这些默认的序列化转换器，优先级很高，在上述代码中，找到第一个合适的转换器，就会返回调用，这才导致我的两种自定义序列化方式，没有生效。\"]},{\"header\":\"解决方法\",\"slug\":\"解决方法\",\"contents\":[\"找到原因，解决就简单了，我在WebMvc配置类中，将自定义序列化器排序放在最前面即可，这样我的序列化器优先级就会很高了\",\"@Override public void configureMessageConverters(List<HttpMessageConverter<?>> converters) { CostJackson2HttpMessageConverter costJackson2HttpMessageConverter = new CostJackson2HttpMessageConverter(); converters.add(0, costJackson2HttpMessageConverter); converters.add(1, fastJsonHttpMessageConverter()); } \"]}],\"customFields\":{\"1\":[\"java\",\"Spring Web\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/%E9%83%A8%E7%BD%B2%E5%88%B0Docker.html\":{\"title\":\"部署到Docker\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"将Spring Boot 项目部署打包成Docker 镜像文件，可以很方便部署和移植，有两种方式将Java 项目打包成Docker镜像，Dockerfile 和 Docker-Compose。\",\"Dockerfile适用于定义单个容器的构建过程，通常用于构建和部署单个应用。dockerfile的产物是镜像\",\"Docker Compose是一个工具，用于定义和管理多个Docker容器的配置。Docker Compose可以一次性启动、停止和管理多个容器，可以通过简单的命令来管理整个应用的生命周期。compose的产物是容器，容器是一个镜像的实例。\",\"下面详细介绍两种方式的操作\"]},{\"header\":\"Dockerfile\",\"slug\":\"dockerfile\",\"contents\":[]},{\"header\":\"操作步骤\",\"slug\":\"操作步骤\",\"contents\":[\"我们需要准备以下工作\",\"在pom.xml创建一个Dockerfile（用于描述Docker打包成镜像的相关操作）\",\"将项目文件提交到可以运行docker的环境中，执行命令，生成镜像\"]},{\"header\":\"Dockerfile语法说明\",\"slug\":\"dockerfile语法说明\",\"contents\":[\"Dockerfile是一个纯文本文件，通过编写指令配置相应的参数\"]},{\"header\":\"FROM\",\"slug\":\"from\",\"contents\":[\"两种形式如下：\",\"FROM <IMAGE> FROM <IMAGE>:<TAG> \",\"通过FROM指定的镜像名称必须是一个已经存在的镜像，这个镜像称之为基础镜像，必须位于第一条非注释指令\"]},{\"header\":\"MAINTAINER\",\"slug\":\"maintainer\",\"contents\":[\"MAINTAINER <NAME> \",\"指定镜像的作者信息，包含镜像的所有者和联系人信息\"]},{\"header\":\"RUN\",\"slug\":\"run\",\"contents\":[\"用于指定构建镜像时运行的命令，两种模式：\",\"RUN <command> (shell模式) RUN [ \\\"executable\\\", \\\"param1\\\", \\\"param2\\\" ] (exec模式) \",\"在shell模式下，是使用/bin/sh -c COMMAND来运行命令的 在exec模式下可以指定其他的shell来运行命令RUN [“/bin/bash”, “-c”, “echo hello”]\",\"多条RUN指令可以合并为一条：\",\"RUN yum install httpd && yum install ftp \",\"这样在构建的时候会减少产生中间层镜像\"]},{\"header\":\"EXPOSE\",\"slug\":\"expose\",\"contents\":[\"指定运行该镜像的容器使用的端口，可以是多个。\",\"EXPOSE <PORT> \",\"使用这个指令的目的是告诉应用程序容器内应用程序会使用的端口，在运行时还需要使用-p参数指定映射端口。这是docker处于安全的目的，不会自动打开端口。\",\"docker run -p 80 -d dockertest/dockerfile_build nginx -g \\\"daemon off\\\" \"]},{\"header\":\"CMD\",\"slug\":\"cmd\",\"contents\":[\"用于提供容器运行的默认命令，如果在docker run时指定了运行的命令，则CMD命令不会执行。CMD有三种模式：\",\"CMD <command> (shell模式) CMD [ \\\"executable\\\", \\\"param1\\\", \\\"param2\\\" ] (exec模式) CMD [ 'param1', 'param2'] (通常与ENTRYPOINT搭配指定ENTRYPOINT的默认参数) \"]},{\"header\":\"ENTRYPOINT\",\"slug\":\"entrypoint\",\"contents\":[\"与CMD类似，ENTRYPOINT不会被docker run中指定的命令覆盖，如果想覆盖ENTRYPOINT，则需要在docker run中指定--entrypoint选项\",\"它有两种模式：\",\"ENTRYPOINT <command> (shell模式) ENTRYPOINT [ \\\"executable\\\", \\\"param1\\\", \\\"param2\\\" ] (exec模式) \"]},{\"header\":\"ADD和COPY\",\"slug\":\"add和copy\",\"contents\":[\"作用都是将文件或目录复制到Dockerfile构建的镜像中\",\"ADD <src> <dest> ADD [\\\"<src>\\\" \\\"<dest>\\\"] (适用于文件路径包含空格的情况) COPY <src> <dest> ADD [\\\"<src>\\\" \\\"<dest>\\\"] (适用于文件路径包含空格的情况) \",\"ADD包含了类似tar的解压功能，如果只是单纯复制文件，建议使用COPY，而且，两者的源文件路径使用Dockerfile相对路径，目标路径使用绝对路径。\",\"COPY index.html /var/www/html \"]},{\"header\":\"VOLUME\",\"slug\":\"volume\",\"contents\":[\"用于向容器添加卷，可以提供共享存储等功能\",\"VOLUME ['/data'] \"]},{\"header\":\"WORKDIR\",\"slug\":\"workdir\",\"contents\":[\"在容器内部设置工作目录，这样ENTRYPOINT和CMD指定的命令都会在容器中这个目录下进行。\",\"WORKDIR /path/to/workdir \"]},{\"header\":\"ENV\",\"slug\":\"env\",\"contents\":[\"用于设置环境变量\",\"ENV <KEY> <VALUE> ENV <KEY>=<VALUE> \"]},{\"header\":\"USER\",\"slug\":\"user\",\"contents\":[\"用于指定镜像为什么用户去运行\",\"USER nginx \",\"镜像就会以nginx身份运行，可以使用uid，gid等各种组合使用\"]},{\"header\":\"ONBUILD\",\"slug\":\"onbuild\",\"contents\":[\"为镜像创建触发器，当一个镜像被用作其他镜像的基础镜像时，这个触发器会被执行。当子镜像被构建时会插入触发器中的指令。\",\"ONBUILD COPY index.html /var/www/html \"]},{\"header\":\"一个示例\",\"slug\":\"一个示例\",\"contents\":[\"# 该镜像需要依赖的基础镜像 FROM openjdk:8 # 将当前目录下的jar包复制到docker容器的/目录下 ADD ./spring-boot.jar /spring-boot.jar # 声明服务运行在8080端口 EXPOSE 8080 # 指定docker容器启动时运行jar包 ENTRYPOINT [\\\"java\\\", \\\"-jar\\\",\\\"/spring-boot.jar\\\"] # 指定维护者的名字 MAINTAINER demo \"]},{\"header\":\"Dockerfile的构建过程\",\"slug\":\"dockerfile的构建过程\",\"contents\":[\"docker会从Dockerfile文件头FROM指定的基础镜像运行一个容器\",\"然后执行一条指令，对容器修改\",\"接着执行类似docker commit的操作，创建新的镜像层\",\"在基于刚创建的镜像运行一个新的容器\",\"执行Dockerfile下一条指令，直到所有指令执行完毕\",\"docker会删除中间层创建的容器，但不会删除中间层镜像，所以可以使用docker run运行一个中间层容器，从而查看每一步构建后的镜像状态，这样就可以进行调试。\"]},{\"header\":\"构建缓存\",\"slug\":\"构建缓存\",\"contents\":[\"docker在构建过程中会将之前构建的镜像看做缓存。\",\"当第一次构建的时候，构建过程会比较慢，而在此进行相同的构建的时候，会看见using cache字样，表示使用了缓存，构建过程也非常快。\",\"如果不想使用构建缓存，则在docker build中使用—no-cache选项。\",\"还可以在Dockerfile中使用ENV REFRESH_DATE 2018-01-01来制定缓存刷新时间，更改这个时间，就会让后面的命令不使用缓存。\"]},{\"header\":\"Docker-Compose\",\"slug\":\"docker-compose\",\"contents\":[\"使用Docker-Compose，不仅可以创建项目对应的镜像，还可以生成对应的容器，启动相关中间件，都可以已yml文件形式管理\"]},{\"header\":\"操作步骤\",\"slug\":\"操作步骤-1\",\"contents\":[\"Docker-Compose首先要配置Dokcerfile，配置项目本身的镜像相关，然后配置docker-compose.yml文件，说明启动哪些线管容器\"]},{\"header\":\"语法说明\",\"slug\":\"语法说明\",\"contents\":[]},{\"header\":\"1.image\",\"slug\":\"_1-image\",\"contents\":[\"指定为镜像名称或镜像ID。\",\"如果镜像不存在，Compose将尝试从互联网拉取这个镜像，例如： image: ubuntu image: orchardup/postgresql image: a4bc65fd\",\"指定服务的镜像名，若本地不存在，则 Compose 会去仓库拉取这个镜像:\",\"services: web: image: nginx \"]},{\"header\":\"2.build\",\"slug\":\"_2-build\",\"contents\":[\"指定Dockerfile所在文件夹的路径。Compose将会利用他自动构建这个镜像，然后使用这个镜像。 build: ./dir\"]},{\"header\":\"3.command\",\"slug\":\"_3-command\",\"contents\":[\"覆盖容器启动后默认执行的命令。 command: bundle exec thin -p 3000\"]},{\"header\":\"4.links\",\"slug\":\"_4-links\",\"contents\":[\"链接到其他服务容器，使用服务名称(同时作为别名)或服务别名（SERVICE:ALIAS）都可以\",\"links: - db - db:database - redis \",\"注意：使用别名会自动在服务器中的/etc/hosts 里创建，如：172.17.2.186 db，相应的环境变量也会被创建。\"]},{\"header\":\"5.external_links\",\"slug\":\"_5-external-links\",\"contents\":[\"链接到docker-compose.yml外部的容器，甚至并非是Compose管理的容器。参数格式和links类似。 external_links:\",\"- redis_1 - project_db_1:mysql - project_db_2:sqlserver \"]},{\"header\":\"6.ports\",\"slug\":\"_6-ports\",\"contents\":[\"暴露端口信息。格式\",\"宿主机器端口：容器端口（HOST:CONTAINER）\",\"或者仅仅指定容器的端口（宿主机器将会随机分配端口）都可以。\",\"ports: - \\\"3306\\\" - \\\"8080:80\\\" - \\\"127.0.0.1:8090:8001\\\" \",\"注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 你可能会得到错误得结果，因为 YAML 将会解析 xx:yy 这种数字格式为 60 进制。所以建议采用字符串格式。\"]},{\"header\":\"7.expose\",\"slug\":\"_7-expose\",\"contents\":[\"暴露端口，与posts不同的是expose只可以暴露端口而不能映射到主机，只供外部服务连接使用；仅可以指定内部端口为参数。\",\"expose: - \\\"3000\\\" - \\\"8000\\\" \"]},{\"header\":\"8.volumes\",\"slug\":\"_8-volumes\",\"contents\":[\"设置卷挂载的路径。\",\"可以设置宿主机路径:容器路径（host:container）或加上访问模式（host:container:ro）ro就是readonly的意思，只读模式。\",\"volumes: - /var/lib/mysql:/var/lib/mysql - /configs/mysql:/etc/configs/:ro \"]},{\"header\":\"9.volunes_from\",\"slug\":\"_9-volunes-from\",\"contents\":[\"挂载另一个服务或容器的所有数据卷。\",\"volumes_from: - service_name - container_name \"]},{\"header\":\"10.environment\",\"slug\":\"_10-environment\",\"contents\":[\"设置环境变量。可以属于数组或字典两种格式。\",\"如果只给定变量的名称则会自动加载它在Compose主机上的值，可以用来防止泄露不必要的数据。\",\"environment: - RACK_ENV=development - SESSION_SECRET \"]},{\"header\":\"11.env_file\",\"slug\":\"_11-env-file\",\"contents\":[\"从文件中获取环境变量，可以为单独的文件路径或列表。 如果通过docker-compose -f FILE指定了模板文件，则env_file中路径会基于模板文件路径。 如果有变量名称与environment指令冲突，则以后者为准。\",\"env_file: .env env_file: - ./common.env - ./apps/web.env - /opt/secrets.env \"]},{\"header\":\"12.extends\",\"slug\":\"_12-extends\",\"contents\":[\"基于已有的服务进行服务扩展。例如我们已经有了一个webapp服务，模板文件为common.yml.\",\"# common.yml webapp: build: ./webapp environment: - DEBUG=false - SEND_EMAILS=false \",\"编写一个新的 development.yml 文件，使用 common.yml 中的 webapp 服务进行扩展。 development.yml\",\"web: extends: file: common.yml service: webapp: ports: - \\\"8080:80\\\" links: - db envelopment: - DEBUG=true db: image: mysql:5.7 \"]},{\"header\":\"13.net\",\"slug\":\"_13-net\",\"contents\":[\"设置网络模式。使用和docker client 的 --net 参数一样的值。\",\"# 容器默认连接的网络，是所有Docker安装时都默认安装的docker0网络. net: \\\"bridge\\\" # 容器定制的网络栈. net: \\\"none\\\" # 使用另一个容器的网络配置 net: \\\"container:[name or id]\\\" # 在宿主网络栈上添加一个容器，容器中的网络配置会与宿主的一样 net: \\\"host\\\" \",\"Docker会为每个节点自动创建三个网络： 网络名称 作用 bridge 容器默认连接的网络，是所有Docker安装时都默认安装的docker0网络 none 容器定制的网络栈 host 在宿主网络栈上添加一个容器，容器中的网络配置会与宿主的一样 附录： 操作名称 命令 创建网络 docker network create -d bridge mynet 查看网络列表 docker network ls\"]},{\"header\":\"14.pid\",\"slug\":\"_14-pid\",\"contents\":[\"和宿主机系统共享进程命名空间，打开该选项的容器可以相互通过进程id来访问和操作。\",\"pid: \\\"host\\\" \"]},{\"header\":\"15.dns\",\"slug\":\"_15-dns\",\"contents\":[\"配置DNS服务器。可以是一个值，也可以是一个列表。 dns: 8.8.8.8 dns: - 8.8.8.8 - 9.9.9.9 \"]},{\"header\":\"16.cap_add，cap_drop\",\"slug\":\"_16-cap-add-cap-drop\",\"contents\":[\"添加或放弃容器的Linux能力（Capability）。\",\"cap_add: - ALL cap_drop: - NET_ADMIN - SYS_ADMIN \"]},{\"header\":\"17.dns_search\",\"slug\":\"_17-dns-search\",\"contents\":[\"配置DNS搜索域。可以是一个值也可以是一个列表。\",\"dns_search: example.com dns_search: - domain1.example.com \\\\ - domain2.example.com working_dir, entrypoint, user, hostname, domainname, mem_limit, privileged, restart, stdin_open, tty, cpu_shares \",\"这些都是和 docker run 支持的选项类似。\",\"cpu_shares: 73 working_dir: /code entrypoint: /code/entrypoint.sh user: postgresql hostname: foo domainname: foo.com mem_limit: 1000000000 privileged: true restart: always stdin_open: true tty: true \"]},{\"header\":\"18.healthcheck\",\"slug\":\"_18-healthcheck\",\"contents\":[\"健康检查，这个非常有必要，等服务准备好以后再上线，避免更新过程中出现短暂的无法访问。\",\"healthcheck: test: [\\\"CMD\\\", \\\"curl\\\", \\\"-f\\\", \\\"http://localhost/alive\\\"] interval: 5s timeout: 3s \",\"其实大多数情况下健康检查的规则都会写在 Dockerfile 中:\",\"FROM nginx RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/* HEALTHCHECK --interval=5s --timeout=3s CMD curl -f http://localhost/alive || exit 1 \"]},{\"header\":\"19.depends_on\",\"slug\":\"_19-depends-on\",\"contents\":[\"依赖的服务，优先启动，例：\",\"depends_on: - redis \"]},{\"header\":\"20.deploy\",\"slug\":\"_20-deploy\",\"contents\":[\"部署相关的配置都在这个节点下，例：\",\"deploy: mode: replicated replicas: 2 restart_policy: condition: on-failure max_attempts: 3 update_config: delay: 5s order: start-first # 默认为 stop-first，推荐设置先启动新服务再终止旧的 resources: limits: cpus: \\\"0.50\\\" memory: 1g deploy: mode: global # 不推荐全局模式（仅个人意见）。 placement: constraints: [node.role == manager] \"]},{\"header\":\"一个YAML配置示例\",\"slug\":\"一个yaml配置示例\",\"contents\":[\"version: \\\"3\\\" services: # 定义 Spring Boot 应用程序所需要的服务（容器） myproject: # 构建镜像的路径。\\\".\\\" 表示 Dockerfile 文件所在的当前目录 build: . # 指定容器名称 container_name: myproject # 容器所要使用的端口号 ports: - \\\"8080:8080\\\" # 指定容器启动后所需要等待的其它服务的启动时间 depends_on: - database - redis # 环境变量设置 environment: - SPRING_PROFILES_ACTIVE=prod - SPRING_DATASOURCE_URL=jdbc:mysql://database:3306/mydb?useSSL=false - SPRING_DATASOURCE_USERNAME=root - SPRING_DATASOURCE_PASSWORD=123456 - SPRING_REDIS_HOST=redis # 定义数据库服务（容器） database: image: mysql:5.7 container_name: database ports: - \\\"3306:3306\\\" environment: - MYSQL_ROOT_PASSWORD=123456 - MYSQL_DATABASE=mydb # 定义 Redis 服务（容器） redis: image: redis:alpine container_name: redis ports: - \\\"6379:6379\\\" \"]},{\"header\":\"Docker-compose常用命令\",\"slug\":\"docker-compose常用命令\",\"contents\":[\"docker-compose 命令 --help 获得一个命令的帮助 docker-compose up -d nginx 构建启动nignx容器 docker-compose exec nginx bash 登录到nginx容器中 docker-compose down 此命令将会停止 up 命令所启动的容器，并移除网络 docker-compose ps 列出项目中目前的所有容器 docker-compose restart nginx 重新启动nginx容器 docker-compose build nginx 构建镜像 docker-compose build --no-cache nginx 不带缓存的构建 docker-compose top 查看各个服务容器内运行的进程 docker-compose logs -f nginx 查看nginx的实时日志 docker-compose images 列出 Compose 文件包含的镜像 docker-compose config 验证文件配置，当配置正确时，不输出任何内容，当文件配置错误，输出错误信息。 docker-compose events --json nginx 以json的形式输出nginx的docker日志 docker-compose pause nginx 暂停nignx容器 docker-compose unpause nginx 恢复ningx容器 docker-compose rm nginx 删除容器（删除前必须关闭容器，执行stop） docker-compose stop nginx 停止nignx容器 docker-compose start nginx 启动nignx容器 docker-compose restart nginx 重启项目中的nignx容器 docker-compose run --no-deps --rm php-fpm php -v 在php-fpm中不启动关联容器，并容器执行php -v 执行完成后删除容器 \"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"Docker Compose 部署 Spring Boot 项目 - 知乎 (zhihu.com)\",\"Dockerfile 详解，看这一篇就够了 - 知乎 (zhihu.com)\",\"Docker-Compose入门到精通 （图解+秒懂+史上最全）_docker-compose图-CSDN博客\"]}],\"customFields\":{\"0\":[\"Docker\"],\"1\":[\"Java\",\"Docker\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/Hive%20%E9%9D%A2%E8%AF%95%E9%A2%98.html\":{\"title\":\"Hive概览\",\"contents\":[{\"header\":\"什么是Hive？它的主要作用是什么？\",\"slug\":\"什么是hive-它的主要作用是什么\",\"contents\":[]},{\"header\":\"Hive中的数据存储在哪里？它是如何组织的？\",\"slug\":\"hive中的数据存储在哪里-它是如何组织的\",\"contents\":[\"Hive中的数据存储在HDFS（Hadoop分布式文件系统）中，它是Hadoop生态系统的一部分。HDFS是一个分布式、可扩展的文件系统，设计用于存储大规模数据，并且具有高可靠性和容错性。Hive利用了HDFS的存储能力来管理和存储数据。 Hive将数据组织成表格（Tables），类似于传统的关系型数据库中的表格。每个Hive表格都有一组定义好的列（Columns），每列有其对应的数据类型。表格还可以根据数据特性和查询需求进行分区（Partition）和桶（Bucket）。 分区： Hive中的分区允许你将表格的数据按照特定的列值进行逻辑分组。例如，你可以将销售数据表格按照日期分区，每个日期对应一个分区。分区可以帮助提高查询性能，因为查询可以只针对特定分区进行，而不必处理整个表格。 桶： Hive中的桶是一种数据存储和查询优化的方式。桶将表格的数据划分为固定数量的桶，每个桶包含特定范围的数据。桶可以帮助在特定列上进行更高效的连接操作，因为数据在桶中按照哈希函数进行分配，类似于哈希连接的思想。 在HDFS中，Hive表格的数据被存储为一系列的数据块，这些数据块分布在Hadoop集群的各个节点上。每个数据块通常都有一个默认大小（例如128MB或256MB），这有助于并行处理和查询。 总之，Hive将数据存储在Hadoop分布式文件系统（HDFS）中，通过表格、列、分区和桶来组织数据。这种组织方式使得Hive能够有效地处理大规模数据，并且可以通过分区和桶来优化查询性能。\"]},{\"header\":\"Hive和传统关系型数据库之间有什么区别？\",\"slug\":\"hive和传统关系型数据库之间有什么区别\",\"contents\":[\"Hive和传统关系型数据库之间存在许多区别，这些区别涵盖了数据模型、查询语言、存储引擎、适用场景等多个方面。以下是Hive和传统关系型数据库之间的一些主要区别：\",\"数据模型：\",\"Hive：Hive的数据模型是基于分布式文件系统的，主要用于处理大规模数据，适合数据仓库和分析。它支持表格、列、分区和桶等概念。\",\"关系型数据库：传统关系型数据库的数据模型是基于表格的，适用于小到中等规模的数据存储和事务处理。\",\"查询语言：\",\"Hive：Hive的查询语言是HiveQL，它类似于SQL，但不完全相同，适用于大数据处理。\",\"关系型数据库：传统关系型数据库使用SQL作为查询语言，主要用于事务处理和小规模数据查询。\",\"数据存储和处理方式：\",\"Hive：Hive数据存储在HDFS中，通过MapReduce等分布式计算框架进行数据处理。适用于大规模数据的批量处理。\",\"关系型数据库：传统关系型数据库使用B树等数据结构存储数据，通过事务处理和索引进行数据操作。\",\"事务支持：\",\"Hive：Hive在某些版本中开始支持事务，但其事务支持相对较弱，主要用于更新和删除操作。\",\"关系型数据库：传统关系型数据库通常具有强大的事务支持，适合支持复杂的事务操作。\",\"查询性能：\",\"Hive：Hive的查询性能相对较慢，因为它主要用于批量处理大规模数据。\",\"关系型数据库：传统关系型数据库通常具有较好的实时查询性能，适合交互式查询和事务处理。\",\"存储成本：\",\"Hive：Hive的数据存储成本较低，因为它可以在廉价的硬件上进行分布式存储。\",\"关系型数据库：传统关系型数据库的存储成本相对较高，因为它通常需要高性能硬件和专用存储。\",\"适用场景：\",\"Hive：Hive适用于大数据处理、数据仓库和批量分析，特别是在需要处理PB级别数据的情况下。\",\"关系型数据库：传统关系型数据库适用于事务处理、小规模数据存储和实时查询。\",\"综上所述，Hive和传统关系型数据库有着明显的区别，适用于不同的数据处理需求。选择使用哪种数据库取决于你的数据规模、查询要求、性能需求以及特定的应用场景。\"]},{\"header\":\"什么是Hive的数据模型？\",\"slug\":\"什么是hive的数据模型\",\"contents\":[\"Hive的数据模型是一种抽象，用于将数据存储在分布式文件系统中，并以类似于传统关系型数据库的方式进行管理和查询。Hive的数据模型包括以下主要概念：\",\"表格（Table）： 表格是Hive中数据的基本组织单位，类似于关系型数据库中的表格。每个表格由一组命名的列定义，每列都有其数据类型。\",\"列（Column）： 列是表格的属性，类似于关系型数据库中的列。每列都有一个名称和数据类型，定义了表格中存储的数据的结构。\",\"行（Row）： 行是表格中的一个记录，表示数据的一条记录。每行包含每列对应的数据值。\",\"分区（Partition）： 分区是Hive中的一种组织数据的方式，允许将表格数据分为不同的逻辑分区。分区通常根据数据的某个属性（如日期、地区）进行分组，以便更高效地查询和管理数据。\",\"桶（Bucket）： 桶是Hive中另一种数据组织方式，用于更好地优化查询性能。桶将表格数据划分为固定数量的桶，每个桶包含一部分数据，通常通过哈希函数来分配数据。\",\"数据类型： Hive支持各种数据类型，包括基本数据类型（如整数、字符串、布尔值）、复杂数据类型（如数组、映射）以及自定义数据类型。\",\"Hive的数据模型允许用户通过HiveQL（类似于SQL的查询语言）来进行表格的创建、数据加载、查询、转换和分析操作。HiveQL查询会被翻译成MapReduce作业或其他分布式计算框架的任务，以在分布式环境中处理数据。虽然Hive的数据模型类似于传统关系型数据库，但由于Hive是在大数据环境下运行的，因此其处理方式和性能特征与传统数据库有一些不同。\"]},{\"header\":\"Hive中的分区和桶是什么？它们有什么作用？\",\"slug\":\"hive中的分区和桶是什么-它们有什么作用\",\"contents\":[\"在Hive中，分区（Partition）和桶（Bucket）是两种数据组织方式，用于优化查询性能和管理大规模数据。 分区（Partition）： 分区是将表格数据按照某个列或一组列的值进行逻辑分组的过程。每个分区实际上是一个子目录，它包含了具有相同分区键值的行数据。分区通常根据数据的特征进行划分，例如按照日期、地区、类别等进行分区。 分区的作用：\",\"提高查询性能： 当数据分区时，查询可以只针对特定分区进行，而不必扫描整个表格。这可以大大提高查询的性能，尤其是当数据量很大时。\",\"管理数据： 分区可以帮助更好地组织和管理数据，使数据更具可读性和可维护性。例如，可以轻松删除或移动特定分区的数据。\",\"优化数据加载： 分区可以加速数据加载过程。只需加载新分区的数据，而不是整个表格。\",\"桶（Bucket）： 桶是将表格数据划分为固定数量的部分，每个部分称为一个桶。桶的划分是通过某个列的哈希函数来实现的，确保相同哈希值的行被放入同一个桶中。在桶中，数据按照哈希值进行分布，而不是按照分区键值。 桶的作用：\",\"更好的查询性能： 桶可以使特定列上的连接操作更高效，因为相同哈希值的数据将位于同一个桶中，减少了数据的移动和扫描。\",\"均衡数据分布： 桶可以确保数据分布相对均衡，减少了数据倾斜问题的可能性。\",\"局部排序： 桶内的数据可以被排序，这在一些查询和分析中可能有用。\",\"需要注意的是，分区和桶并不是必须的，而是在特定情况下用于优化性能的技术。在使用分区和桶之前，需要仔细考虑数据的特性、查询需求和查询模式，以确定是否使用这些数据组织方式。\"]},{\"header\":\"什么是HiveQL？它与SQL有什么不同？\",\"slug\":\"什么是hiveql-它与sql有什么不同\",\"contents\":[\"HiveQL（Hive Query Language）是Hive中的查询语言，类似于传统关系型数据库中的SQL（Structured Query Language）。HiveQL允许用户使用类似于SQL的语法来查询和操作Hive中的数据，但由于Hive的底层数据存储和处理方式与传统关系型数据库不同，因此HiveQL与SQL之间存在一些重要的区别：\",\"数据模型： Hive的数据模型是基于分布式文件系统的，而传统关系型数据库的数据模型是基于表格的。这导致HiveQL与SQL在一些数据定义和处理方面存在差异。\",\"查询引擎： HiveQL查询被翻译成MapReduce作业或其他分布式计算框架的任务，以在分布式环境中处理数据。而SQL查询通常在传统关系型数据库的查询引擎上执行。\",\"性能特征： 由于Hive主要用于大规模数据批量处理，HiveQL的查询性能通常相对较慢。传统关系型数据库的SQL查询通常具有更好的实时查询性能。\",\"数据类型： 虽然HiveQL和SQL都支持基本的数据类型（如整数、字符串、日期等），但由于Hive的大数据背景，它还支持更多复杂的数据类型，如数组、映射等。\",\"函数和操作： HiveQL和SQL都支持各种内置函数，但由于数据处理方式的不同，它们之间的函数和操作可能存在细微的差异。\",\"索引和键约束： 传统关系型数据库通常支持索引和键约束来优化查询性能和保障数据完整性。而Hive并不直接支持传统索引和键约束。\",\"事务支持： 传统关系型数据库通常具有强大的事务支持，可以支持复杂的事务处理。在某些版本中，Hive开始支持一些事务操作，但其事务支持较弱。\",\"尽管HiveQL和SQL之间存在差异，但HiveQL的设计目标是使数据分析师和开发人员可以在大数据环境下轻松使用类似于SQL的语法来查询和处理数据。对于熟悉SQL的用户来说，学习和使用HiveQL通常较为容易。\"]},{\"header\":\"如何在Hive中创建表格？\",\"slug\":\"如何在hive中创建表格\",\"contents\":[\"打开Hive命令行界面： 打开终端并运行hive命令，进入Hive的命令行界面。\",\"编写CREATE TABLE语句： 使用HiveQL语句来创建表格。以下是一个示例，演示如何创建一个名为employees的表格，其中包含id、name和salary三列：\",\"CREATE TABLE employees ( id INT, name STRING, salary DOUBLE ); \",\"在这个示例中，我们定义了一个名为employees的表格，包含了三列：id（整数类型）、name（字符串类型）和salary（双精度浮点数类型）\",\"指定表格属性和选项（可选）： 你可以通过选项来指定表格的一些属性，例如存储格式、分区键、桶等。以下是一些示例：\",\"STORED AS PARQUET; \",\"PARTITIONED BY (year INT, month INT); \",\"CLUSTERED BY (id) INTO 10 BUCKETS; \",\"查看表格列表： 可以使用SHOW TABLES命令查看当前数据库中的所有表格，确认新表格是否创建成功。\"]},{\"header\":\"如何在Hive中加载数据？\",\"slug\":\"如何在hive中加载数据\",\"contents\":[\"在Hive中加载数据通常使用LOAD DATA命令来实现。这个命令用于将外部数据加载到Hive表格中。下面是使用LOAD DATA命令加载数据的基本步骤：\",\"准备数据： 首先，确保你的数据已经准备好并存储在Hadoop分布式文件系统（HDFS）中或其他Hive支持的文件系统中。数据可以是文本文件、CSV文件、Parquet文件等。\",\"打开Hive命令行界面： 打开终端并运行hive命令，进入Hive的命令行界面。\",\"选择数据库（如果需要）： 如果你想将数据加载到特定的数据库中，请使用**USE <database_name>;**语句来切换到该数据库。\",\"编写LOAD DATA语句： 使用LOAD DATA语句来加载数据。以下是一个示例，演示如何从HDFS加载数据到名为employees的表格中：\",\"LOAD DATA INPATH '/user/hadoop/employee_data.txt' INTO TABLE employees; \",\"在这个示例中，我们假设数据文件employee_data.txt已经存储在HDFS的**/user/hadoop/路径下，将其加载到名为employees**的表格中。\",\"执行LOAD DATA语句： 输入完整的LOAD DATA语句后，按Enter键执行该语句。Hive将会将数据加载到指定的表格中。\",\"确认数据加载： 可以使用SELECT语句来查询表格中的数据，以确认数据是否成功加载。\",\"需要注意的是，LOAD DATA命令是将数据从HDFS或其他支持的文件系统加载到Hive表格中。它是一个批量操作，适用于大量数据的加载。如果需要实时地将数据插入到表格中，可以考虑使用HiveQL的INSERT INTO语句。 另外，数据文件的格式和表格的存储格式需要匹配，否则可能需要进行数据转换。例如，如果表格的存储格式是Parquet，那么需要确保加载的数据文件也是Parquet格式。\"]},{\"header\":\"什么是外部表和管理表？它们之间的区别是什么？\",\"slug\":\"什么是外部表和管理表-它们之间的区别是什么\",\"contents\":[\"在Hive中，外部表（External Table）和管理表（Managed Table）是两种不同的表格类型，它们在数据管理和存储方面有一些重要的区别。 外部表（External Table）： 外部表是Hive中的一种表格类型，它与数据存储在Hive中的位置相对应，但不会在Hive管理的数据目录中维护数据。外部表的数据可以存储在HDFS中或其他文件系统中（如本地文件系统、Amazon S3等），并且可以在表格定义中指定数据的位置。外部表允许你将外部数据与Hive的元数据关联起来，从而可以在Hive中进行查询和分析。 管理表（Managed Table）： 管理表是Hive中的另一种表格类型，也称为内部表（Internal Table）。管理表的数据存储在Hive管理的数据目录中，Hive对数据的生命周期、元数据和存储位置进行管理。当你删除管理表时，Hive会自动删除其相关的数据。 外部表和管理表之间的区别：\",\"数据管理：\",\"外部表：数据存储在外部，Hive不管理数据的生命周期，不会自动删除数据。\",\"管理表：数据存储在Hive管理的数据目录中，Hive负责数据的管理，包括数据的添加、删除和清理。\",\"数据存储：\",\"外部表：数据可以存储在HDFS或其他文件系统中。\",\"管理表：数据存储在Hive管理的数据目录中。\",\"删除表时的行为：\",\"外部表：删除外部表时，只会删除Hive中的元数据，不会删除数据。\",\"管理表：删除管理表时，Hive会同时删除元数据和数据。\",\"适用场景：\",\"外部表：适用于与Hive关联的外部数据，不需要Hive管理数据生命周期的情况。\",\"管理表：适用于Hive管理数据的情况，可以更方便地控制数据的管理和清理。\",\"总的来说，外部表和管理表是根据数据管理需求选择的两种不同的表格类型。选择哪种类型取决于你是否需要Hive来管理数据，以及数据是否存储在Hive管理之外的位置。\"]},{\"header\":\"如何执行基本的SELECT查询？如何执行聚合查询？\",\"slug\":\"如何执行基本的select查询-如何执行聚合查询\",\"contents\":[\"在Hive中执行基本的SELECT查询和聚合查询与传统的SQL类似，你可以使用HiveQL语句来完成。下面分别演示如何执行基本的SELECT查询和聚合查询： 执行基本的SELECT查询： 假设我们有一个名为employees的表格，其中包含id、name和salary三列。以下是如何执行基本的SELECT查询来获取所有员工的信息：\",\"sqlCopy code SELECT * FROM employees; \",\"上述查询将会返回employees表格中所有员工的所有信息。 执行聚合查询： 假设我们继续使用上述employees表格，以下是如何执行一个聚合查询，计算平均工资：\",\"SELECT AVG(salary) AS average_salary FROM employees; \",\"上述查询将会计算employees表格中所有员工的平均工资，并将结果别名为average_salary。\",\"除了上述示例，HiveQL还支持更多的聚合函数（如SUM、COUNT、MIN、MAX等）和其他查询操作（如JOIN、GROUP BY、ORDER BY等），以及支持在大规模数据上执行这些操作的能力。 请注意，Hive的查询性能相对较慢，特别是对于大规模数据的查询。这是因为Hive主要用于批量处理大数据，不适合实时查询。在编写查询时，请考虑使用合适的数据类型、分区和桶等来优化查询性能。\"]},{\"header\":\"如何对Hive中的数据进行分组和排序？\",\"slug\":\"如何对hive中的数据进行分组和排序\",\"contents\":[\"在Hive中，你可以使用GROUP BY和ORDER BY子句来对数据进行分组和排序。这些子句可以与HiveQL的SELECT语句一起使用，以便按照特定的列进行分组和排序。以下是如何在Hive中对数据进行分组和排序的示例： 分组数据（GROUP BY）： 假设我们有一个名为sales的表格，其中包含product、category和revenue三列。以下是如何使用GROUP BY对销售数据按照类别进行分组，并计算每个类别的总收入：\",\"SELECT category, SUM(revenue) AS total_revenue FROM sales GROUP BY category; \",\"上述查询将会按照category列对销售数据进行分组，并计算每个类别的总收入。\",\"继续使用上述sales表格，以下是如何使用ORDER BY对销售数据按照总收入降序排序\",\"SELECT category, SUM(revenue) AS total_revenue FROM sales GROUP BY category ORDER BY total_revenue DESC; \",\"上述查询将会先按照category列对销售数据进行分组，然后按照计算得到的总收入降序排序。\",\"需要注意的是，Hive的ORDER BY子句会将数据收集到一个单一的Reduce任务中进行排序，这可能会影响性能。如果在分组和排序的操作中遇到性能问题，可以考虑使用CLUSTER BY子句，它可以结合桶来提高查询性能。 总之，在Hive中，你可以使用GROUP BY和ORDER BY子句来对数据进行分组和排序，以满足不同的查询需求。根据数据量和性能要求，你可能需要考虑一些优化措施来提高查询性能。\"]},{\"header\":\"什么是Hive的用户定义函数（UDF）和用户定义聚合函数（UDAF）？如何使用它们？\",\"slug\":\"什么是hive的用户定义函数-udf-和用户定义聚合函数-udaf-如何使用它们\",\"contents\":[]},{\"header\":\"如何优化Hive查询的性能?\",\"slug\":\"如何优化hive查询的性能\",\"contents\":[\"优化Hive查询的性能是一个复杂的过程，涉及多个方面，包括数据存储、数据处理、查询设计等。以下是一些常见的方法和技巧，可以帮助你优化Hive查询的性能：\",\"数据存储优化：\",\"分区和桶： 使用分区和桶来组织和存储数据，以提高查询性能。\",\"选择适当的存储格式： 选择适合数据类型和查询模式的存储格式，如Parquet、ORC等，以减少数据存储和I/O成本。\",\"查询设计优化：\",\"选择性投影： 在SELECT语句中只选择需要的列，避免不必要的数据传输和处理。\",\"*避免SELECT ： 避免使用SELECT *，而是明确列出需要的列。\",\"合理使用JOIN： 尽量使用INNER JOIN，避免大表的CROSS JOIN和笛卡尔积。\",\"数据预处理和ETL：\",\"数据清洗： 在加载数据之前进行数据清洗，处理缺失值和异常数据。\",\"数据压缩： 使用压缩算法减少数据存储和传输开销。\",\"性能监控和调优：\",\"Explain命令： 使用EXPLAIN命令来分析查询计划，了解查询的执行流程。\",\"Profile查询： 使用PROFILE命令分析查询的性能瓶颈，如IO、CPU等。\",\"资源管理： 使用资源管理器（如YARN）来分配足够的资源给查询作业。\",\"并行处理和分区：\",\"并行度调整： 根据集群资源，调整查询的并行度，以充分利用集群资源。\",\"合理分区： 在表格设计中使用合理的分区键，以便查询时可以只处理必要的分区。\",\"数据倾斜处理：\",\"均匀分布： 在设计分区和桶时避免数据倾斜，以确保数据均匀分布。\",\"处理倾斜键： 对于数据倾斜问题，可以使用技术如SMB Join、Map-side Join等来处理。\",\"适当的硬件配置和资源规划：\",\"增加资源： 根据查询工作负载的需要，适当增加集群的计算和存储资源。\",\"资源规划： 使用YARN等资源管理器来分配资源，以避免资源竞争和冲突。\",\"缓存和预热：\",\"使用Hive缓存： 使用Hive提供的查询缓存来加速相同查询的执行。\",\"预热： 在查询前对可能需要的数据进行预热，从而加速查询。\",\"总之，优化Hive查询的性能需要综合考虑数据存储、查询设计、资源配置等多个因素。建议根据实际需求和查询模式来选择合适的优化方法。在处理大规模数据时，往往需要结合多种技术和策略来达到最佳性能。\"]},{\"header\":\"什么是数据倾斜？如何处理数据倾斜问题？\",\"slug\":\"什么是数据倾斜-如何处理数据倾斜问题\",\"contents\":[\"数据倾斜是指在分布式计算环境中，数据在不同的计算节点上分布不均匀，导致部分节点上的任务处理时间明显长于其他节点，从而影响整体计算性能。数据倾斜可能会导致某些节点负载过重，而其他节点处于空闲状态，从而降低了整体的并行处理能力。 数据倾斜问题在大规模数据处理中经常会遇到，尤其是在JOIN、GROUP BY等操作中。一些常见的情况可能包括：\",\"在JOIN操作中，某些键值对的数量远远超过其他键值对，导致连接操作非常耗时。\",\"在GROUP BY操作中，某些键值对的数据量远大于其他键值对，导致在一个节点上产生大量的分组操作，影响性能。\",\"在聚合操作中，某些值的频率非常高，导致部分节点处理的数据量较大。\",\"以下是一些处理数据倾斜问题的方法：\",\"随机化键值分布： 在可能的情况下，将键值进行随机化处理，以减少特定键值集中在某个节点上的可能性。\",\"增加分区： 对于分区表，可以增加分区来将数据分散到更多的节点上，减轻数据倾斜。\",\"使用SMB Join： 对于JOIN操作，可以尝试使用SMB（Skew-Resistant Multi-Join）Join技术，将倾斜的键值单独处理。\",\"Map-side Join： 对于小表与大表的JOIN，可以使用Map-side Join，在Map阶段进行连接操作，减少Shuffle过程。\",\"使用Combiner： 对于支持Combiner的操作，可以在Map阶段使用Combiner来减少Shuffle的数据量。\",\"采用桶： 使用桶（Bucket）将数据划分成更小的块，均匀分布数据，以减少数据倾斜。\",\"动态调整并行度： 在发现数据倾斜后，可以动态调整任务的并行度，将倾斜数据分散到更多的节点上。\",\"增加Reduce任务： 在倾斜数据的Reduce任务上增加更多的任务，将数据均匀分布。\",\"使用外部表： 将倾斜数据存储在外部表中，从而在查询时可以更灵活地处理。\",\"使用动态分区： 在动态分区表中，可以根据数据分布情况动态创建分区，避免分区键的数据倾斜问题。\",\"数据倾斜处理的方法因情况而异，需要根据具体的数据、查询和场景来选择合适的策略。在实际应用中，可能需要尝试多种方法来找到最佳的解决方案。\"]},{\"header\":\"如何选择合适的分区和桶来优化查询性能？\",\"slug\":\"如何选择合适的分区和桶来优化查询性能\",\"contents\":[\"选择合适的分区和桶来优化查询性能是一个关键的设计决策，可以显著影响Hive查询的执行效率。分区和桶的选择需要根据数据特点、查询模式和性能要求来决定。以下是一些指导原则，可以帮助你选择合适的分区和桶来优化查询性能： 选择合适的分区：\",\"根据查询条件： 选择查询中最常用的条件作为分区键，这样可以减少不必要的数据扫描。例如，如果根据日期范围查询数据，可以选择以日期为分区键。\",\"均匀分布： 选择能够将数据均匀分布的列作为分区键，避免数据倾斜问题。\",\"高基数列： 如果列的基数（不同值的数量）很高，那么可以将该列作为分区键，以减少每个分区的数据量。\",\"避免高基数列： 如果列的基数非常高，分区可能会变得过多，导致管理和查询的复杂性增加。\",\"时间周期： 对于时间序列数据，可以按照一定的时间周期进行分区，例如按年、月、日等分区。\",\"分区键的数据类型： 分区键的数据类型应该选择Hive支持的数据类型，以避免数据转换的开销。\",\"选择合适的桶：\",\"数据均匀性： 选择可以将数据均匀分布的列作为桶列，以避免数据倾斜问题。\",\"查询频繁度： 选择经常被查询的列作为桶列，这样可以在连接和聚合操作中提高查询性能。\",\"连接操作： 在连接操作中，将连接键作为桶列可以提高连接性能，减少Shuffle的数据量。\",\"内存限制： 桶的数量应该适当，不要过多，以免超过内存限制导致性能下降。\",\"不同大小： 桶的大小可以根据数据分布情况和查询需求进行选择，避免过大或过小的桶。\",\"动态分桶： 可以在数据加载时根据数据分布情况动态选择桶列和桶数量。\",\"在实际应用中，需要综合考虑分区和桶的选择，根据数据和查询的特点来做出决策。在选择分区和桶时，可以尝试不同的设计，并进行性能测试和分析，从而找到最佳的方案。同时，随着数据的变化和查询需求的演化，可能需要对分区和桶的设计进行调整和优化。\"]},{\"header\":\"如何使用压缩来提高Hive作业的性能？\",\"slug\":\"如何使用压缩来提高hive作业的性能\",\"contents\":[\"使用压缩可以有效地提高Hive作业的性能，减少存储空间和I/O开销，从而加速数据的读写和查询过程。压缩技术可以减少数据在磁盘上的占用空间，减少数据传输和存储开销，从而间接地提升查询性能。以下是如何使用压缩来优化Hive作业的一些方法：\",\"选择适当的压缩格式： Hive支持多种压缩格式，如Snappy、Gzip、LZO、Zstandard等。选择适合你数据类型和查询模式的压缩格式是重要的一步。例如，Snappy通常会在压缩和解压缩之间取得很好的平衡。\",\"压缩表格： 在创建表格时，可以使用STORED AS子句来指定表格的压缩格式。例如：\",\"CREATE TABLE compressed_table (...) STORED AS ORC TBLPROPERTIES (\\\"orc.compress\\\"=\\\"SNAPPY\\\"); \",\"压缩分区： 对于分区表，可以在分区级别指定压缩格式，以根据不同的分区数据类型和查询模式选择合适的压缩格式。\",\"使用压缩选项： 在查询中，可以使用压缩选项来指定数据的压缩格式。例如，使用**SET hive.exec.compress.output=true;**来在输出时使用压缩。\",\"压缩临时文件： Hive作业中产生的临时文件也可以通过配置来启用压缩，以减少I/O开销。\",\"压缩数据导入： 在数据导入过程中，可以使用压缩选项来指定要导入的数据文件的压缩格式。\",\"压缩存储格式： Hive支持多种存储格式，如ORC和Parquet，这些格式内置了压缩功能，可以进一步减少存储和I/O开销。\",\"评估性能影响： 尽管压缩可以提高存储效率，但压缩和解压缩操作也会对查询性能产生一定影响。在使用压缩时，需要评估存储节省和性能影响之间的权衡。\",\"请注意，选择合适的压缩格式和配置取决于数据类型、查询模式、集群资源等多个因素。通过测试和评估不同的压缩方案，可以找到最适合你应用的优化策略。\"]},{\"header\":\"什么是Hive的Cost-Based Optimizer（CBO）？它如何影响查询计划？\",\"slug\":\"什么是hive的cost-based-optimizer-cbo-它如何影响查询计划\",\"contents\":[\"Hive的Cost-Based Optimizer（CBO）是一种查询优化器，它通过估计查询不同执行计划的代价来选择最优的执行计划。CBO基于查询的统计信息、数据分布和系统资源等因素，为查询选择最佳的物理执行计划，从而提高查询性能。 在传统的查询优化器中，规则优化器（Rule-Based Optimizer，RBO）通常使用固定的规则来决定查询计划，不考虑查询的实际代价。而CBO则是基于代价模型来决定查询计划的，它会综合考虑查询处理的时间、I/O开销、CPU开销等因素，从而选择最优的执行计划。 CBO对查询计划的影响包括：\",\"更准确的代价估计： CBO使用统计信息来估计每个操作的代价，从而更准确地反映实际的执行成本。这有助于选择代价最小的执行计划。\",\"自适应性： CBO可以根据查询的复杂性和数据分布等情况，动态地调整查询计划，从而更好地适应不同的查询。\",\"避免传统优化器的限制： 传统的规则优化器可能会受限于固定的规则，无法应对复杂的查询场景。CBO能够处理更多的查询情况，提供更好的优化结果。\",\"性能改进： 通过选择更优的执行计划，CBO可以显著提高查询性能，减少不必要的计算和I/O开销。\",\"需要注意的是，CBO的性能提升不是绝对的，它也可能会因为查询统计信息的不准确或者其他因素而导致一些问题。在使用CBO时，应该定期更新查询统计信息，进行性能测试和分析，以确保获得最佳的查询性能。如果在某些情况下CBO表现不佳，你也可以选择在查询中强制使用特定的执行计划或者关闭CBO。\"]},{\"header\":\"如何在Hive中执行数据转换和清洗操作？\",\"slug\":\"如何在hive中执行数据转换和清洗操作\",\"contents\":[\"在Hive中执行数据转换和清洗操作通常涉及使用HiveQL语言以及内置函数来处理数据。以下是一些常见的数据转换和清洗操作示例：\",\"SELECT CAST(column_name AS INT) FROM table_name; \",\"字符串处理： 使用内置函数来进行字符串操作，如CONCAT、SUBSTRING、TRIM等。\",\"SELECT CONCAT(first_name, ' ', last_name) AS full_name FROM employees; \",\"空值处理： 使用COALESCE函数将NULL替换为其他值，或者使用CASE语句处理空值情况。\",\"SELECT COALESCE(column_name, 'N/A') FROM table_name; \",\"日期和时间处理： 使用日期和时间函数来进行日期格式化、计算等操作，如DATE_FORMAT、DATEDIFF、DATE_ADD等。\",\"SELECT DATE_FORMAT(date_column, 'yyyy-MM-dd') AS formatted_date FROM orders; \",\"数据清洗： 使用正则表达式函数如REGEXP_REPLACE来进行数据清洗，如移除特定字符或格式化数据。\",\"SELECT REGEXP_REPLACE(phone_number, '[^0-9]', '') AS cleaned_phone FROM customers; \",\"条件转换： 使用CASE语句进行条件判断和值转换，根据不同条件返回不同的值。\",\"SELECT product_name, CASE WHEN price > 100 THEN 'Expensive' ELSE 'Affordable' END AS price_category FROM products; \",\"数据拆分： 使用SPLIT函数将包含多个值的字符串拆分成多个字段，然后进行处理。\",\"SELECT SPLIT(address, ',')[0] AS city, SPLIT(address, ',')[1] AS state FROM customers; \",\"数据合并： 使用CONCAT_WS函数将多个字段合并为一个字符串，根据指定的分隔符。\",\"SELECT CONCAT_WS(', ', city, state) AS location FROM locations; \",\"数值处理： 使用数值函数如ROUND、CEIL、FLOOR等来处理数值数据。\",\"SELECT ROUND(price, 2) AS rounded_price FROM products; \",\"上述示例只是一些常见的数据转换和清洗操作。根据你的实际需求，你可以使用HiveQL中的各种内置函数、表达式和语句来进行更复杂的数据处理。当处理大规模数据时，还应该考虑性能和效率，选择适当的优化策略。\"]},{\"header\":\"Hive支持哪些内置的数据转换函数和操作？\",\"slug\":\"hive支持哪些内置的数据转换函数和操作\",\"contents\":[\"Hive提供了许多内置的数据转换函数和操作，用于在查询中进行数据处理、转换和清洗。以下是一些常见的Hive内置数据转换函数和操作： 1. 字符串函数：\",\"CONCAT(str1, str2, ...)：连接多个字符串。\",\"UPPER(string)：将字符串转换为大写。\",\"LOWER(string)：将字符串转换为小写。\",\"TRIM(string)：去除字符串两端的空格。\",\"SUBSTRING(string, start, length)：提取子字符串。\",\"REGEXP_REPLACE(string, pattern, replacement)：使用正则表达式替换字符串中的内容。\",\"2. 数值函数：\",\"ROUND(number, decimal_places)：四舍五入到指定小数位数。\",\"CEIL(number)：向上取整。\",\"FLOOR(number)：向下取整。\",\"ABS(number)：取绝对值。\",\"SIGN(number)：返回数值的符号。\",\"3. 日期和时间函数：\",\"YEAR(date)：提取日期中的年份。\",\"MONTH(date)：提取日期中的月份。\",\"DAY(date)：提取日期中的日。\",\"DATE_FORMAT(date, format)：将日期格式化为指定格式。\",\"CURRENT_DATE()：返回当前日期。\",\"4. 类型转换函数：\",\"CAST(expression AS type)：将表达式转换为指定类型。\",\"5. 条件函数：\",\"CASE：用于条件判断和值选择，包括简单CASE和搜索CASE。\",\"6. 数组和Map函数：\",\"ARRAY()：创建数组。\",\"MAP()：创建Map。\",\"ARRAY_CONTAINS(array, value)：检查数组中是否包含指定值。\",\"MAP_KEYS(map)：返回Map中的键。\",\"MAP_VALUES(map)：返回Map中的值。\",\"7. 分析函数：\",\"ROW_NUMBER()：为每行分配一个唯一的数值。\",\"RANK()：为每行分配一个排名，相同值具有相同排名。\",\"以上仅是Hive内置函数的一小部分示例。Hive还提供了更多用于数学运算、日期处理、字符串操作、条件判断、数组和Map操作等的函数。在编写Hive查询时，可以使用这些内置函数来处理数据，进行转换、清洗和计算等操作。如果需要更详细的函数列表和用法，你可以参考Hive的官方文档。\"]},{\"header\":\"如何将Hive表格中的数据导出到其他格式（如CSV、Parquet）？\",\"slug\":\"如何将hive表格中的数据导出到其他格式-如csv、parquet\",\"contents\":[\"将Hive表格中的数据导出到其他格式（如CSV、Parquet）可以通过不同的方法实现，具体取决于你选择的导出格式和工具。以下是一些常见的方法来导出Hive表格数据到其他格式： 1. 导出为CSV格式： 可以使用Hive自带的INSERT语句结合本地文件系统的命令，将查询结果导出为CSV格式文件。下面是一个示例，将查询结果导出为CSV文件：\",\"sqlCopy code INSERT OVERWRITE LOCAL DIRECTORY '/path/to/output' ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' SELECT * FROM your_table; \",\"2. 导出为Parquet格式： Parquet是一种高效的列式存储格式，适合大数据分析。你可以使用INSERT语句将数据导出为Parquet格式，前提是Hive表格的存储格式已经是Parquet。\",\"sqlCopy code INSERT OVERWRITE DIRECTORY '/path/to/output' STORED AS PARQUET SELECT * FROM your_table; \",\"3. 使用Hive的EXPORT命令： Hive提供了EXPORT命令，可以将表格数据导出为文本文件。你可以使用类似以下的命令将数据导出为CSV格式：\",\"sqlCopy code EXPORT TABLE your_table TO '/path/to/output' DELIMITED FIELDS TERMINATED BY ','; \",\"4. 使用Hadoop的DistCp命令： 如果要将数据从Hive表格导出到HDFS上的其他目录，你可以使用Hadoop的DistCp命令：\",\"bashCopy code hadoop distcp hdfs://source_path hdfs://destination_path \",\"注意事项：\",\"在执行数据导出之前，确保目标文件路径具有适当的权限。\",\"导出大量数据时，要注意文件大小和分区等因素，以避免数据倾斜和性能问题。\",\"导出数据时可以使用压缩，以减少存储空间和网络传输开销。\",\"根据实际需求，选择适合你场景的方法来导出Hive表格数据。每种方法都有其特定的用途和注意事项，因此在使用之前最好进行测试和评估。\"]}],\"customFields\":{\"1\":[\"hadoop\",\"大数据\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/Solr/Solr%E9%9D%A2%E8%AF%95%E9%A2%98.html\":{\"title\":\"Solr概览\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Solr是Apache下的一个顶级开源项目，采用Java开发，它是基于Lucene的全文搜索服务器 Lucene是一个基于Java的全文信息检索工具包，它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能 Solr底层的核心技术是使用Lucene来实现的\",\"垂直搜索功能\",\"提供一套强大Data Schema来定义字段，类型和设置文本分析\",\"提供基于Web的管理界面\",\"Solr垂直搜索功能指的是对某一特定领域或者主题的数据进行索引和搜索的能力 对外提供类似于Web-service的API接口\",\"接收查询请求：当用户在搜索界面输入查询关键词时，Solr接收该请求并发送给服务器。\",\"解析查询请求：Solr接收到查询请求后，会进行解析和预处理操作。这个过程包括解析查询字符串、去除停用词（无关紧要的词汇，如\\\"的\\\"、\\\"和\\\"等）、分词等。\",\"构建搜索索引：经过预处理后，Solr会根据配置的索引映射关系，将查询关键词转换为对应的索引字段。然后，根据索引构建搜索倒排索引表，用于快速定位符合条件的文档。\",\"执行查询：根据用户的查询条件，Solr使用倒排索引表进行匹配，找到符合条件的文档。这个过程会根据配置的排序规则进行排序，并使用各种查询优化技术（如查询扩展、结果去重等）来提高搜索结果的质量。\",\"返回结果：Solr将查询结果按照指定的排序方式进行排序，并将结果以XML或JSON等格式返回给客户端。\",\"渲染结果：客户端接收到搜索结果后，根据需求进行数据的解析和展示。\",\"倒排索引原理是将文档进行分词处理，标记每个词都出现在哪些文档里面，这样就可以快速查询某个词所出现的文档位置 倒排索引是一种倒排表（ inverted index），它以字或词为关键字进行索引。在这个索引表中，每个记录都对应一个关键词，记录的内容是出现该关键词的文档编号和该词在该文档中的位置信息。这种数据结构使得我们可以快速查找到包含特定关键词的所有文档。\",\"在Solr中，索引是一种数据结构，用于存储和组织文档数据，以便能够快速进行搜索和查询。创建过程如下：\",\"创建索引字段：首先，需要定义索引字段（field），它们是用于存储和检索文档数据的属性。例如，可以创建一个名为\\\"title\\\"的索引字段，用于存储文档的标题信息。\",\"定义字段类型：为了确保索引字段能够正确地存储和检索数据，需要为每个字段定义一个合适的字段类型（field type）。字段类型定义了字段的存储方式、索引方式以及其他相关属性。例如，可以为\\\"title\\\"字段定义一个名为\\\"text_general\\\"的字段类型，该类型适用于存储文本数据并进行全文搜索。\",\"创建索引：在定义好索引字段和字段类型后，可以使用Solr的API或管理界面创建索引。在创建索引时，需要指定要索引的文档数据以及对应的索引字段。\",\"提交文档：一旦文档数据被添加到索引中，需要提交这些文档以使其可供搜索。可以使用Solr的API或管理界面提交文档。\",\"Solr的分布式架构通过使用Zookeeper作为协调服务，以Zookeeper为注册中心来管理 事务日志事务日志确保更新无丢失，即使文档没有索引到磁盘\",\"查看日志：查看Solr的日志文件，通常在Solr的安装目录下的logs文件夹中，找到相关的错误信息。\",\"清理临时信息：在创建索引失败后，可能需要清理临时信息，以便重新创建索引。\",\"重新创建索引：根据错误信息，采取相应的措施，重新创建索引。\",\"创建定时检查：通过定时检查降索引重新创建进Solr中，在创建索引时，有可能会出现索引重复或重名等冲突。需要先查看已有的索引，找出冲突的索引并删除，再重新创建所需索引。\",\"调整索引参数：根据实际情况，调整Solr的配置参数，例如缓存大小、线程池大小等，以提高索引的性能和成功率。\",\"通过内置分析器进行自定义处理，例如，可以使用Icu4j库来实现中文分词，然后将其包装成自定义分析器。IK Analyzer也是中文分析器\",\"通过中文分词插件，例如结巴分词插件（Jieba）或盘古分词插件（Pangu）等\",\"Solr中的复制和复制集是用来实现数据冗余和灾备的方法。复制是指将一个Solr索引节点的内容复制到另一个Solr索引节点上，以实现数据冗余。复制集是指由一个主Solr索引节点和若干个从Solr索引节点组成的集群，其中主节点负责接收和存储文档，从节点从主节点复制文档以实现数据冗余和灾备。 就是构建集群，可以通过Solr的API或者Zookeeper来管理\",\"Solr如何处理大量数据的索引和搜索？\",\"Solr如何与其他系统集成？\",\"这些问题涵盖了Solr的基本概念、功能和架构。在面试中，你可以根据对方的回答来进一步探讨和深入了解他们对Solr的理解和经验。记得根据对方的回答提出更具体的问题，以便更好地评估他们的技能和知识水平。\"]}],\"customFields\":{\"1\":[\"搜索引擎\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%9F%BA%E7%A1%80/Lucene.html\":{\"title\":\"Lucene\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"Lucene是Apache基金会jakarta项目组的一个子项目；\",\"Lucene是一个开放源码的全文检索引擎工具包，提供了完整的查询引擎和索引引擎，部分语种文本分析引擎；\",\"Lucene 在开源的搜索引擎里一直处于垄断地位，它的实现语言是 Java 语言。\",\"现在常用的ElasticSearch、Solr等全文搜索引擎均是基于Lucene实现的。但Lucene是单机的模式\"]},{\"header\":\"Lucene基础工作流程\",\"slug\":\"lucene基础工作流程\",\"contents\":[\"索引的生成分为两个部分：\",\"1）创建阶段：\",\"添加文档阶段，通过IndexWriter调用addDocument方法生成正向索引文件；\",\"文档添加后，通过flush或merge操作生成倒排索引文件。\",\"2） 搜索阶段：\",\"用户通过查询语句向Lucene发送查询请求；\",\"通过IndexSearch下的IndexReader读取索引库内容，获取文档索引；\",\"得到搜索结果后，基于搜索算法对结果进行排序后返回。\"]},{\"header\":\"Lucene索引构成\",\"slug\":\"lucene索引构成\",\"contents\":[]},{\"header\":\"正向索引\",\"slug\":\"正向索引\",\"contents\":[\"Lucene的基础层次结构由索引、段、文档、域、词五个部分组成。正向索引的生成即为基于Lucene的基础层次结构一级一级处理文档并分解域存储词的过程。\",\"索引文件层级关系如图1所示：\",\"索引：Lucene索引库包含了搜索文本的所有内容，可以通过文件或文件流的方式存储在不同的数据库或文件目录下。\",\"段：一个索引中包含多个段，段与段之间相互独立。由于Lucene进行关键词检索时需要加载索引段进行下一步搜索，如果索引段较多会增加较大的I/O开销，减慢检索速度，因此写入时会通过段合并策略对不同的段进行合并。\",\"文档：Lucene会将文档写入段中，一个段中包含多个文档。\",\"域：一篇文档会包含多种不同的字段，不同的字段保存在不同的域中。\",\"词：Lucene会通过分词器将域中的字符串通过词法分析和语言处理后拆分成词，Lucene通过这些关键词进行全文检索。\"]},{\"header\":\"倒排索引\",\"slug\":\"倒排索引\",\"contents\":[\"Lucene全文索引的核心是基于倒排索引实现的快速索引机制。\",\"倒排索引原理如图2所示，倒排索引简单来说就是基于分析器将文本内容进行分词后，记录每个词出现在哪篇文章中，从而通过用户输入的搜索词查询出包含该词的文章。\",\"根据图2，我们可以看出当我们模糊查询，相关性匹配（搜索）时，正排索引需要仔细比对值，而倒排效率则更高，不用过滤无关词段\",\"倒排正是将这些文本记录拆分成词组，来关联索引\",\"**问题：**上述倒排索引使用时每次都需要将索引词加载到内存中，当文章数量较多，篇幅较长时，索引词可能会占用大量的存储空间，加载到内存后内存损耗较大。\",\"从Lucene4开始，Lucene采用了FST来减少索引词带来的空间消耗。\",\"FST(Finite StateTransducers)，中文名有限状态机转换器。其主要特点在于以下四点：\",\"查找词的时间复杂度为O(len(str))；\",\"通过将前缀和后缀分开存储的方式，减少了存放词所需的空间；\",\"加载时仅将前缀放入内存索引，后缀词在磁盘中进行存放，减少了内存索引使用空间的损耗；\",\"FST结构在对PrefixQuery、FuzzyQuery、RegexpQuery等查询条件查询时，查询效率高。\"]},{\"header\":\"一个Demo\",\"slug\":\"一个demo\",\"contents\":[\"基于Lucene的引擎有很多，很多很简单好用、甚至支持了分布式，各种分词插件，但这儿还是简单演示一下基础的代码操作\",\" public static void main(String[] args) throws IOException, ParseException { // 创建分析和索引文档目录 StandardAnalyzer analyzer = new StandardAnalyzer(); Path path = Paths.get(\\\"E:/lucene-demo-index\\\", new String[0]); Directory index = FSDirectory.open(path); //增 // 1.创建索引，初始化一个writer IndexWriterConfig config = new IndexWriterConfig(analyzer); config.setOpenMode(IndexWriterConfig.OpenMode.CREATE); IndexWriter writer = new IndexWriter(index, config); // 2 写入索引 addDoc(writer, \\\"Lucene in Action\\\", \\\"193398817\\\"); addDoc(writer, \\\"Lucene for Dummies\\\", \\\"55320055Z\\\"); addDoc(writer, \\\"Managing Gigabytes\\\", \\\"55063554A\\\"); addDoc(writer, \\\"The Art of Computer Science\\\", \\\"9900333X\\\"); writer.close(); // 查 // 1.构造1个查询 String queryStr = \\\"lucene\\\"; Query q = new QueryParser(\\\"title\\\", analyzer).parse(queryStr); System.out.println(\\\"query: \\\" + q.toString()); int hitsPerPage = 10; // 2 创建索引查询 IndexReader reader = DirectoryReader.open(index); IndexSearcher searcher = new IndexSearcher(reader); // 3 执行查询 TopDocs docs = searcher.search(q, hitsPerPage); ScoreDoc[] hits = docs.scoreDocs; //结果展示 System.out.println(\\\"found \\\" + hits.length + \\\" results\\\"); for(ScoreDoc hit : hits) { int docId = hit.doc; Document doc = searcher.doc(docId); System.out.println(doc.get(\\\"title\\\") + \\\" - \\\" + doc.get(\\\"isbn\\\")); } } private static void addDoc(IndexWriter writer, String title, String isbn) throws IOException { Document doc = new Document(); doc.add(new TextField(\\\"title\\\", title, Field.Store.YES)); doc.add(new StringField(\\\"isbn\\\", isbn, Field.Store.YES)); writer.addDocument(doc); } \"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"深度解析 Lucene 轻量级全文索引实现原理 - 知乎 (zhihu.com)\",\"Lucene简介（一个 Demo 示例）_lucene demo-CSDN博客\"]}],\"customFields\":{\"1\":[\"es\",\"搜索引擎\",\"Java\",\"solr\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/SpringData/Spring%20Data%20JDBC.html\":{\"title\":\"Spring Data JDBC\",\"contents\":[{\"header\":\"准备工作\",\"slug\":\"准备工作\",\"contents\":[\"Spring Data JDBC 是对 JDBC Template 的封装，简化对JDBC数据库连接的操作。在Spring Boot当中，使用JDBC只需要两个包 Spring Data JDBC 的 Starter 包和 数据库驱动包\",\"JDBC Starter 包 内含的Spring Core模块可能会和本身 Spring Boot 内含 Spring Core模块出现版本替换到最新，导致版本不兼容问题\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-jdbc</artifactId> <version>${jdbc.template.version}</version> </dependency> \",\"配置项上和常规数据库连接配置相同，下面使用的是Mysql配置\",\"spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/demo?useUnicode=true&characterEncoding=utf8&useSSL=false&rewriteBatchedStatements=true&serverTimezone=GMT%2b8&nullCatalogMeansCurrent=true spring.datasource.username= spring.datasource.password= spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.auto-commit=true \"]},{\"header\":\"单条数据执行\",\"slug\":\"单条数据执行\",\"contents\":[\"JDBC 查询单挑数据，注入JdbcTemplate 即可\",\"@Autowired public PersonService(JdbcTemplate jdbcTemplate, NamedParameterJdbcTemplate namedParameterJdbcTemplate){ this.jdbcTemplate = jdbcTemplate; this.namedParameterJdbcTemplate = namedParameterJdbcTemplate; } \",\"使用示例：\",\"// 查询 public Person query(String id){ Person person = jdbcTemplate.queryForObject(\\\"select * from tb_person where id = ?\\\", rowMapper, id); return person; } // 保存 public int savePeron(Person person){ Object[] args = new Object[3]; args[0] = person.getId(); args[1] = person.getName(); args[2] = person.getIdCard(); return jdbcTemplate.update(\\\"insert into tb_person values (?,?,?)\\\", args); } // 删除 public int deletePerson(String id){ return jdbcTemplate.update(\\\"delete from tb_person where id = ?\\\", id); } // 修改 public int updatePerson(Person person){ return jdbcTemplate.update(\\\"update tb_person set name = ? where id = ?\\\", person.getName(), person.getId()); } \",\"查询有一个 rowMapper 参数，是jdbc 对查询结果的映射，需要自己编写，这样方便我们接收查询结果。\",\"/** * 数据映射 */ private RowMapper<Person> rowMapper = new RowMapper<Person>() { @Override public Person mapRow(ResultSet resultSet, int i) throws SQLException { Person person = Person.builder().id(resultSet.getString(\\\"id\\\")) .name(resultSet.getString(\\\"name\\\")) .idCard(resultSet.getString(\\\"id_card\\\")) .build(); return person; } }; \"]},{\"header\":\"批量执行\",\"slug\":\"批量执行\",\"contents\":[]},{\"header\":\"JdbcTemplate\",\"slug\":\"jdbctemplate\",\"contents\":[\"对于批量执行，使用JdbcTemplate可以做，如下:\",\"/** * 批量查询 * @return */ public List<Person> queryList(){ List<Person> personList = jdbcTemplate.query(\\\"select * from tb_person\\\", rowMapper); return personList; } /*** * 批量保存 * @param people * @return 返回-2是数据库驱动问题 */ public int[] saveAll(List<Person> people){ // 方法1 List<Object[]> args = new ArrayList<>(); for (Person person : people) { Object[] arg = new Object[3]; arg[0] = person.getId(); arg[1] = person.getName(); arg[2] = person.getIdCard(); args.add(arg); } int[] ints1 = jdbcTemplate.batchUpdate(\\\"insert into tb_person values (?,?,?)\\\", args); // 方法2 int[] ints2 = namedParameterJdbcTemplate.batchUpdate(\\\"insert into tb_person values (:id,:name,:IdCard)\\\", SqlParameterSourceUtils.createBatch(people)); return ints2; } \"]},{\"header\":\"NamedParameterJdbcTemplate\",\"slug\":\"namedparameterjdbctemplate\",\"contents\":[\"可以看到，使用查询时候，JdbcTemplate还算方便，涉及修改操作，JdbcTemplate就稍显麻烦，JDBC框架提供了一系列的工具方便复杂场景使用。NamedParameterJdbcTemplate就是一个，可以通过对sql中的参数命名，帮助我们操作。上述批量保存方法中，方法就是使用了NamedParameterJdbcTemplate的批量保存。其他批量操作也非常简单。\",\"/** * 批量删除 * @param idList * @return */ public int deletePerson(List<String> idList){ Map<String, Object> params = new HashMap<>(); params.put(\\\"param\\\", idList); int update = namedParameterJdbcTemplate.update(\\\"delete from tb_person where id in (:param)\\\", params); return update; } \",\"我们可以使用 :参数名 的形式替换 JdbcTemplate 中的? ，通过\",\"Map<String, Object> params = new HashMap<>(); \",\"来传递参数，Map的key就对应填入参数的位置。\",\"这里还使用了框架内置的SqlParameterSourceUtils静态工具提供的批处理转换方法。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Java\",\"Spring Data\"]}},\"/wait/\":{\"title\":\"Wait\",\"contents\":[]},\"/wait/%E5%B7%A5%E4%BD%9C%E6%B5%81/\":{\"title\":\"工作流\",\"contents\":[]},\"/zh/%E5%85%B6%E4%BB%96/\":{\"title\":\"其他\",\"contents\":[]},\"/zh/\":{\"title\":\"Zh\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/\":{\"title\":\"Java基础\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/\":{\"title\":\"Spring Batch\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/\":{\"title\":\"Spring框架们\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7/\":{\"title\":\"辅助工具\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop/\":{\"title\":\"Hadoop\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/Solr/\":{\"title\":\"Solr\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java%E5%A4%A7%E6%95%B0%E6%8D%AE/%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%9F%BA%E7%A1%80/\":{\"title\":\"中间件基础\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/SpringData/\":{\"title\":\"Spring Data\",\"contents\":[]}}}");self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
