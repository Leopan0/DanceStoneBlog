const g=(o,a)=>{const i=o.toLowerCase(),e=a.toLowerCase(),s=[];let n=0,l=0;const c=(t,p=!1)=>{let r="";l===0?r=t.length>20?`… ${t.slice(-20)}`:t:p?r=t.length+l>100?`${t.slice(0,100-l)}… `:t:r=t.length>20?`${t.slice(0,20)} … ${t.slice(-20)}`:t,r&&s.push(r),l+=r.length,p||(s.push(["strong",a]),l+=a.length,l>=100&&s.push(" …"))};let h=i.indexOf(e,n);if(h===-1)return null;for(;h>=0;){const t=h+e.length;if(c(o.slice(n,h)),n=t,l>100)break;h=i.indexOf(e,n)}return l<100&&c(o.slice(n),!0),s},d=Object.entries,y=Object.keys,f=o=>o.reduce((a,{type:i})=>a+(i==="title"?50:i==="heading"?20:i==="custom"?10:1),0),$=(o,a)=>{var i;const e={};for(const[s,n]of d(a)){const l=((i=a[s.replace(/\/[^\\]*$/,"")])==null?void 0:i.title)||"",c=`${l?`${l} > `:""}${n.title}`,h=g(n.title,o);h&&(e[c]=[...e[c]||[],{type:"title",path:s,display:h}]),n.customFields&&d(n.customFields).forEach(([t,p])=>{p.forEach(r=>{const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"custom",path:s,index:t,display:u}])})});for(const t of n.contents){const p=g(t.header,o);p&&(e[c]=[...e[c]||[],{type:"heading",path:s+(t.slug?`#${t.slug}`:""),display:p}]);for(const r of t.contents){const u=g(r,o);u&&(e[c]=[...e[c]||[],{type:"content",header:t.header,path:s+(t.slug?`#${t.slug}`:""),display:u}])}}}return y(e).sort((s,n)=>f(e[s])-f(e[n])).map(s=>({title:s,contents:e[s]}))},m=JSON.parse("{\"/\":{\"/intro.html\":{\"title\":\"介绍页\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"欢迎来到本站，本站仅是个人代码的记录和分享，相关内容均是开源\",\"加密内容是还没准备好的内容，不用保持好奇\",\"欢迎到媒体站留言或者提交Issue 到Github\",\"本站采用以下技术支撑，感谢开发这些技术的大佬\",\"静态页面引擎：VuePress\",\"主题：vuepress-theme-hope (vuejs.press)\",\"本站内容，转载需要遵照协议注明出处\"]}]},\"/demo/disable.html\":{\"title\":\"布局与功能禁用\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"禁用\"]}},\"/demo/encrypt.html\":{\"title\":\"密码加密的文章\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"文章加密\"]}},\"/demo/markdown.html\":{\"title\":\"Markdown 展示\",\"contents\":[{\"header\":\"Markdown 介绍\",\"slug\":\"markdown-介绍\",\"contents\":[]},{\"header\":\"Markdown 配置\",\"slug\":\"markdown-配置\",\"contents\":[]},{\"header\":\"Markdown 扩展\",\"slug\":\"markdown-扩展\",\"contents\":[]},{\"header\":\"VuePress 扩展\",\"slug\":\"vuepress-扩展\",\"contents\":[]},{\"header\":\"主题扩展\",\"slug\":\"主题扩展\",\"contents\":[]},{\"header\":\"自定义容器\",\"slug\":\"自定义容器\",\"contents\":[]},{\"header\":\"代码块\",\"slug\":\"代码块\",\"contents\":[]},{\"header\":\"上下角标\",\"slug\":\"上下角标\",\"contents\":[]},{\"header\":\"自定义对齐\",\"slug\":\"自定义对齐\",\"contents\":[]},{\"header\":\"Attrs\",\"slug\":\"attrs\",\"contents\":[]},{\"header\":\"脚注\",\"slug\":\"脚注\",\"contents\":[]},{\"header\":\"标记\",\"slug\":\"标记\",\"contents\":[]},{\"header\":\"任务列表\",\"slug\":\"任务列表\",\"contents\":[]},{\"header\":\"图片增强\",\"slug\":\"图片增强\",\"contents\":[]}],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"Markdown\"]}},\"/demo/page.html\":{\"title\":\"页面配置\",\"contents\":[{\"header\":\"页面信息\",\"slug\":\"页面信息\",\"contents\":[]},{\"header\":\"页面内容\",\"slug\":\"页面内容\",\"contents\":[]}],\"customFields\":{\"0\":[\"使用指南\"],\"1\":[\"页面配置\",\"使用指南\"]}},\"/demo/\":{\"title\":\"主要功能与配置演示\",\"contents\":[],\"customFields\":{\"0\":[\"使用指南\"]}},\"/wait/%E5%B7%A5%E4%BD%9C%E6%B5%81/Lua%E8%84%9A%E6%9C%AC.html\":{\"title\":\"Redis与Lua脚本\",\"contents\":[{\"header\":\"如何执行Lua脚本\",\"slug\":\"如何执行lua脚本\",\"contents\":[]},{\"header\":\"执行命令\",\"slug\":\"执行命令\",\"contents\":[]},{\"header\":\"EVAL\",\"slug\":\"eval\",\"contents\":[]},{\"header\":\"EVALSHA\",\"slug\":\"evalsha\",\"contents\":[]},{\"header\":\"辅助命令\",\"slug\":\"辅助命令\",\"contents\":[]},{\"header\":\"script load\",\"slug\":\"script-load\",\"contents\":[]},{\"header\":\"script exists\",\"slug\":\"script-exists\",\"contents\":[]},{\"header\":\"script kill\",\"slug\":\"script-kill\",\"contents\":[]},{\"header\":\"script flush\",\"slug\":\"script-flush\",\"contents\":[]}],\"customFields\":{\"1\":[\"Lua\",\"Redis\"]}},\"/zh/%E5%85%B6%E4%BB%96/%E5%AD%A6%E4%B9%A0%E5%B7%A5%E5%85%B7%E4%BB%AC.html\":{\"title\":\"学习工具论\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"对于软件行业来说，技术更新，非常快，不断学习，是我们行业生涯要做的事。没有天赋的我，便开始研究起学习方法论了。这篇博客，便是介绍我的学习工具，希望对大家有所帮助。\",\"除了软件工具，我想把物质的放在第一位，这些看似无关的物质条件，可以帮助我们保持更好的状态。\",\"首先最重要的便是水，水可以帮助我们更好的思考，解决久坐、熬夜、用眼过度带来的问题，希望大家能多喝水\",\"第二重要的便是咖啡因饮料（无论茶、咖啡还是咖啡因含汽饮料），谁年轻不是精力旺盛的，彷佛一天不够用一样。但是专注对于学习和工作的效率都太重要了，特别是年龄增长以后。我曾经是个拒绝咖啡因的人。上瘾，听起来对人体不太好。但这只是工具。\",\"含碳水的小零食也应该常备，胖子也会低血糖，别低估你的大脑消耗。\",\"相信不少人会有工具纠结症。尤其是开发初学时候，我常常纠结eclipse和IDEA谁更好（别纠结，没什么用）。同样的，做学习笔记，我也不纠结，根据需求去选择工具，相信这样对你有所参考。\",\"首先是博客，记录和分享是我常用的形式，记录让我不会忘记，方便搜索查找，分享会让我认真对待文字，让我过一段时间都能看懂。其次是随手记录，很多笔记软件，往往PC上看着挺好用的，实际很手机上很不方便。当然我是做IT的，代码示例方便与否都是重要的参考项。当然还有成本和可迁移性。\",\"代码上让我首先排除掉Word等一种文档工具，支持代码展示的Markdown一类就是我的首选。通过本地来看我选择Typora，体验太好了，而且我有OneDrive，可以节省很多成本，但是Typora不支持移动端。不能满足我随手记的需求。然后发现了Markdown的云文档产品，可以补充我的随手记需求，语雀、金山文档、Notion都是可以的（大厂，不担心跑路）。最后是分享，开始我是在微信、知乎做分享，后面不想出卖自己的数据，干脆通过GitHub Pages 搞了一个，节省了不少成本和时间。\",\"总体来说就是云文档（我用的免费的）随手记，东西多了可以直接导出Markdown，通过Typora整理保存在OneDrive，不错的通过GitHub Pages分享出来。\",\"国人很注重私密分享，一个好的技术群，能收获不少技术和发展的点，光靠着百度的学习指南，太过概念了，很多细节技术，要多交流才能了解到，所以加个群吧\"]}],\"customFields\":{\"1\":[\"学习方法论\",\"工具\"]}},\"/zh/%E5%85%B6%E4%BB%96/%E6%90%AD%E5%BB%BA%E9%97%AE%E9%A2%98.html\":{\"title\":\"搭建博客\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"搭建过程中遇到一些问题，使用了一些解决方案，在此处写下来，提供给大家参考\"]},{\"header\":\"图片问题\",\"slug\":\"图片问题\",\"contents\":[]},{\"header\":\"路径问题\",\"slug\":\"路径问题\",\"contents\":[\"Vuepress对于带有空格的文件夹支持都不太好，图片无法读取\",\"建议： 不要使用带有空格的文件夹 or 条件允许使用图片服务器吧\",\"但是文件带有空格也会有问题， typora 能正常访问，最好不要使用带有空格的路径\"]},{\"header\":\"CSS渲染问题\",\"slug\":\"css渲染问题\",\"contents\":[\"构建过程中遇到明明dev环境正常查看，但是build后css失效问题，这个和使用的vue-theme-hope主题有关，具体解决方法参考这位博主的文章\",\"关于vuepress-theme-hope运行build后静态网页的css样式失效的问题 - 掘金 (juejin.cn)\"]}]},\"/zh/%E5%89%8D%E7%AB%AF/\":{\"title\":\"前端相关文章\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"前端相关文章\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/\":{\"title\":\"后端\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"后端相关文章\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/\":{\"title\":\"Java\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/Java%20SE%E8%A6%81%E7%82%B9.html\":{\"title\":\"Java SE 要点\",\"contents\":[{\"header\":\"枚举\",\"slug\":\"枚举\",\"contents\":[\"原文：枚举为什么可以用==判断相等\",\"详细：答案是肯定的，因为枚举有着严格的实例化控制，所以你可以用 == 去做比较符，这个用法，在官方文档中也有明确的说明。而且枚举equals（）方法的底层就是用==判断。\",\" /** * Returns true if the specified object is equal to this * enum constant. * * @param other the object to be compared for equality with this object. * @return true if the specified object is equal to this * enum constant. */ public final boolean equals(Object other) { return this==other; } \",\"基础类型的包装类型也用 == 判断\"]},{\"header\":\"Pattern.compile函数的用法\",\"slug\":\"pattern-compile函数的用法\",\"contents\":[\"在使用Pattern.compile函数时，可以加入控制正则表达式的匹配行为的参数： Pattern Pattern.compile(String regex, int flag) \",\"flag的取值范围如下：\",\"Pattern.CANON_EQ 当且仅当两个字符的\\\"正规分解(canonical decomposition)\\\"都完全相同的情况下，才认定匹配。比如用了这个标志之后，表达式\\\"\\\\u030A\\\"会匹配?。默认情况下，不考虑规范相等性(canonical equivalence)\\\"。\",\"Pattern.CASE_INSENSITIVE 默认情况下，大小写不明感的匹配只适用于US-ASCII字符集。这个标志能让表达式忽略大小写进行匹配。要想对Unicode字符进行大小不明感的匹 配，只要将UNICODE_CASE与这个标志合起来就行了。\",\"Pattern.COMMENTS 在这种模式下，匹配时会忽略(正则表达式里的)空格字符(译者注：不是指表达式里的\\\"\\\\s\\\"，而是指表达式里的空格，tab，回车之类)。注释从#开始，一直到这行结束。可以通过嵌入式的标志来启用Unix行模式。\",\"Pattern.DOTALL 在这种模式下，表达式'.'可以匹配任意字符，包括表示一行的结束符。默认情况下，表达式'.'不匹配行的结束符。\",\"Pattern.MULTILINE 在这种模式下， ^ 和 $ 分别匹配一行的开始和结束。此外，^ 仍然匹配字符串的开始，$ 也匹配字符串的结束。默认情况下，这两个表达式仅仅匹配字符串的开始和结束。\",\"Pattern.UNICODE_CASE 在这个模式下，如果你还启用了CASE_INSENSITIVE标志，那么它会对Unicode字符进行大小写不明感的匹配。默认情况下，大小写不敏感的匹配只适用于US-ASCII字符集。\",\"Pattern.UNIX_LINES 在这个模式下，只有'\\\\n'才被认作一行的中止，并且与.，^，以及$进行匹配。\"]},{\"header\":\"lamda表达式\",\"slug\":\"lamda表达式\",\"contents\":[]},{\"header\":\"ParallelStream\",\"slug\":\"parallelstream\",\"contents\":[\"​\\t\\tparallelStream 是 Java lamda表达式当中并行流写法，本质上并行方法，效率很高，但是需要注意会产生多线程下的各种问题，比如使用非线程安全的集合类，会导致空指针和数组下标越界等问题。如果使用.collect最终将结果收集起来就不会有个这个问题或者使用CopyOnWriteArrayList， CopyOnWriteArraySet 这类线程安全的集合来避免问题。千万不能使用 parallelStream 去循环处理非线程安全的流程。\"]}],\"customFields\":{\"1\":[\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/%E7%BA%BF%E7%A8%8B%E6%B1%A0.html\":{\"title\":\"线程池\",\"contents\":[{\"header\":\"始\",\"slug\":\"始\",\"contents\":[\"​ 多线程是项目扩大以后，必不可少需要应用到的技术。线程池的应用，更加便利多线程的使用。 Java 默认支持线程池，在 Java 线程池的基础上，Pivotal 在 Spring 框架内，提供了 Spring 线程池。两者皆是主流的线程池。\"]},{\"header\":\"基本概念\",\"slug\":\"基本概念\",\"contents\":[\"线程池，本质上是一种对象池，用于管理线程资源。\",\"在任务执行前，需要从线程池中拿出线程来执行。\",\"在任务执行完成之后，需要把线程放回线程池。\",\"通过线程的这种反复利用机制，可以有效地避免直接创建线程所带来的坏处。\"]},{\"header\":\"优势\",\"slug\":\"优势\",\"contents\":[\"降低资源的消耗。线程本身是一种资源，创建和销毁线程会有CPU开销；创建的线程也会占用一定的内存。\",\"提高任务执行的响应速度。任务执行时，可以不必等到线程创建完之后再执行。\",\"提高线程的可管理性。线程不能无限制地创建，需要进行统一的分配、调优和监控。\"]},{\"header\":\"缺点\",\"slug\":\"缺点\",\"contents\":[\"频繁的线程创建和销毁会占用更多的CPU和内存\",\"频繁的线程创建和销毁会对 GC 产生比较大的压力\",\"线程太多，线程切换带来的开销将不可忽视\",\"线程太少，多核CPU得不到充分利用，是一种浪费\"]},{\"header\":\"线程池处理流程\",\"slug\":\"线程池处理流程\",\"contents\":[\"判断核心线程池是否已满，如果不是，则创建线程执行任务\",\"如果核心线程池满了，判断队列是否满了，如果队列没满，将任务放在队列中\",\"如果队列满了，则判断线程池是否已满，如果没满，创建线程执行任务\",\"如果线程池也满了，则按照拒绝策略对任务进行处理\"]},{\"header\":\"Java 线程池\",\"slug\":\"java-线程池\",\"contents\":[\"JDK 里 ThreadPoolExecutor 的线程池处理流程\"]},{\"header\":\"入门案例\",\"slug\":\"入门案例\",\"contents\":[\"public class ThreadPoolTest { public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(5); for (int i = 0; i < 10; i++) { executor.submit(() -> { System.out.println(\\\"thread id is: \\\" + Thread.currentThread().getId()); try { Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } }); } } } \",\"​ 在这个例子中，我们首先创建了一个固定长度为5的线程池。然后使用循环的方式往线程池中提交了10个任务，每个任务休眠1秒。在任务休眠之前，将任务所在的线程id进行打印输出。所以，理论上只会打印5个不同的线程id，且每个线程id会被打印2次。\"]},{\"header\":\"Executors-Java线程池\",\"slug\":\"executors-java线程池\",\"contents\":[\"Executors是一个线程池工厂，提供了很多的工厂方法。\",\"// 创建单一线程的线程池 public static ExecutorService newSingleThreadExecutor(); // 创建固定数量的线程池 public static ExecutorService newFixedThreadPool(int nThreads); // 创建带缓存的线程池 public static ExecutorService newCachedThreadPool(); // 创建定时调度的线程池 public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize); // 创建流式（fork-join）线程池 public static ExecutorService newWorkStealingPool(); \"]},{\"header\":\"创建单一线程的线程池\",\"slug\":\"创建单一线程的线程池\",\"contents\":[\"顾名思义，这个线程池只有一个线程。若多个任务被提交到此线程池，那么会被缓存到队列（队列长度为Integer.MAX_VALUE）。当线程空闲的时候，按照FIFO的方式进行处理。\"]},{\"header\":\"创建固定数量的线程池\",\"slug\":\"创建固定数量的线程池\",\"contents\":[\"和 创建单一线程的线程池 类似，只是这儿可以并行处理任务的线程数更多一些罢了。若多个任务被提交到此线程池，会有下面的处理过程。\",\"如果线程的数量未达到指定数量，则创建线程来执行任务\",\"如果线程池的数量达到了指定数量，并且有线程是空闲的，则取出空闲线程执行任务\",\"如果没有线程是空闲的，则将任务缓存到队列（队列长度为Integer.MAX_VALUE）。当线程空闲的时候，按照FIFO的方式进行处理\"]},{\"header\":\"创建带缓存的线程池\",\"slug\":\"创建带缓存的线程池\",\"contents\":[\"这种方式创建的线程池，核心线程池的长度为0，线程池最大长度为Integer.MAX_VALUE 。由于本身使用SynchronousQueue 作为等待队列的缘故，导致往队列里面每插入一个元素，必须等待另一个线程从这个队列删除一个元素。\"]},{\"header\":\"创建定时调度的线程池\",\"slug\":\"创建定时调度的线程池\",\"contents\":[\"和上面3个工厂方法返回的线程池类型有所不同，它返回的是ScheduledThreadPoolExecutor类型的线程池。平时我们实现定时调度功能的时候，可能更多的是使用第三方类库，比如：quartz等。但是对于更底层的功能，我们仍然需要了解。下面是案例\",\"public class ThreadPoolTest { public static void main(String[] args) { ScheduledExecutorService executor = Executors.newScheduledThreadPool(2); // 定时调度，每个调度任务会至少等待`period`的时间， // 如果任务执行的时间超过`period`，则等待的时间为任务执行的时间 executor.scheduleAtFixedRate(() -> { try { Thread.sleep(10000); System.out.println(System.currentTimeMillis() / 1000); } catch (InterruptedException e) { e.printStackTrace(); } }, 0, 2, TimeUnit.SECONDS); // 定时调度，第二个任务执行的时间 = 第一个任务执行时间 + `delay` executor.scheduleWithFixedDelay(() -> { try { Thread.sleep(5000); System.out.println(System.currentTimeMillis() / 1000); } catch (InterruptedException e) { e.printStackTrace(); } }, 0, 2, TimeUnit.SECONDS); // 定时调度，延迟`delay`后执行，且只执行一次 executor.schedule(() -> System.out.println(\\\"5 秒之后执行 schedule\\\"), 5, TimeUnit.SECONDS); } } \",\"上述代码不同调度方法简述：\",\"scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)，定时调度，每个调度任务会至少等待 period 的时间，如果任务执行的时间超过period，则等待的时间为任务执行的时间\",\"scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)，定时调度，第二个任务执行的时间 = 第一个任务执行时间 + `delay\",\"schedule(Runnable command, long delay, TimeUnit unit)，定时调度，延迟delay后执行，且只执行一次\"]},{\"header\":\"手动创建线程池\",\"slug\":\"手动创建线程池\",\"contents\":[\"​ 理论上，我们可以通过Executors来创建线程池，这种方式非常简单。但正是因为简单，所以限制了线程池的功能。比如：无长度限制的队列，可能因为任务堆积导致OOM，这是非常严重的bug，应尽可能地避免。怎么避免？归根结底，还是需要我们通过更底层的方式来创建线程池。\",\"​ 抛开定时调度的线程池不管，我们看看ThreadPoolExecutor。它提供了好几个构造方法，但是最底层的构造方法却只有一个。那么，我们就从这个构造方法着手分析。\",\"public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue<Runnable> workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler); \",\"参数解析：\",\"corePoolSize，线程池中的核心线程数\",\"maximumPoolSize，线程池中的最大线程数\",\"keepAliveTime，空闲时间，当线程池数量超过核心线程数时，多余的空闲线程存活的时间，即：这些线程多久被销毁。\",\"unit，空闲时间的单位，可以是毫秒、秒、分钟、小时和天，等等\",\"workQueue，等待队列，线程池中的线程数超过核心线程数时，任务将放在等待队列，它是一个BlockingQueue类型的对象\",\"threadFactory，线程工厂，我们可以使用它来创建一个线程\",\"handler，拒绝策略，当线程池和等待队列都满了之后，需要通过该对象的回调函数进行回调处理\"]},{\"header\":\"等待队列-workQueue\",\"slug\":\"等待队列-workqueue\",\"contents\":[\"等待队列是BlockingQueue类型的，理论上只要是它的子类，我们都可以用来作为等待队列。\",\"同时，jdk内部自带一些阻塞队列，我们来看看大概有哪些。\",\"ArrayBlockingQueue，队列是有界的，基于数组实现的阻塞队列\",\"LinkedBlockingQueue，队列可以有界，也可以无界。基于链表实现的阻塞队列\",\"SynchronousQueue，不存储元素的阻塞队列，每个插入操作必须等到另一个线程调用移除操作，否则插入操作将一直处于阻塞状态。该队列也是Executors.newCachedThreadPool()的默认队列\",\"PriorityBlockingQueue，带优先级的无界阻塞队列\",\"通常情况下，我们需要指定阻塞队列的上界（比如1024）。另外，如果执行的任务很多，我们可能需要将任务进行分类，然后将不同分类的任务放到不同的线程池中执行。\"]},{\"header\":\"线程工厂-threadFactory\",\"slug\":\"线程工厂-threadfactory\",\"contents\":[\"ThreadFactory是一个接口，只有一个方法。既然是线程工厂，那么我们就可以用它生产一个线程对象。来看看这个接口的定义。\",\"public interface ThreadFactory { /** * Constructs a new {@code Thread}. Implementations may also initialize * priority, name, daemon status, {@code ThreadGroup}, etc. * * @param r a runnable to be executed by new thread instance * @return constructed thread, or {@code null} if the request to * create a thread is rejected */ Thread newThread(Runnable r); } \",\"Executors的实现使用了默认的线程工厂-DefaultThreadFactory。它的实现主要用于创建一个线程，线程的名字为pool-{poolNum}-thread-{threadNum}。\",\"static class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; DefaultThreadFactory() { SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = \\\"pool-\\\" + poolNumber.getAndIncrement() + \\\"-thread-\\\"; } public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; } } \",\"很多时候，我们需要自定义线程名字。我们只需要自己实现ThreadFactory，用于创建特定场景的线程即可。\"]},{\"header\":\"拒绝策略-handler\",\"slug\":\"拒绝策略-handler\",\"contents\":[\"所谓拒绝策略，就是当线程池满了、队列也满了的时候，我们对任务采取的措施。或者丢弃、或者执行、或者其他...\",\"jdk自带4种拒绝策略，我们来看看。\",\"CallerRunsPolicy // 在调用者线程执行\",\"AbortPolicy // 直接抛出RejectedExecutionException异常\",\"DiscardPolicy // 任务直接丢弃，不做任何处理\",\"DiscardOldestPolicy // 丢弃队列里最旧的那个任务，再尝试执行当前任务\",\"这四种策略各有优劣，比较常用的是DiscardPolicy，但是这种策略有一个弊端就是任务执行的轨迹不会被记录下来。所以，我们往往需要实现自定义的拒绝策略， 通过实现RejectedExecutionHandler接口的方式。\"]},{\"header\":\"提交任务的几种方式\",\"slug\":\"提交任务的几种方式\",\"contents\":[\"往线程池中提交任务，主要有两种方法，execute()和submit()。\",\"execute()用于提交不需要返回结果的任务，我们看一个例子。\",\"public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(2); executor.execute(() -> System.out.println(\\\"hello\\\")); } \",\"submit()用于提交一个需要返回结果的任务。该方法返回一个Future对象，通过调用这个对象的get()方法，我们就能获得返回结果。get()方法会一直阻塞，直到返回结果返回。另外，我们也可以使用它的重载方法get(long timeout, TimeUnit unit)，这个方法也会阻塞，但是在超时时间内仍然没有返回结果时，将抛出异常TimeoutException。\",\"public static void main(String[] args) throws Exception { ExecutorService executor = Executors.newFixedThreadPool(2); Future<Long> future = executor.submit(() -> { System.out.println(\\\"task is executed\\\"); return System.currentTimeMillis(); }); System.out.println(\\\"task execute time is: \\\" + future.get()); } \"]},{\"header\":\"关闭线程池\",\"slug\":\"关闭线程池\",\"contents\":[\"在线程池使用完成之后，我们需要对线程池中的资源进行释放操作，这就涉及到关闭功能。我们可以调用线程池对象的 shutdown() 和 shutdownNow() 方法来关闭线程池。\",\"这两个方法都是关闭操作，又有什么不同呢？\",\"shutdown()会将线程池状态置为SHUTDOWN，不再接受新的任务，同时会等待线程池中已有的任务执行完成再结束。\",\"shutdownNow()会将线程池状态置为SHUTDOWN，对所有线程执行interrupt()操作，清空队列，并将队列中的任务返回回来。\",\"另外，关闭线程池涉及到两个返回boolean的方法，isShutdown()和isTerminated，分别表示是否关闭和是否终止。\"]},{\"header\":\"如何正确配置线程池的参数\",\"slug\":\"如何正确配置线程池的参数\",\"contents\":[\"前面我们讲到了手动创建线程池涉及到的几个参数，那么我们要如何设置这些参数才算是正确的应用呢？实际上，需要根据任务的特性来分析。\",\"任务的性质：CPU密集型、IO密集型和混杂型\",\"任务的优先级：高中低\",\"任务执行的时间：长中短\",\"任务的依赖性：是否依赖数据库或者其他系统资源\",\"不同的性质的任务，我们采取的配置将有所不同。在《Java并发编程实践》中有相应的计算公式。\",\"通常来说，如果任务属于CPU密集型，那么我们可以将线程池数量设置成CPU的个数，以减少线程切换带来的开销。如果任务属于IO密集型，我们可以将线程池数量设置得更多一些，比如CPU个数*2。\",\"PS：我们可以通过Runtime.getRuntime().availableProcessors()来获取CPU的个数\"]},{\"header\":\"线程池监控\",\"slug\":\"线程池监控\",\"contents\":[\"如果系统中大量用到了线程池，那么我们有必要对线程池进行监控。利用监控，我们能在问题出现前提前感知到，也可以根据监控信息来定位可能出现的问题。\",\"那么我们可以监控哪些信息？又有哪些方法可用于我们的扩展支持呢？\",\"首先，ThreadPoolExecutor自带了一些方法。\",\"long getTaskCount()，获取已经执行或正在执行的任务数\",\"long getCompletedTaskCount()，获取已经执行的任务数\",\"int getLargestPoolSize()，获取线程池曾经创建过的最大线程数，根据这个参数，我们可以知道线程池是否满过\",\"int getPoolSize()，获取线程池线程数\",\"int getActiveCount()，获取活跃线程数（正在执行任务的线程数）\",\"其次，ThreadPoolExecutor留给我们自行处理的方法有3个，它在ThreadPoolExecutor中为空实现（也就是什么都不做）。\",\"protected void beforeExecute(Thread t, Runnable r) // 任务执行前被调用\",\"protected void afterExecute(Runnable r, Throwable t) // 任务执行后被调用\",\"protected void terminated() // 线程池结束后被调用\",\"public class ThreadPoolTest { public static void main(String[] args) { ExecutorService executor = new ThreadPoolExecutor(1, 1, 1, TimeUnit.SECONDS, new ArrayBlockingQueue<>(1)) { @Override protected void beforeExecute(Thread t, Runnable r) { System.out.println(\\\"beforeExecute is called\\\"); } @Override protected void afterExecute(Runnable r, Throwable t) { System.out.println(\\\"afterExecute is called\\\"); } @Override protected void terminated() { System.out.println(\\\"terminated is called\\\"); } }; executor.submit(() -> System.out.println(\\\"this is a task\\\")); executor.shutdown(); } } \",\"PS：在使用submit()的时候一定要注意它的返回对象Future，为了避免任务执行异常被吞掉的问题，我们需要调用Future.get()方法。另外，使用execute()将不会出现这种问题。\"]},{\"header\":\"Spring 线程池\",\"slug\":\"spring-线程池\",\"contents\":[]},{\"header\":\"常用线程池\",\"slug\":\"常用线程池\",\"contents\":[]},{\"header\":\"TaskExecutor接口相关实现类\",\"slug\":\"taskexecutor接口相关实现类\",\"contents\":[\"名字\",\"特点\",\"SimpleAsyncTaskExecutor\",\"每次请求新开线程，没有最大线程数设置.不是真的线程池，这个类不重用线程，每次调用都会创建一个新的线程。 --【1】\",\"SyncTaskExecutor\",\"不是异步的线程.同步可以用SyncTaskExecutor，但这个可以说不算一个线程池，因为还在原线程执行。这个类没有实现异步调用，只是一个同步操作。\",\"ConcurrentTaskExecutor\",\"Executor的适配类，不推荐使用。如果ThreadPoolTaskExecutor不满足要求时，才用考虑使用这个类。\",\"SimpleThreadPoolTaskExecutor\",\"监听Spring’s lifecycle callbacks，并且可以和Quartz的Component兼容.是Quartz的SimpleThreadPool的类。线程池同时被quartz和非quartz使用，才需要使用此类。\",\"ThreadPoolTaskExecutor\",\"最常用。要求jdk版本大于等于5。可以在程序而不是xml里修改线程池的配置.其实质是对java.util.concurrent.ThreadPoolExecutor的包装。\",\"TimerTaskExecutor\",\"此实现使用CommonJ WorkManager作为其后备服务提供程序，并且是在Spring应用程序上下文中在WebLogic或WebSphere上设置基于CommonJ的线程池集成的中心便利类。\",\"WorkManagerTaskExecutor\",\"此实现在JSR-236兼容的运行时环境（例如Java EE 7+应用程序服务器）中使用JNDI获取的ManagedExecutorService，为此目的替换CommonJ WorkManager。(说明了就是依赖环境)\"]},{\"header\":\"相关注解\",\"slug\":\"相关注解\",\"contents\":[\"注解名\",\"解释\",\"@EnableAsync\",\"开启异步执行。官方文档中解释:该注解添加到@Configuration标注的类上以开始异步执行。开启后@Async标注的方法或类即可异步执行。\",\"@Async\",\"异步执行注解。可标注类和方法。标注类时，则该类下所有方法均可使用异步执行。标注方法时，则该方法可使用异步执行。当标注有@Configuration注解的配置类上标注了@EnableAsync注解后即可生效。\"]},{\"header\":\"SyncTaskExecutor 同步线程池\",\"slug\":\"synctaskexecutor-同步线程池\",\"contents\":[\"SyncTaskExecutor：同步可以用SyncTaskExecutor，但这个可以说不算一个线程池，因为还在原线程执行。这个类没有实现异步调用，只是一个同步操作。\",\"也可以用 ThreadPoolTaskExecutor 结合 FutureTask 做到同步。\",\"SyncTaskExecutor与ThreadPoolTaskExecutor区别，前者是同步执行器，执行任务同步，后者是线程池，执行任务异步。\"]},{\"header\":\"Spring 异步线程池实现原理\",\"slug\":\"spring-异步线程池实现原理\",\"contents\":[\"@EnableAsync 注解加入时，导入类AsyncConfigurationSelector，在容器种注册一个ProxyAsyncConfiguration，继承关系如下\",\"AbstractAsyncConfiguration 源码：\",\"@Configuration public abstract class AbstractAsyncConfiguration implements ImportAware { @Nullable protected AnnotationAttributes enableAsync; @Nullable protected Supplier<Executor> executor; @Nullable protected Supplier<AsyncUncaughtExceptionHandler> exceptionHandler; // 这里主要就是检查将其导入的类上是否有EnableAsync注解 // 如果没有的话就报错 @Override public void setImportMetadata(AnnotationMetadata importMetadata) { this.enableAsync = AnnotationAttributes.fromMap( importMetadata.getAnnotationAttributes(EnableAsync.class.getName(), false)); if (this.enableAsync == null) { throw new IllegalArgumentException( \\\"@EnableAsync is not present on importing class \\\" + importMetadata.getClassName()); } } // 将容器中配置的AsyncConfigurer注入 @Autowired(required = false) void setConfigurers(Collection<AsyncConfigurer> configurers) { if (CollectionUtils.isEmpty(configurers)) { return; } if (configurers.size() > 1) { throw new IllegalStateException(\\\"Only one AsyncConfigurer may exist\\\"); } AsyncConfigurer configurer = configurers.iterator().next(); // 异步执行嘛，所以我们可以配置使用的线程池 this.executor = configurer::getAsyncExecutor; // 另外也可以配置异常处理器 this.exceptionHandler = configurer::getAsyncUncaughtExceptionHandler; } } \",\"ProxyAsyncConfiguration 源码：\",\" @Configuration @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public class ProxyAsyncConfiguration extends AbstractAsyncConfiguration { @Bean(name = TaskManagementConfigUtils.ASYNC_ANNOTATION_PROCESSOR_BEAN_NAME) @Role(BeanDefinition.ROLE_INFRASTRUCTURE) public AsyncAnnotationBeanPostProcessor asyncAdvisor() { AsyncAnnotationBeanPostProcessor bpp = new AsyncAnnotationBeanPostProcessor(); // 将通过AsyncConfigurer配置好的线程池跟异常处理器设置到这个后置处理器中 bpp.configure(this.executor, this.exceptionHandler); Class<? extends Annotation> customAsyncAnnotation = this.enableAsync.getClass(\\\"annotation\\\"); if (customAsyncAnnotation != AnnotationUtils.getDefaultValue(EnableAsync.class, \\\"annotation\\\")) { bpp.setAsyncAnnotationType(customAsyncAnnotation); } // bpp.setProxyTargetClass(this.enableAsync.getBoolean(\\\"proxyTargetClass\\\")); bpp.setOrder(this.enableAsync.<Integer>getNumber(\\\"order\\\")); return bpp; } } \",\"​ 这个类本身是一个配置类，它的作用是向容器中添加一个AsyncAnnotationBeanPostProcessor。到这一步我们基本上就可以明白了，@Async注解的就是通过AsyncAnnotationBeanPostProcessor这个后置处理器生成一个代理对象来实现异步的，接下来我们就具体看看AsyncAnnotationBeanPostProcessor是如何生成代理对象的，我们主要关注一下几点即可：\",\"是在生命周期的哪一步完成的代理？\",\"切点的逻辑是怎么样的？它会对什么样的类进行拦截？\",\"通知的逻辑是怎么样的？是如何实现异步的？\"]},{\"header\":\"是在生命周期的哪一步完成的代理？\",\"slug\":\"是在生命周期的哪一步完成的代理\",\"contents\":[\"​ 在这个后置处理器的postProcessAfterInitialization方法中完成了代理，直接定位到这个方法，这个方法位于父类AbstractAdvisingBeanPostProcessor中，具体代码如下：\",\"public Object postProcessAfterInitialization(Object bean, String beanName) { // 没有通知，或者是AOP的基础设施类，那么不进行代理 if (this.advisor == null || bean instanceof AopInfrastructureBean) { return bean; } // 对已经被代理的类，不再生成代理，只是将通知添加到代理类的逻辑中 // 这里通过beforeExistingAdvisors决定是将通知添加到所有通知之前还是添加到所有通知之后 // 在使用@Async注解的时候，beforeExistingAdvisors被设置成了true // 意味着整个方法及其拦截逻辑都会异步执行 if (bean instanceof Advised) { Advised advised = (Advised) bean; if (!advised.isFrozen() && isEligible(AopUtils.getTargetClass(bean))) { if (this.beforeExistingAdvisors) { advised.addAdvisor(0, this.advisor); } else { advised.addAdvisor(this.advisor); } return bean; } } // 判断需要对哪些Bean进行来代理 if (isEligible(bean, beanName)) { ProxyFactory proxyFactory = prepareProxyFactory(bean, beanName); if (!proxyFactory.isProxyTargetClass()) { evaluateProxyInterfaces(bean.getClass(), proxyFactory); } proxyFactory.addAdvisor(this.advisor); customizeProxyFactory(proxyFactory); return proxyFactory.getProxy(getProxyClassLoader()); } return bean; } \"]},{\"header\":\"接着我们就要思考，切点的过滤规则是什么呢？\",\"slug\":\"接着我们就要思考-切点的过滤规则是什么呢\",\"contents\":[\"类上添加了@Async注解或者类中含有被@Async注解修饰的方法。基于此，我们看看这个isEligible这个方法的实现逻辑，这个方位位于AbstractBeanFactoryAwareAdvisingPostProcessor中，也是AsyncAnnotationBeanPostProcessor的父类，对应代码如下：\",\"// AbstractBeanFactoryAwareAdvisingPostProcessor的isEligible方法 // 调用了父类 protected boolean isEligible(Object bean, String beanName) { return (!AutoProxyUtils.isOriginalInstance(beanName, bean.getClass()) && super.isEligible(bean, beanName)); } protected boolean isEligible(Object bean, String beanName) { return isEligible(bean.getClass()); } protected boolean isEligible(Class<?> targetClass) { Boolean eligible = this.eligibleBeans.get(targetClass); if (eligible != null) { return eligible; } if (this.advisor == null) { return false; } // 这里完成的判断 eligible = AopUtils.canApply(this.advisor, targetClass); this.eligibleBeans.put(targetClass, eligible); return eligible; } \",\"实际上最后就是根据advisor来确定是否要进行代理，在Spring中AOP相关的API及源码解析，原来AOP是这样子的这篇文章中我们提到过，advisor实际就是一个绑定了切点的通知，那么AsyncAnnotationBeanPostProcessor这个advisor是什么时候被初始化的呢？我们直接定位到AsyncAnnotationBeanPostProcessor的setBeanFactory方法，其源码如下：\",\"public void setBeanFactory(BeanFactory beanFactory) { super.setBeanFactory(beanFactory); // 在这里new了一个AsyncAnnotationAdvisor AsyncAnnotationAdvisor advisor = new AsyncAnnotationAdvisor(this.executor, this.exceptionHandler); if (this.asyncAnnotationType != null) { advisor.setAsyncAnnotationType(this.asyncAnnotationType); } advisor.setBeanFactory(beanFactory); // 完成了初始化 this.advisor = advisor; } // 我们来看看AsyncAnnotationAdvisor中的切点匹配规程是怎么样的，直接定位到这个类的buildPointcut方法中，其源码如下： protected Pointcut buildPointcut(Set<Class<? extends Annotation>> asyncAnnotationTypes) { ComposablePointcut result = null; for (Class<? extends Annotation> asyncAnnotationType : asyncAnnotationTypes) { // 就是根据这两个匹配器进行匹配的 Pointcut cpc = new AnnotationMatchingPointcut(asyncAnnotationType, true); Pointcut mpc = new AnnotationMatchingPointcut(null, asyncAnnotationType, true); if (result == null) { result = new ComposablePointcut(cpc); } else { result.union(cpc); } result = result.union(mpc); } return (result != null ? result : Pointcut.TRUE); } \",\"代码很简单，就是根据cpc跟mpc两个匹配器来进行匹配的，第一个是检查类上是否有@Async注解，第二个是检查方法是是否有@Async注解。\"]},{\"header\":\"通知的逻辑是怎么样的？是如何实现异步的？\",\"slug\":\"通知的逻辑是怎么样的-是如何实现异步的\",\"contents\":[\"前面也提到了advisor是一个绑定了切点的通知，前面分析了它的切点，那么现在我们就来看看它的通知逻辑，直接定位到AsyncAnnotationAdvisor中的buildAdvice方法，源码如下：\",\"protected Advice buildAdvice(@Nullable Supplier<Executor> executor, @Nullable Supplier<AsyncUncaughtExceptionHandler> exceptionHandler) { AnnotationAsyncExecutionInterceptor interceptor = new AnnotationAsyncExecutionInterceptor(null); interceptor.configure(executor, exceptionHandler); return interceptor; } \",\"简单吧，加了一个拦截器而已，对于interceptor类型的对象，我们关注它的核心方法invoke就行了，代码如下：\",\"public Object invoke(final MethodInvocation invocation) throws Throwable { Class<?> targetClass = (invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null); Method specificMethod = ClassUtils.getMostSpecificMethod(invocation.getMethod(), targetClass); final Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); // 异步执行嘛，先获取到一个线程池 AsyncTaskExecutor executor = determineAsyncExecutor(userDeclaredMethod); if (executor == null) { throw new IllegalStateException( \\\"No executor specified and no default executor set on AsyncExecutionInterceptor either\\\"); } // 然后将这个方法封装成一个 Callable对象传入到线程池中执行 Callable<Object> task = () -> { try { Object result = invocation.proceed(); if (result instanceof Future) { return ((Future<?>) result).get(); } } catch (ExecutionException ex) { handleError(ex.getCause(), userDeclaredMethod, invocation.getArguments()); } catch (Throwable ex) { handleError(ex, userDeclaredMethod, invocation.getArguments()); } return null; }; // 将任务提交到线程池 return doSubmit(task, executor, invocation.getMethod().getReturnType()); } \"]},{\"header\":\"Spring 异步线程池\",\"slug\":\"spring-异步线程池\",\"contents\":[\"Spring 异步线程池配置\",\"@Configuration // @EnableAsync添加到配置文件或者Spring Boot 启动类上 @EnableAsync // 继承 AsyncConfigurer 接口 public class CustomAsyncConfigurer implements AsyncConfigurer { /** * 设置线程池相关的配置 * @return ThreadPoolTaskExecutor */ @Override public Executor getAsyncExecutor() { // 使用Spring Boot 异步线程池 ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(10); // 设置核心线程池大小(这里设置为初始化10个) executor.setMaxPoolSize(30); // 设置最大线程池大小(当核心线程池不够用时候，会自动在原基础上增加。最大为30个) executor.setQueueCapacity(2000); // 设置队列容量为2000个。 executor.initialize(); return executor; } } \",\"// @Async 加在类或者方法上声明这是个异步类或者异步方法 @Async public void testThreadPoolTaskExecutor(Long id) { System.err.println(\\\"--- testThreadPoolTaskExecutor --\\\" + id); } \"]},{\"header\":\"SimpleAsyncTaskExecutor\",\"slug\":\"simpleasynctaskexecutor\",\"contents\":[\"​ 异步执行用户任务的 SimpleAsyncTaskExecutor 。每次执行客户提交给它的任务时，它会启动新的线程，并允许开发者控制并发线程的上限（concurrencyLimit），从而起到一定的资源节流作用。默认时，concurrencyLimit 取值为 -1，即不启用资源节流。\",\"SimpleAsyncTaskExecutor实现源码：\",\"public class SimpleAsyncTaskExecutor extends CustomizableThreadCreator implements AsyncListenableTaskExecutor, Serializable { //限流主要实现 private final SimpleAsyncTaskExecutor.ConcurrencyThrottleAdapter concurrencyThrottle = new SimpleAsyncTaskExecutor.ConcurrencyThrottleAdapter(); private ThreadFactory threadFactory; //设置最大的线程数量 public void setConcurrencyLimit(int concurrencyLimit) { this.concurrencyThrottle.setConcurrencyLimit(concurrencyLimit); } //是否开启了限流 限流数量大于0？ public final boolean isThrottleActive() { return this.concurrencyThrottle.isThrottleActive(); } public void execute(Runnable task, long startTimeout) { Assert.notNull(task, \\\"Runnable must not be null\\\"); //1.是否开启限流 否则不开启限流处理 if(this.isThrottleActive() && startTimeout > 0L) { //2.执行开始之前检测是否可以满足要求 当前数量++ this.concurrencyThrottle.beforeAccess(); //3.开启限流将执行的Runable进行封装，执行完成调用final方法 当前数量-- this.doExecute(new SimpleAsyncTaskExecutor.ConcurrencyThrottlingRunnable(task)); } else { this.doExecute(task); } } //异步提交有返回值 public Future<?> submit(Runnable task) { FutureTask future = new FutureTask(task, (Object)null); this.execute(future, 9223372036854775807L); return future; } public <T> Future<T> submit(Callable<T> task) { FutureTask future = new FutureTask(task); this.execute(future, 9223372036854775807L); return future; } public ListenableFuture<?> submitListenable(Runnable task) { ListenableFutureTask future = new ListenableFutureTask(task, (Object)null); this.execute(future, 9223372036854775807L); return future; } public <T> ListenableFuture<T> submitListenable(Callable<T> task) { ListenableFutureTask future = new ListenableFutureTask(task); this.execute(future, 9223372036854775807L); return future; } protected void doExecute(Runnable task) { //拥有工厂？没有的话调用父类可以设置各种参数的创建线程 Thread thread = this.threadFactory != null?this.threadFactory.newThread(task):this.createThread(task); thread.start(); } //父类的方法，方便配置线程，方便xml设置线程参数CustomizableThreadCreator public Thread createThread(Runnable runnable) { Thread thread = new Thread(getThreadGroup(), runnable, nextThreadName()); thread.setPriority(getThreadPriority()); thread.setDaemon(isDaemon()); return thread; } } \"]},{\"header\":\"ThreadPoolTaskExecutor\",\"slug\":\"threadpooltaskexecutor\",\"contents\":[\"ThreadPoolTaskExecutor 拥有强大的功能，相比 SimpleAsyncTaskExecutor， 提供了线程复用，不用每一次都新启一个线程，效率更高\"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"[Java线程池的使用 - 简书 (jianshu.com)https://www.cnblogs.com/duanxz/p/9435343.html)\",\"Spring自带的线程池ThreadPoolTaskExecutor - 知乎 (zhihu.com)\",\"spring任务执行器与任务调度器(TaskExecutor And TaskScheduler) - 简书 (jianshu.com)\",\"(4条消息) @Async的使用、原理及使用时可能导致的问题_程序猿DD-CSDN博客\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatch.html\":{\"title\":\"Spring Batch Java 批处理框架\",\"contents\":[{\"header\":\"Spring Batch 概念\",\"slug\":\"spring-batch-概念\",\"contents\":[]},{\"header\":\"批处理的核心场景\",\"slug\":\"批处理的核心场景\",\"contents\":[\"从某个位置读取大量的记录，位置可以是数据库、文件或者外部推送队列（MQ）。\",\"根据业务需要实时处理读取的数据。\",\"将处理后的数据写入某个位置，可以第一条一样，可以是数据库、文件或者推送到队列。\"]},{\"header\":\"Spring Batch能解决的批处理场景\",\"slug\":\"spring-batch能解决的批处理场景\",\"contents\":[\"​ Spring Batch为批处理提供了一个轻量化的解决方案，它根据批处理的需要迭代处理各种记录，提供事物功能。但是Spring Batch仅仅适用于\\\"脱机\\\"场景，在处理的过程中不能和外部进行任何交互，也不允许有任何输入。\"]},{\"header\":\"Spring Batch的目标\",\"slug\":\"spring-batch的目标\",\"contents\":[\"开发人员仅关注业务逻辑，底层框架的交互交由Spring Batch去处理。\",\"能够清晰分离业务与框架，框架已经限定了批处理的业务切入点，业务开发只需关注这些切入点（Read、Process、Write）。\",\"提供开箱即用的通用接口。\",\"快速轻松的融入Spring 框架，基于Spring Framework能够快速扩展各种功能。\",\"所有现有核心服务都应易于更换或扩展，而不会对基础架构层产生任何影响。\"]},{\"header\":\"Spring Batch结构\",\"slug\":\"spring-batch结构\",\"contents\":[\"​ 如上图，通常情况下一个独立的JVM程序就是仅仅用于处理批处理，而不要和其他功能重叠。 在最后一层基础设置（Infrastructure）部分主要分为3个部分。JobLauncher、Job以及Step。每一个Step又细分为ItemReader、ItemProcessor、ItemWirte。使用Spring Batch主要就是知道每一个基础设置负责的内容，然后在对应的设施中实现对应的业务。\"]},{\"header\":\"Spring Batch 批处理原则与建议\",\"slug\":\"spring-batch-批处理原则与建议\",\"contents\":[\"当我们构建一个批处理的过程时，必须注意以下原则：\",\"通常情况下，批处理的过程对系统和架构的设计要够要求比较高，因此尽可能的使用通用架构来处理批量数据处理，降低问题发生的可能性。Spring Batch是一个是一个轻量级的框架，适用于处理一些灵活并没有到海量的数据。\",\"批处理应该尽可能的简单，尽量避免在单个批处理中去执行过于复杂的任务。我们可以将任务分成多个批处理或者多个步骤去实现。\",\"保证数据处理和物理数据紧密相连。笼统的说就是我们在处理数据的过程中有很多步骤，在某些步骤执行完时应该就写入数据，而不是等所有都处理完。\",\"尽可能减少系统资源的使用、尤其是耗费大量资源的IO以及跨服务器引用，尽量分配好数据处理的批量。\",\"定期分析系统的IO使用情况、SQL语句的执行情况等，尽可能的减少不必要的IO操作。优化的原则有： \",\"尽量在一次事物中对同一数据进行读取或写缓存。\",\"一次事物中，尽可能在开始就读取所有需要使用的数据。\",\"优化索引，观察SQL的执行情况，尽量使用主键索引，尽量避免全表扫描或过多的索引扫描。\",\"不要在批处理中对相同的数据执行2次相同的操作。\",\"对于批处理程序而言应该在批处理启动之前就分配足够的内存，以免处理的过程中去重新申请新的内存页。\",\"对数据的完整性应该从最差的角度来考虑，每一步的处理都应该建立完备的数据校验。\",\"对于数据的总量我们应该有一个和数据记录在数据结构的某个字段上。\",\"所有的批处理系统都需要进行压力测试。\",\"如果整个批处理的过程是基于文件系统，在处理的过程中请切记完成文件的备份以及文件内容的校验。\"]},{\"header\":\"批处理的通用策略\",\"slug\":\"批处理的通用策略\",\"contents\":[\"和软件开发的设计模式一样，批处理也有各种各样的现成模式可供参考。当一个开发（设计）人员开始执行批处理任务时，应该将业务逻辑拆分为一下的步骤或者板块分批执行：\",\"数据转换：某个（某些）批处理的外部数据可能来自不同的外部系统或者外部提供者，这些数据的结构千差万别。在统一进行批量数据处理之前需要对这些数据进行转换，合并为一个统一的结构。因此在数据开始真正的执行业务处理之前，可以先搭建批处理任务将这些数据统一转换。\",\"数据校验：批处理是对大量数据进行处理，并且数据的来源千差万别，所以批处理的输入数据需要对数据的完整性性进行校验（比如校验字段数据是否缺失）。另外批处理输出的数据也需要进行合适的校验（例如处理了100条数据，校验100条数据是否校验成功）\",\"提取数据：批处理的工作是逐条从数据库或目标文件读取记录（records），提取时可以通过一些规则从数据源中进行数据筛选。\",\"数据实时更新处理：根据业务要求，对实时数据进行处理。某些时候一行数据记录的处理需要绑定在一个事物之下。\",\"输出记录到标准的文档格式：数据处理完成之后需要根据格式写入到对应的外部数据系统中。\",\"以上五个步骤是一个标准的数据批处理过程，Spring batch框架为业务实现提供了以上几个功能入口。\"]},{\"header\":\"数据额外处理\",\"slug\":\"数据额外处理\",\"contents\":[\"某些情况需要实现对数据进行额外处理，在进入批处理之前通过其他方式将数据进行处理。主要内容有：\",\"排序：由于批处理是以独立的行数据（record）进行处理的，在处理的时候并不知道记录前后关系。因此如果需要对整体数据进行排序，最好事先使用其他方式完成。\",\"分割：数据拆分也建议使用独立的任务来完成。理由类似排序，因为批处理的过程都是以行记录为基本处理单位的，无法再对分割之后的数据进行扩展处理。\",\"合并：理由如上。\"]},{\"header\":\"常规数据源\",\"slug\":\"常规数据源\",\"contents\":[\"批处理的数据源通常包括：\",\"数据库驱动链接（链接到数据库）对数据进行逐条提取。\",\"文件驱动链接，对文件数据进行提取\",\"消息驱动链接，从MQ、kafka等消息系统提取数据。\"]},{\"header\":\"典型的处理过程\",\"slug\":\"典型的处理过程\",\"contents\":[\"在业务停止的窗口期进行批数据处理，例如银行对账、清结算都是在12点日切到黎明之间。简称为离线处理。\",\"在线或并发批处理，但是需要对实际业务或用户的响应进行考量。\",\"并行处理多种不同的批处理作业。\",\"分区处理：将相同的数据分为不同的区块，然后按照相同的步骤分为许多独立的批处理任务对不同的区块进行处理。\",\"以上处理过程进行组合。\",\"在执行2,3点批处理时需要注意事物隔离等级。\"]},{\"header\":\"Spring Batch批处理的核心概念\",\"slug\":\"spring-batch批处理的核心概念\",\"contents\":[\"下图是批处理的核心流程图。\",\"​ Spring Batch同样按照批处理的标准实现了各个层级的组件。并且在框架级别保证数据的完整性和事物性。\",\"​ 如图所示，在一个标准的批处理任务中组要涵盖的核心概念有JobLauncher、Job、Step，一个Job可以涵盖多个Step，一个Job对应一个启动的JobLauncher。一个Step中分为ItemReader、ItemProcessor、ItemWriter，根据字面意思它们分别对应数据提取、数据处理和数据写入。此外JobLauncher、Job、Step也称之为批处理的元数据（Metadata），它们会被存储到JobRepository中。\"]},{\"header\":\"Job\",\"slug\":\"job\",\"contents\":[\"简单的说Job是封装一个批处理过程的实体，与其他的Spring项目类似，Job可以通过XML或Java类配置，称职为”Job Configuration“.如下图Job是单个批处理的最顶层。\",\"为了便于理解，可以建立的理解为Job就是每一步（Step）实例的容器。他结合了多个Step，为它们提供统一的服务同时也为Step提供个性化的服务，比如步骤重启。通常情况下Job的配置包含以下内容：\",\"Job的名称\",\"定义和排序Step执行实例。\",\"标记每个Step是否可以重启。\",\"Spring Batch为Job接口提供了默认的实现——SimpleJob类，在类中实现了一些标准的批处理方法。下面的代码展示了如可申明一个Job (在Config类中)。\",\"@Bean public Job footballJob() { return this.jobBuilderFactory //get中命名了Job的名称 .get(\\\"footballJob\\\") //playerLoad、gameLoad、playerSummarization都是Step .start(playerLoad()) .next(gameLoad()) .next(playerSummarization()) .end() .build(); } \"]},{\"header\":\"JobInstance\",\"slug\":\"jobinstance\",\"contents\":[\"JobInstance是指批处理作业运行的实例。例如一个批处理必须在每天执行一次，系统在2019年5月1日执行了一次我们称之为2019-05-01的实例，类似的还会有2019-05-02、2019-05-03实例。在特定的运行实践中，一个Job只有一个JobInstance以及对应的JobParameters ，但是可以有多个JobExecution 。（JobParameters 、JobExecution 见后文）。同一个JobInstance 具有相同的上下文（ExecutionContext 内容见后文）。\"]},{\"header\":\"JobParameters\",\"slug\":\"jobparameters\",\"contents\":[\"前面讨论了JobInstance 与Job 的区别，但是具体的区别内容都是通过JobParameters 体现的。一个JobParameters 对象中包含了一系列Job运行相关的参数，这些参数可以用于参考或者用于实际的业务使用。对应的关系如下图：\",\"当我们执行2个不同的JobInstance时JobParameters中的属性都会有差异。可以简单的认为一个JobInstance的标识就是 Job + JobParameters 。\"]},{\"header\":\"JobExecution\",\"slug\":\"jobexecution\",\"contents\":[\"​ JobExecution 可以理解为单次运行Job的容器。一次JobInstance执行的结果可能是成功、也可能是失败。但是对于Spring Batch框架而言，只有返回运行成功才会视为完成一次批处理。例如2019-05-01执行了一次JobInstance，但是执行的过程失败，因此第二次还会有一个“相同的”的 JobInstance 被执行。\",\"​ Job 可以定义批处理如何执行，JobInstance 纯粹的就是一个处理对象，把所有的内容、对象组织在一起，主要是为了当面临问题时定义正确的重启参数。而JobExecution是运行时的“容器”，记录动态运行时的各种属性和上线文，主要有一下内容：\",\"属性\",\"说明\",\"status\",\"状态类名为BatchStatus，它指示了执行的状态。在执行的过程中状态为BatchStatus#STARTED，失败：BatchStatus#FAILED，完成：BatchStatus#COMPLETED\",\"startTime\",\"java.util.Date对象，标记批处理任务启动的系统时间，批处理任务未启动数据为空\",\"endTime\",\"java.util.Date对象，结束时间无论是否成功都包含该数据，如未处理完为空\",\"exitStatus\",\"ExitStatus类，记录运行结果。\",\"createTime\",\"java.util.Date, JobExecution的创建时间，某些使用execution已经创建但是并未开始运行。\",\"lastUpdate\",\"java.util.Date，最后一次更新时间\",\"executionContext\",\"批处理任务执行的所有用户数据\",\"failureExceptions\",\"记录在执行Job时的异常，对于排查问题非常有用\",\"对应的每次执行的结果会在元数据库中体现为：\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_EXECUTION_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"IDENTIFYING\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"BATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2019-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"​\",\"​ 当某个Job批处理任务失败之后会在对应的数据库表中路对应的状态。假设1月1号执行的任务失败，技术团队花费了大量的时间解决这个问题到了第二天21才继续执行这个任务。\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"2\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_EXECUTION_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"IDENTIFYING\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"2\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"TRUE\",\"3\",\"DATE\",\"schedule.Date\",\"2019-01-02\",\"TRUE\",\"BATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2019-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"2\",\"1\",\"2019-01-02 21:00\",\"2017-01-02 21:30\",\"COMPLETED\",\"3\",\"2\",\"2019-01-02 21:31\",\"2017-01-02 22:29\",\"COMPLETED\",\"从数据上看好似JobInstance是一个接一个顺序执行的，但是对于Spring Batch并没有进行任何控制。不同的JobInstance很有可能是同时在运行（相同的JobInstance同时运行会抛出JobExecutionAlreadyRunningException异常）。\"]},{\"header\":\"Step\",\"slug\":\"step\",\"contents\":[\"​ Step是批处理重复运行的最小单元，它按照顺序定义了一次执行的必要过程。因此每个Job可以视作由一个或多个多个Step组成。一个Step包含了所有所有进行批处理的必要信息，这些信息的内容是由开发人员决定的并没有统一的标准。一个Step可以很简单，也可以很复杂。他可以是复杂业务的组合，也有可能仅仅用于迁移数据。与JobExecution的概念类似，Step也有特定的StepExecution，关系结构如下：\"]},{\"header\":\"StepExecution\",\"slug\":\"stepexecution\",\"contents\":[\"​ StepExecution表示单次执行Step的容器，每次Step执行时都会有一个新的StepExecution被创建。与JobExecution不同的是，当某个Step执行失败后并不会再次尝试重新执行该Step。StepExecution包含以下属性：\",\"属性\",\"说明\",\"status\",\"状态类名为BatchStatus，它指示了执行的状态。在执行的过程中状态为BatchStatus#STARTED，失败：BatchStatus#FAILED，完成：BatchStatus#COMPLETED\",\"startTime\",\"java.util.Date对象，标记StepExecution启动的系统时间，未启动数据为空\",\"endTime\",\"java.util.Date对象，结束时间，无论是否成功都包含该数据，如未处理完为空\",\"exitStatus\",\"ExitStatus类，记录运行结果。\",\"createTime\",\"java.util.Date,JobExecution的创建时间，某些使用execution已经创建但是并未开始运行。\",\"lastUpdate\",\"java.util.Date，最后一次更新时间\",\"executionContext\",\"批处理任务执行的所有用户数据\",\"readCount\",\"成功读取数据的次数\",\"wirteCount\",\"成功写入数据的次数\",\"commitCount\",\"成功提交数据的次数\",\"rollbackCount\",\"回归数据的次数，有业务代码触发\",\"readSkipCount\",\"当读数据发生错误时跳过处理的次数\",\"processSkipCount\",\"当处理过程发生错误，跳过处理的次数\",\"filterCount\",\"被过滤规则拦截未处理的次数\",\"writeSkipCount\",\"写数据失败，跳过处理的次数\"]},{\"header\":\"ExecutionContext\",\"slug\":\"executioncontext\",\"contents\":[\"前文已经多次提到ExecutionContext。可以简单的认为ExecutionContext提供了一个Key/Value机制，在StepExecution和JobExecution对象的任何位置都可以获取到ExecutionContext中的任何数据。最有价值的作用是记录数据的执行位置，以便发生重启时候从对应的位置继续执行：\",\"executionContext.putLong(getKey(LINES_READ_COUNT), reader.getPosition()) \",\"比如在任务中有一个名为“loadData”的Step，他的作用是从文件中读取数据写入到数据库，当第一次执行失败后，数据库中有如下数据：\",\"BATCH_JOB_INSTANCE：\",\"JOB_INST_ID\",\"JOB_NAME\",\"1\",\"EndOfDayJob\",\"BATCH_JOB_EXECUTION_PARAMS：\",\"JOB_INST_ID\",\"TYPE_CD\",\"KEY_NAME\",\"DATE_VAL\",\"1\",\"DATE\",\"schedule.Date\",\"2019-01-01\",\"ATCH_JOB_EXECUTION：\",\"JOB_EXEC_ID\",\"JOB_INST_ID\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"2017-01-01 21:00\",\"2017-01-01 21:30\",\"FAILED\",\"BATCH_STEP_EXECUTION：\",\"STEP_EXEC_ID\",\"JOB_EXEC_ID\",\"STEP_NAME\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"1\",\"1\",\"loadData\",\"2017-01-01 21:00\",\"2017-01-01 21:30\",\"BATCH_STEP_EXECUTION_CONTEXT：\",\"STEP_EXEC_ID\",\"SHORT_CONTEXT\",\"1\",\"{piece.count=40321}\",\"​ 在上面的例子中，Step 运行30分钟处理了40321个“pieces”，我们姑且认为“pieces”表示行间的行数（实际就是每个Step完成循环处理的个数）。这个值会在每个commit之前被更新记录在ExecutionContext中（更新需要用到StepListener后文会详细说明）。当我们再次重启这个Job时并记录在BATCH_STEP_EXECUTION_CONTEXT中的数据会加载到ExecutionContext中,这样当我们继续执行批处理任务时可以从上一次中断的位置继续处理。例如下面的代码在ItemReader中检查上次执行的结果，并从中断的位置继续执行：\",\"if (executionContext.containsKey(getKey(LINES_READ_COUNT))) { log.debug(\\\"Initializing for restart. Restart data is: \\\" + executionContext); long lineCount = executionContext.getLong(getKey(LINES_READ_COUNT)); LineReader reader = getReader(); Object record = \\\"\\\"; while (reader.getPosition() < lineCount && record != null) { record = readLine(); } } \",\"ExecutionContext是根据JobInstance进行管理的，因此只要是相同的实例都会具备相同的ExecutionContext（无论是否停止）。此外通过以下方法都可以获得一个ExecutionContext：\",\"ExecutionContext ecStep = stepExecution.getExecutionContext(); ExecutionContext ecJob = jobExecution.getExecutionContext(); \",\"但是这2个ExecutionContext并不相同，前者是在一个Step中每次Commit数据之间共享，后者是在Step与Step之间共享。\"]},{\"header\":\"JobRepository\",\"slug\":\"jobrepository\",\"contents\":[\"​ JobRepository是所有前面介绍的对象实例的持久化机制。他为JobLauncher、Job、Step的实现提供了CRUD操作。当一个Job第一次被启动时，一个JobExecution会从数据源中获取到，同时在执行的过程中StepExecution、JobExecution的实现都会记录到数据源中。挡在程序启动时使用@EnableBatchProcessing注解，JobRepository会进行自动化配置。\"]},{\"header\":\"JobLauncher\",\"slug\":\"joblauncher\",\"contents\":[\"JobLauncher为Job的启动运行提供了一个边界的入口，在启动Job的同时还可以定制JobParameters：\",\"public interface JobLauncher { public JobExecution run(Job job, JobParameters jobParameters) throws JobExecutionAlreadyRunningException, JobRestartException,JobInstanceAlreadyCompleteException,JobParametersInvalidException; } \",\"![cm9c3yeke6](img\\\\Spring Batch\\\\cm9c3yeke6.png)\"]},{\"header\":\"Spring Batch——Step控制\",\"slug\":\"spring-batch——step控制\",\"contents\":[]},{\"header\":\"Spring Batch——Job配置与运行\",\"slug\":\"spring-batch——job配置与运行\",\"contents\":[]},{\"header\":\"Spring Batch——Item概念及使用代码\",\"slug\":\"spring-batch——item概念及使用代码\",\"contents\":[\"在上文中介绍一个标准的批处理分为 Job 和 Step 。本文将结合代码介绍在Step中Reader、Processor、Writer的实际使用。\"]},{\"header\":\"Reader\",\"slug\":\"reader\",\"contents\":[\"Reader是指从各种各样的外部输入中获取数据，框架为获取各种类型的文件已经预定义了常规的Reader实现类。Reader通过ItemReader接口实现：\",\"public interface ItemReader<T> { T read() throws Exception, UnexpectedInputException, ParseException, NonTransientResourceException; } \",\"read方法的作用就是读取一条数据，数据以泛型T的实体结构返回，当read返回null时表示所有数据读取完毕。返回的数据可以是任何结构，比如文件中的一行字符串，数据库的一行数据，或者xml文件中的一系列元素，只要是一个Java对象即可。\"]},{\"header\":\"Writer\",\"slug\":\"writer\",\"contents\":[\"Writer通过ItemWriter接口实现：\",\"public interface ItemWriter<T> { void write(List<? extends T> items) throws Exception; } \",\"Writer是Reader的反向操作，是将数据写入到特定的数据源中。在上文已经介绍Writer是根据chunk属性设定的值按列表进行操作的，所以传入的是一个List结构。chunk用于表示批处理的事物分片，因此需要注意的是，在writer方法中进行完整数据写入事物操作。例如向数据库写入List中的数据，在写入完成之后再提交事物。\"]},{\"header\":\"读写的组合模式\",\"slug\":\"读写的组合模式\",\"contents\":[\"无论是读还是写，有时会需要从多个不同的来源获取文件，或者写入到不同的数据源，或者是需要在读和写之间处理一些业务。可以使用组合模式来实现这个目的：\",\"public class CompositeItemWriter<T> implements ItemWriter<T> { ItemWriter<T> itemWriter; public CompositeItemWriter(ItemWriter<T> itemWriter) { this.itemWriter = itemWriter; } public void write(List<? extends T> items) throws Exception { //Add business logic here itemWriter.write(items); } public void setDelegate(ItemWriter<T> itemWriter){ this.itemWriter = itemWriter; } } \"]},{\"header\":\"Processor\",\"slug\":\"processor\",\"contents\":[\"除了使用组合模式，直接使用Processor是一种更优雅的方法。Processor是Step中的可选项，但是批处理大部分时候都需要对数据进行处理，因此框架提供了ItemProcessor接口来满足 Processor 过程：\",\"public interface ItemProcessor<I, O> { O process(I item) throws Exception; } \",\"Processor的结构非常简单也是否易于理解。传入一个类型<I>，然后由Processor处理成为<O>。\"]},{\"header\":\"Processor链\",\"slug\":\"processor链\",\"contents\":[\"在一个Step中可以使用多个Processor来按照顺序处理业务，此时同样可以使用CompositeItem模式来实现：\",\"@Bean public CompositeItemProcessor compositeProcessor() { //创建 CompositeItemProcessor CompositeItemProcessor<Foo,Foobar> compositeProcessor = new CompositeItemProcessor<Foo,Foobar>(); List itemProcessors = new ArrayList(); //添加第一个 Processor itemProcessors.add(new FooTransformer()); //添加第二个 Processor itemProcessors.add(new BarTransformer()); //添加链表 compositeProcessor.setDelegates(itemProcessors); return processor; } \"]},{\"header\":\"过滤记录\",\"slug\":\"过滤记录\",\"contents\":[\"​ 在Reader读取数据的过程中，并不是所有的数据都可以使用，此时Processor还可以用于过滤非必要的数据，同时不会影响Step的处理过程。只要ItemProcesspr的实现类在procss方法中返回null即表示改行数据被过滤掉了。\"]},{\"header\":\"ItemStream\",\"slug\":\"itemstream\",\"contents\":[\"​ 在上文中已经提到了ItemStream。Spring Batch的每一步都是无状态的，进而Reader和Writer也是无状态的，这种方式能够很好的隔离每行数据的处理，也能将容错的范围收窄到可以空子的范围。但是这并不意味着整个批处理的过程中并不需要控制状态。例如从数据库持续读入或写入数据，每次Reader和Writer都单独去申请数据源的链接、维护数据源的状态（打开、关闭等）。因此框架提供了ItemStream接口来完善这些操作：\",\"public interface ItemStream { void open(ExecutionContext executionContext) throws ItemStreamException; void update(ExecutionContext executionContext) throws ItemStreamException; void close() throws ItemStreamException; } \"]},{\"header\":\"持久化数据\",\"slug\":\"持久化数据\",\"contents\":[\"​ 在使用Spring Batch之前需要初始化他的元数据存储（Meta-Data Schema）,也就是要将需要用到的表导入到对应的数据库中。当然，Spring Batch支持不使用任何持久化数据库，仅仅将数据放到内存中，不设置DataSource即可。\"]},{\"header\":\"初始化序列\",\"slug\":\"初始化序列\",\"contents\":[\"Spring Batch相关的工作需要使用序列SEQUENCE：\",\"CREATE SEQUENCE BATCH_STEP_EXECUTION_SEQ; CREATE SEQUENCE BATCH_JOB_EXECUTION_SEQ; CREATE SEQUENCE BATCH_JOB_SEQ; \",\"有些数据库不支持SEQUENCE，可以通过表代理，比如在MySql（InnoDB数据库）中：\",\"CREATE TABLE BATCH_STEP_EXECUTION_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_STEP_EXECUTION_SEQ values(0); CREATE TABLE BATCH_JOB_EXECUTION_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_JOB_EXECUTION_SEQ values(0); CREATE TABLE BATCH_JOB_SEQ (ID BIGINT NOT NULL); INSERT INTO BATCH_JOB_SEQ values(0); \"]},{\"header\":\"关于Version字段\",\"slug\":\"关于version字段\",\"contents\":[\"某些表中都有Version字段。因为Spring的更新策略是乐观锁，因此在进行数据更新之后都会对表的Version字段进行+1处理。在内存与数据库交互的过程中，会使用采用getVersion、increaseVersion（+1）、updateDataAndVersion的过程，如果在update的时候发现Version不是预计的数值（+1），则会抛出OptimisticLockingFailureException的异常。当同一个Job在进群中不同服务上执行时，需要注意这个问题。\"]},{\"header\":\"BATCH_JOB_INSTANCE\",\"slug\":\"batch-job-instance\",\"contents\":[\"BATCH_JOB_INSTANCE用于记录JobInstance，在数据批处理概念中介绍了他的工作方式，其结构为：\",\"CREATE TABLE BATCH_JOB_INSTANCE ( JOB_INSTANCE_ID BIGINT PRIMARY KEY , VERSION BIGINT, JOB_NAME VARCHAR(100) NOT NULL , JOB_KEY VARCHAR(2500) ); \",\"字段\",\"说明\",\"JOB_INSTANCE_ID\",\"主键，主键与单个JobInstance相关。当获取到某个JobInstance实例后，通过getId方法可以获取到此数据\",\"VERSION\",\"JOB_NAME\",\"Job的名称，用于标记运行的Job，在创建Job时候指定\",\"JOB_KEY\",\"JobParameters的序列化数值。在数据批处理概念中介绍了一个JobInstance相当于Job+JobParameters。他用于标记同一个Job不同的实例\"]},{\"header\":\"BATCH_JOB_EXECUTION_PARAMS\",\"slug\":\"batch-job-execution-params\",\"contents\":[\"BATCH_JOB_EXECUTION_PARAMS对应的是JobParameters对象。其核心功能是存储Key-Value结构的各种状态数值。字段中IDENTIFYING=true用于标记那些运行过程中必须的数据（可以理解是框架需要用到的数据），为了存储key-value结构该表一个列数据格式：\",\"CREATE TABLE BATCH_JOB_EXECUTION_PARAMS ( JOB_EXECUTION_ID BIGINT NOT NULL , TYPE_CD VARCHAR(6) NOT NULL , KEY_NAME VARCHAR(100) NOT NULL , STRING_VAL VARCHAR(250) , DATE_VAL DATETIME DEFAULT NULL , LONG_VAL BIGINT , DOUBLE_VAL DOUBLE PRECISION , IDENTIFYING CHAR(1) NOT NULL , constraint JOB_EXEC_PARAMS_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ); \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"与BATCH_JOB_EXECUTION表关联的外键，详见数据批处理概念中Job、JobInstance、JobExecute的关系\",\"TYPE_CD\",\"用于标记数据的对象类型，例如 string、date、long、double，非空\",\"KEY_NAME\",\"key的值\",\"STRING_VAL\",\"string类型的数值\",\"DATE_VAL\",\"date类型的数值\",\"LONG_VAL\",\"long类型的数值\",\"DOUBLE_VAL\",\"double类型的数值\",\"IDENTIFYING\",\"标记这对key-valuse是否来自于JobInstace自身\"]},{\"header\":\"BATCH_JOB_EXECUTION\",\"slug\":\"batch-job-execution\",\"contents\":[\"关联JobExecution，每当运行一个Job都会产生一个新的JobExecution，对应的在表中都会新增一行数据。\",\"CREATE TABLE BATCH_JOB_EXECUTION ( JOB_EXECUTION_ID BIGINT PRIMARY KEY , VERSION BIGINT, JOB_INSTANCE_ID BIGINT NOT NULL, CREATE_TIME TIMESTAMP NOT NULL, START_TIME TIMESTAMP DEFAULT NULL, END_TIME TIMESTAMP DEFAULT NULL, STATUS VARCHAR(10), EXIT_CODE VARCHAR(20), EXIT_MESSAGE VARCHAR(2500), LAST_UPDATED TIMESTAMP, JOB_CONFIGURATION_LOCATION VARCHAR(2500) NULL, constraint JOB_INSTANCE_EXECUTION_FK foreign key (JOB_INSTANCE_ID) references BATCH_JOB_INSTANCE(JOB_INSTANCE_ID) ) ; \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"JobExecution的主键，JobExecution::getId方法可以获取到该值\",\"VERSION\",\"JOB_INSTANCE_ID\",\"关联到JobInstace的外键，详见数据批处理概念中Job、JobInstance、JobExecute的关系\",\"CREATE_TIME\",\"创建时间戳\",\"START_TIME\",\"开始时间戳\",\"END_TIME\",\"结束时间戳，无论成功或失败都会更新这一项数据。如果某行数据该值为空表示运行期间出现错误，并且框架无法更新该值\",\"STATUS\",\"JobExecute的运行状态:COMPLETED、STARTED或者其他状态。此数值对应Java中BatchStatus枚举值\",\"EXIT_CODE\",\"JobExecute执行完毕之后的退出返回值\",\"EXIT_MESSAGE\",\"JobExecute退出的详细内容，如果是异常退出可能会包括异常堆栈的内容\",\"LAST_UPDATED\",\"最后一次更新的时间戳\"]},{\"header\":\"BATCH_STEP_EXECUTION\",\"slug\":\"batch-step-execution\",\"contents\":[\"该表对应的是StepExecution，其结构和BATCH_JOB_EXECUTION基本相似，只是对应的对象是Step，增加了与之相对的一些字段数值：\",\"CREATE TABLE BATCH_STEP_EXECUTION ( STEP_EXECUTION_ID BIGINT PRIMARY KEY , VERSION BIGINT NOT NULL, STEP_NAME VARCHAR(100) NOT NULL, JOB_EXECUTION_ID BIGINT NOT NULL, START_TIME TIMESTAMP NOT NULL , END_TIME TIMESTAMP DEFAULT NULL, STATUS VARCHAR(10), COMMIT_COUNT BIGINT , READ_COUNT BIGINT , FILTER_COUNT BIGINT , WRITE_COUNT BIGINT , READ_SKIP_COUNT BIGINT , WRITE_SKIP_COUNT BIGINT , PROCESS_SKIP_COUNT BIGINT , ROLLBACK_COUNT BIGINT , EXIT_CODE VARCHAR(20) , EXIT_MESSAGE VARCHAR(2500) , LAST_UPDATED TIMESTAMP, constraint JOB_EXECUTION_STEP_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ) ; \",\"未填入内容部分见BATCH_JOB_EXECUTION说明。\",\"字段\",\"说明\",\"STEP_EXECUTION_ID\",\"StepExecute对应的主键\",\"VERSION\",\"STEP_NAME\",\"Step名称\",\"JOB_EXECUTION_ID\",\"关联到BATCH_JOB_EXECUTION表的外键，标记该StepExecute所属的JobExecute\",\"START_TIME\",\"END_TIME\",\"STATUS\",\"COMMIT_COUNT\",\"执行过程中，事物提交的次数，该值与数据的规模以及chunk的设置有关\",\"READ_COUNT\",\"读取数据的次数\",\"FILTER_COUNT\",\"Processor中过滤记录的次数\",\"WRITE_COUNT\",\"吸入数据的次数\",\"READ_SKIP_COUNT\",\"读数据的跳过次数\",\"WRITE_SKIP_COUNT\",\"写数据的跳过次数\",\"PROCESS_SKIP_COUNT\",\"Processor跳过的次数\",\"ROLLBACK_COUNT\",\"回滚的次数\",\"EXIT_CODE\",\"EXIT_MESSAGE\",\"LAST_UPDATED\"]},{\"header\":\"BATCH_JOB_EXECUTION_CONTEXT\",\"slug\":\"batch-job-execution-context\",\"contents\":[\"该表会记录所有与Job相关的ExecutionContext信息。每个ExecutionContext都对应一个JobExecution，在运行的过程中它包含了所有Job范畴的状态数据，这些数据在执行失败后对于后续处理有中重大意义。\",\"CREATE TABLE BATCH_JOB_EXECUTION_CONTEXT ( JOB_EXECUTION_ID BIGINT PRIMARY KEY, SHORT_CONTEXT VARCHAR(2500) NOT NULL, SERIALIZED_CONTEXT CLOB, constraint JOB_EXEC_CTX_FK foreign key (JOB_EXECUTION_ID) references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID) ) ; \",\"字段\",\"说明\",\"JOB_EXECUTION_ID\",\"关联到JobExecution的外键，建立JobExecution和ExecutionContext的关系。\",\"SHORT_CONTEXT\",\"标记SERIALIZED_CONTEXT的版本号\",\"SERIALIZED_CONTEXT\",\"序列化的ExecutionContext\"]},{\"header\":\"BATCH_STEP_EXECUTION_CONTEXT\",\"slug\":\"batch-step-execution-context\",\"contents\":[\"Step中ExecutionContext相关的数据表，结构与BATCH_JOB_EXECUTION_CONTEXT完全一样。\"]},{\"header\":\"表索引建议\",\"slug\":\"表索引建议\",\"contents\":[\"上面的所有建表语句都没有提供索引，但是并不代表索引没有价值。当感觉到SQL语句的执行有效率问题时候，可以增加索引。\",\"索引带来的价值取决于SQL查询的频率以及关联关系，下面是Spring Batch框架在运行过程中会用到的一些查询条件语句，用于参考优化索引：\",\"表\",\"Where条件\",\"执行频率\",\"BATCH_JOB_INSTANCE\",\"JOB_NAME = ? and JOB_KEY = ?\",\"每次Job启动执时\",\"BATCH_JOB_EXECUTION\",\"JOB_INSTANCE_ID = ?\",\"每次Job重启时\",\"BATCH_EXECUTION_CONTEXT\",\"EXECUTION_ID = ? and KEY_NAME = ?\",\"视chunk的大小而定\",\"BATCH_STEP_EXECUTION\",\"VERSION = ?\",\"视chunk的大小而定\",\"BATCH_STEP_EXECUTION\",\"STEP_NAME = ? and JOB_EXECUTION_ID = ?\",\"每一个Step执行之前\"]},{\"header\":\"Spring Batch——文件读写\",\"slug\":\"spring-batch——文件读写\",\"contents\":[\"在上文中Job、Step都是属于框架级别的的功能，大部分时候都是提供一些配置选项给开发人员使用，而Item中的Reader、Processor和Writer是属于业务级别的，它开放了一些业务切入的接口。 但是文件的读写过程中有很多通用一致的功能Spring Batch为这些相同的功能提供了一致性实现类。\"]},{\"header\":\"扁平结构文件\",\"slug\":\"扁平结构文件\",\"contents\":[\"扁平结构文件（也称为矩阵结构文件，后文简称为文件）是最常见的一种文件类型。他通常以一行表示一条记录，字段数据之间用某种方式分割。与标准的格式数据（xml、json等）主要差别在于他没有结构性描述方案（SXD、JSON-SCHEME），进而没有结构性分割规范。因此在读写此类文件之前需要先设定好字段的分割方法。\",\"文件的字段数据分割方式通常有两种：使用分隔符或固定字段长度。前者通常使用逗号（，）之类的符号对字段数据进行划分，后者的每一列字段数据长度是固定的。 框架为文件的读取提供了FieldSet用于将文件结构中的信息映射到一个对象。FieldSet的作用是将文件的数据与类的field进行绑定（field是Java中常见的概念，不清楚的可以了解Java反射）。\"]},{\"header\":\"数据读取\",\"slug\":\"数据读取\",\"contents\":[\"Spring Batch为文件读取提供了FlatFileItemReader类，它为文件中的数据的读取和转换提供了基本功能。在FlatFileItemReader中有2个主要的功能接口，一是Resource、二是LineMapper。 Resource用于外部文件获取，详情请查看Spring核心——资源管理部分的内容，下面是一个例子：\",\"Resource resource = new FileSystemResource(\\\"resources/trades.csv\\\"); \",\"在复杂的生产环境中，文件通常由中心化、或者流程式的基础框架来管理（比如EAI）。因此文件往往需要使用FTP等方式从其他位置获取。如何迁移文件已经超出了Spring Batch框架的范围，在Spring的体系中可以参考Spring Integration项目。\",\"下面是FlatFileItemReader的属性，每一个属性都提供了Setter方法。\",\"属性名\",\"参数类型\",\"说明\",\"comments\",\"String[]\",\"指定文件中的注释前缀，用于过滤注释内容行\",\"encoding\",\"String\",\"指定文件的编码方式，默认为Charset.defaultCharset()\",\"lineMapper\",\"LineMapper\",\"利用LineMapper接口将一行字符串转换为对象\",\"linesToSkip\",\"int\",\"跳过文件开始位置的行数，用于跳过一些字段的描述行\",\"recordSeparatorPolicy\",\"RecordSeparatorPolicy\",\"用于判断数据是否结束\",\"resource\",\"Resource\",\"指定外部资源文件位置\",\"skippedLinesCallback\",\"LineCallbackHandler\",\"当配置linesToSkip，每执行一次跳过都会被回调一次，会传入跳过的行数据内容\",\"每个属性都为文件的解析提供了某方面的功能，下面是结构的说明。\"]},{\"header\":\"LineMapper\",\"slug\":\"linemapper\",\"contents\":[\"这个接口的作用是将字符串转换为对象：\",\"public interface LineMapper { T mapLine(String line, int lineNumber) throws Exception; } \",\"接口的基本处理逻辑是聚合类（FlatFileItemReader）传递一行字符串以及行号给LineMapper::mapLine，方法处理后返回一个映射的对象。\"]},{\"header\":\"LineTokenizer\",\"slug\":\"linetokenizer\",\"contents\":[\"这个接口的作用是将一行数据转换为一个FieldSet结构。对于Spring Batch而言，扁平结构文件的到Java实体的映射都通过FieldSet来控制，因此读写文件的过程需要完成字符串到FieldSet的转换：\",\"public interface LineTokenizer { FieldSet tokenize(String line); } \",\"这个接口的含义是：传递一行字符串数据，然后获取一个FieldSet。\",\"框架为LineTokenizer提供三个实现类：\",\"DelimitedLineTokenizer：利用分隔符将数据转换为FieldSet。最常见的分隔符是逗号,，类提供了分隔符的配置和解析方法。\",\"FixedLengthTokenizer：根据字段的长度来解析出FieldSet结构。必须为记录定义字段宽度。\",\"PatternMatchingCompositeLineTokenizer：使用一个匹配机制来动态决定使用哪个LineTokenizer。\"]},{\"header\":\"FieldSetMapper\",\"slug\":\"fieldsetmapper\",\"contents\":[\"该接口是将FieldSet转换为对象：\",\"public interface FieldSetMapper { T mapFieldSet(FieldSet fieldSet) throws BindException; } \",\"FieldSetMapper通常和LineTokenizer联合在一起使用：String->FieldSet->Object。\"]},{\"header\":\"DefaultLineMapper\",\"slug\":\"defaultlinemapper\",\"contents\":[\"DefaultLineMapper是LineMapper的实现，他实现了从文件到Java实体的映射：\",\"public class DefaultLineMapper implements LineMapper<>, InitializingBean { private LineTokenizer tokenizer; private FieldSetMapper fieldSetMapper; public T mapLine(String line, int lineNumber) throws Exception { return fieldSetMapper.mapFieldSet(tokenizer.tokenize(line)); } public void setLineTokenizer(LineTokenizer tokenizer) { this.tokenizer = tokenizer; } public void setFieldSetMapper(FieldSetMapper fieldSetMapper) { this.fieldSetMapper = fieldSetMapper; } } \",\"在解析文件时数据是按行解析的：\",\"传入一行字符串。\",\"LineTokenizer将字符串解析为FieldSet结构。\",\"FieldSetMapper继续解析为一个Java实体对象返回给调用者。\",\"DefaultLineMapper是框架提供的默认实现类，看似非常简单，但是利用组合模式可以扩展出很多功能。\"]},{\"header\":\"数据自动映射\",\"slug\":\"数据自动映射\",\"contents\":[\"在转换过程中如果将FieldSet的names属性与目标类的field绑定在一起，那么可以直接使用反射实现数据转换，为此框架提供了BeanWrapperFieldSetMapper来实现。\",\"DefaultLineMapper<WeatherEntity> lineMapper = new DefaultLineMapper<>(); //创建LineMapper DelimitedLineTokenizer tokenizer = new DelimitedLineTokenizer(); //创建LineTokenizer tokenizer.setNames(new String[] { \\\"siteId\\\", \\\"month\\\", \\\"type\\\", \\\"value\\\", \\\"ext\\\" }); //设置Field名称 //创建FieldSetMapper BeanWrapperFieldSetMapper<WeatherEntity> wrapperMapper = new BeanWrapperFieldSetMapper<>(); //设置实体，实体的field名称必须和tokenizer.names一致。 wrapperMapper.setTargetType(WeatherEntity.class); // 组合lineMapper lineMapper.setLineTokenizer(tokenizer); lineMapper.setFieldSetMapper(wrapperMapper); \"]},{\"header\":\"文件读取总结\",\"slug\":\"文件读取总结\",\"contents\":[\"上面提到了各种接口和实现，实际上都是围绕着FlatFileItemReader的属性在介绍，虽然内容很多但是实际上就以下几点：\",\"首先要定位文件，Spring Batch提供了Resource相关的定位方法。\",\"其次是将文件中的行字符串数据转换为对象，LineMapper的功能就是完成这个功能。\",\"框架为LineMapper提供了DefaultLineMapper作为默认实现方法，在DefaultLineMapper中需要组合使用LineTokenizer和FieldSetMapper。前者将字符串转为为一个Field，后者将Field转换为目标对象。\",\"LineTokenizer有3个实现类可供使用、FieldSetMapper有一个默认实现类BeanWrapperFieldSetMapper。\"]},{\"header\":\"文件读取可执行源码\",\"slug\":\"文件读取可执行源码\",\"contents\":[\"可执行的源码在下列地址的items子工程中：\",\"Gitee：https://gitee.com/chkui-com/spring-batch-sample\",\"Github：https://github.com/chkui/spring-batch-sample\",\"运行之前需要配置数据库链接，参看源码库中的README.md。\",\"文件读取的主要逻辑在org.chenkui.spring.batch.sample.items.FlatFileReader类：\",\"public class FlatFileReader { // FeildSet的字段名，设置字段名之后可以直接使用名字作为索引获取数据。也可以使用索引位置来获取数据 public final static String[] Tokenizer = new String[] { \\\"siteId\\\", \\\"month\\\", \\\"type\\\", \\\"value\\\", \\\"ext\\\" }; private boolean userWrapper = false; @Bean //定义FieldSetMapper用于FieldSet->WeatherEntity public FieldSetMapper<WeatherEntity> fieldSetMapper() { return new FieldSetMapper<WeatherEntity>() { @Override public WeatherEntity mapFieldSet(FieldSet fieldSet) throws BindException { if (null == fieldSet) { // fieldSet不存在则跳过该行处理 return null; } else { WeatherEntity observe = new WeatherEntity(); observe.setSiteId(fieldSet.readRawString(\\\"siteId\\\")); //Setter return observe; } } }; } @Bean // 配置 Reader public ItemReader<WeatherEntity> flatFileReader( @Qualifier(\\\"fieldSetMapper\\\") FieldSetMapper<WeatherEntity> fieldSetMapper) { FlatFileItemReader<WeatherEntity> reader = new FlatFileItemReader<>(); // 读取资源文件 reader.setResource(new FileSystemResource(\\\"src/main/resources/data.csv\\\")); // 初始化 LineMapper实现类 DefaultLineMapper<WeatherEntity> lineMapper = new DefaultLineMapper<>(); // 创建LineTokenizer接口实现 DelimitedLineTokenizer tokenizer = new DelimitedLineTokenizer(); // 设定每个字段的名称，如果不设置需要使用索引获取值 tokenizer.setNames(Tokenizer); // 设置tokenizer工具 lineMapper.setLineTokenizer(tokenizer); //使用 BeanWrapperFieldSetMapper 使用反射直接转换 if (userWrapper) { BeanWrapperFieldSetMapper<WeatherEntity> wrapperMapper = new BeanWrapperFieldSetMapper<>(); wrapperMapper.setTargetType(WeatherEntity.class); fieldSetMapper = wrapperMapper; } lineMapper.setFieldSetMapper(fieldSetMapper); reader.setLineMapper(lineMapper); // 跳过的初始行，用于过滤字段行 reader.setLinesToSkip(1); reader.open(new ExecutionContext()); return reader; } } \"]},{\"header\":\"按字段长度格读取文件\",\"slug\":\"按字段长度格读取文件\",\"contents\":[\"除了按照分隔符，有些文件可以字段数据的占位长度来提取数据。按照前面介绍的过程，实际上只要修改LineTokenizer接口即可，框架提供了FixedLengthTokenizer类：\",\"@Bean public FixedLengthTokenizer fixedLengthTokenizer() { FixedLengthTokenizer tokenizer = new FixedLengthTokenizer(); tokenizer.setNames(\\\"ISIN\\\", \\\"Quantity\\\", \\\"Price\\\", \\\"Customer\\\"); //Range用于设定数据的长度。 tokenizer.setColumns(new Range(1-12), new Range(13-15), new Range(16-20), new Range(21-29)); return tokenizer; } \"]},{\"header\":\"写入扁平结构文件\",\"slug\":\"写入扁平结构文件\",\"contents\":[\"将数据写入到文件与读取的过程正好相反：将对象转换为字符串。\"]},{\"header\":\"LineAggregator\",\"slug\":\"lineaggregator\",\"contents\":[\"与LineMapper相对应的是LineAggregator，他的功能是将实体转换为字符串：\",\"public interface LineAggregator<T> { public String aggregate(T item); } \"]},{\"header\":\"PassThroughLineAggregator\",\"slug\":\"passthroughlineaggregator\",\"contents\":[\"框架为LineAggregator接口提供了一个非常简单的实现类——PassThroughLineAggregator，其唯一实现就是使用对象的toString方法：\",\"public class PassThroughLineAggregator<T> implements LineAggregator<T> { public String aggregate(T item) { return item.toString(); } } \"]},{\"header\":\"DelimitedLineAggregator\",\"slug\":\"delimitedlineaggregator\",\"contents\":[\"LineAggregator的另外一个实现类是DelimitedLineAggregator。与PassThroughLineAggregator简单直接使用toString方法不同的是，DelimitedLineAggregator需要一个转换接口FieldExtractor：\",\"DelimitedLineAggregator<CustomerCredit> lineAggregator = new DelimitedLineAggregator<>(); lineAggregator.setDelimiter(\\\",\\\"); lineAggregator.setFieldExtractor(fieldExtractor); \"]},{\"header\":\"FieldExtractor\",\"slug\":\"fieldextractor\",\"contents\":[\"FieldExtractor用于实体类到collection结构的转换。它可以和LineTokenizer进行类比，前者是将实体类转换为扁平结构的数据，后者是将String转换为一个FieldSet结构。\",\"public interface FieldExtractor<T> { Object[] extract(T item); } \",\"框架为FieldExtractor接口提供了一个基于反射的实现类BeanWrapperFieldExtractor，其过程就是将实体对象转换为列表：\",\"BeanWrapperFieldExtractor<CustomerCredit> fieldExtractor = new BeanWrapperFieldExtractor<>(); fieldExtractor.setNames(new String[] {\\\"field1\\\", \\\"field2\\\"}); \",\"setName方法用于指定要转换的field列表。\"]},{\"header\":\"输出文件处理\",\"slug\":\"输出文件处理\",\"contents\":[\"​ 文件读取的逻辑非常简单：文件存在打开文件并写入数据，当文件不存在抛出异常。但是写入文件明显不能这么简单粗暴。新建一个JobInstance时最直观的操作是：存在同名文件就抛出异常，不存在则创建文件并写入数据。但是这样做显然有很大的问题，当批处理过程中出现问题需要restart，此时并不会从头开始处理所有的数据，而是要求文件存在并接着继续写入。为了确保这个过程 FlatFileItemWriter默认会在新 JobInstance运行时删除已有文件，而运行重启时继续在文件末尾写入。 FlatFileItemWriter可以使用 shouldDeleteIfExists、 appendAllowed、 shouldDeleteIfEmpty来有针对性的控制文件。\"]},{\"header\":\"文件写入可执源码\",\"slug\":\"文件写入可执源码\",\"contents\":[\"文件写入主要代码在org.chenkui.spring.batch.sample.items.FlatFileWriter：\",\"public class FlatFileWriter { private boolean useBuilder = true; @Bean public ItemWriter<MaxTemperatureEntiry> flatFileWriter() { BeanWrapperFieldExtractor<MaxTemperatureEntiry> fieldExtractor = new BeanWrapperFieldExtractor<>(); fieldExtractor.setNames(new String[] { \\\"siteId\\\", \\\"date\\\", \\\"temperature\\\" }); //设置映射field fieldExtractor.afterPropertiesSet(); //参数检查 DelimitedLineAggregator<MaxTemperatureEntiry> lineAggregator = new DelimitedLineAggregator<>(); lineAggregator.setDelimiter(\\\",\\\"); //设置输出分隔符 lineAggregator.setFieldExtractor(fieldExtractor); //设置FieldExtractor处理器 FlatFileItemWriter<MaxTemperatureEntiry> fileWriter = new FlatFileItemWriter<>(); fileWriter.setLineAggregator(lineAggregator); fileWriter.setResource(new FileSystemResource(\\\"src/main/resources/out-data.csv\\\")); //设置输出文件位置 fileWriter.setName(\\\"outpufData\\\"); if (useBuilder) {//使用builder方式创建 fileWriter = new FlatFileItemWriterBuilder<MaxTemperatureEntiry>().name(\\\"outpufData\\\") .resource(new FileSystemResource(\\\"src/main/resources/out-data.csv\\\")).lineAggregator(lineAggregator) .build(); } return fileWriter; } } \",\"文件的写入过程与读取过程完全对称相反：先用FieldExtractor将对象转换为一个collection结构（列表），然后用lineAggregator将collection转化为带分隔符的字符串。\"]},{\"header\":\"代码说明\",\"slug\":\"代码说明\",\"contents\":[\"代码中的测试数据来自数据分析交流项目bi-process-example，是NOAA的2015年全球天气监控数据。为了便于源码存储进行了大量的删减，原始数据有百万条，如有需要使用下列方式下载： curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2015.csv.gz #数据文件 curl -O ftp://ftp.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt # 文件结构及类型说明\",\"代码实现了读取文件、处理文件、写入文件的整个过程。处理文件的过程是只获取监控的最高温度信息（Type=TMAX），其他都过滤。\",\"本案例的代码使用org.chenkui.spring.batch.sample.flatfile.FlatFileItemApplication::main方法运行，使用的是Command Runner的方式执行（运行方式的说明见Item概念及使用代码的命令行方式运行、Java内嵌运行）。\"]},{\"header\":\"Spring Batch——数据库批数据读写\",\"slug\":\"spring-batch——数据库批数据读写\",\"contents\":[\"本文将接着前面的内容说明数据库如何进行批处理读写。\"]},{\"header\":\"数据读取\",\"slug\":\"数据读取-1\",\"contents\":[\"数据库是绝大部分系统要用到的数据存储工具，因此针对数据库执行批量数据处理任务也是很常见的需求。数据的批量处理与常规业务开发不同，如果一次性读取百万条，对于任何系统而言肯定都是不可取的。为了解决这个问题Spring Batch提供了2套数据读取方案：\",\"基于游标读取数据\",\"基于分页读取数据\"]},{\"header\":\"游标读取数据\",\"slug\":\"游标读取数据\",\"contents\":[\"对于有经验大数据工程师而言数据库游标的操作应该是非常熟悉的，因为这是从数据库读取数据流标准方法，而且在Java中也封装了ResultSet这种面向游标操作的数据结构。\",\"ResultSet一直都会指向结果集中的某一行数据，使用next方法可以让游标跳转到下一行数据。Spring Batch同样使用这个特性来控制数据的读取：\",\"在初始化时打开游标。\",\"每一次调用ItemReader::read方法就从ResultSet获取一行数据并执行next。\",\"返回可用于数据处理的映射结构（map、dict）。\",\"在一切都执行完毕之后，框架会使用回调过程调用ResultSet::close来关闭游标。由于所有的业务过程都绑定在一个事物之上，所以知道到Step执行完毕或异常退出调用执行close。下图展示了数据读取的过程：\",\"SQL语句的查询结果称为数据集（对于大部分数据库而言，其SQL执行结果会产生临时的表空间索引来存放数据集）。游标开始会停滞在ID=2的位置，一次ItemReader执行完毕后会产生对应的实体FOO2，然后游标下移直到最后的ID=6。最后关闭游标。\"]},{\"header\":\"JdbcCursorItemReader\",\"slug\":\"jdbccursoritemreader\",\"contents\":[\"JdbcCursorItemReader是使用游标读取数据集的ItemReader实现类之一。它使用JdbcTemplate中的DataSource控制ResultSet,其过程是将ResultSet的每行数据转换为所需要的实体类。\",\"JdbcCursorItemReader的执行过程有三步：\",\"通过DataSource创建JdbcTemplate。\",\"设定数据集的SQL语句。\",\"创建ResultSet到实体类的映射。 大致如下：\",\"//随风溜达的向日葵 chkui.com JdbcCursorItemReader itemReader = new JdbcCursorItemReader(); itemReader.setDataSource(dataSource); itemReader.setSql(\\\"SELECT ID, NAME, CREDIT from CUSTOMER\\\"); itemReader.setRowMapper(new CustomerCreditRowMapper()); \",\"除了上面的代码，JdbcCursorItemReader还有其他属性：\",\"属性名称\",\"说明\",\"ignoreWarnings\",\"标记当执行SQL语句出现警告时，是输出日志还是抛出异常，默认为true——输出日志\",\"fetchSize\",\"预通知JDBC驱动全量数据的个数\",\"maxRows\",\"设置ResultSet从数据库中一次读取记录的上限\",\"queryTimeout\",\"设置执行SQL语句的等待超时时间，单位秒。当超过这个时间会抛出DataAccessException\",\"verifyCursorPosition\",\"对游标位置进行校验。由于在RowMapper::mapRow方法中ResultSet是直接暴露给使用者的，因此有可能在业务代码层面调用了ResultSet::next方法。将这个属性设置为true,在框架中会有一个位置计数器与ResultSet保持一致，当执行完Reader后位置不一致会抛出异常。\",\"saveState\",\"标记读取的状态是否被存放到ExecutionContext中。默认为true\",\"driverSupportsAbsolute\",\"告诉框架是指直接使用ResultSet::absolute方法来指定游标位置，使用这个属性需要数据库驱动支持。建议在支持absolute特性的数据库上开启这个特性，能够明显的提升性能。默认为false\",\"setUseSharedExtendedConnection\",\"标记读取数据的游标是否与Step其他过程绑定成同一个事物。默认为false,表示读取数据的游标是单独建立连接的，具有自身独立的事物。如果设定为true需要用ExtendedConnectionDataSourceProxy包装DataSource用于管理事物过程。此时游标的创建标记为'READ_ONLY'、'HOLD_CURSORS_OVER_COMMIT'。需要注意的是该属性需要数据库支持3.0以上的JDBC驱动。\",\"执行JdbcCursorItemReader的代码在org.chenkui.spring.batch.sample.items.JdbcReader。启动位置是org.chenkui.spring.batch.sample.database.cursor.JdbcCurosrApplication。\",\"在运行代码之前请先在数据库中执行以下DDL语句，并添加部分测试数据。\",\"CREATE TABLE `tmp_test_weather` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `siteid` varchar(64) NOT NULL COMMENT '业务主键', `month` varchar(64) NOT NULL COMMENT '日期', `type` varchar(64) NOT NULL COMMENT '气象类型', `value` int(11) NOT NULL COMMENT '值', `ext` varchar(255) DEFAULT NULL COMMENT '扩展数据', PRIMARY KEY (`id`) ) ; \",\"运行代码：\",\"//随风溜达的向日葵 chkui.com public class JdbcReader { @Bean public RowMapper<WeatherEntity> weatherEntityRowMapper() { return new RowMapper<WeatherEntity>() { public static final String SITEID_COLUMN = \\\"siteId\\\"; // 设置映射字段 public static final String MONTH_COLUMN = \\\"month\\\"; public static final String TYPE_COLUMN = \\\"type\\\"; public static final String VALUE_COLUMN = \\\"value\\\"; public static final String EXT_COLUMN = \\\"ext\\\"; @Override // 数据转换 public WeatherEntity mapRow(ResultSet resultSet, int rowNum) throws SQLException { WeatherEntity weatherEntity = new WeatherEntity(); weatherEntity.setSiteId(resultSet.getString(SITEID_COLUMN)); weatherEntity.setMonth(resultSet.getString(MONTH_COLUMN)); weatherEntity.setType(WeatherEntity.Type.valueOf(resultSet.getString(TYPE_COLUMN))); weatherEntity.setValue(resultSet.getInt(VALUE_COLUMN)); weatherEntity.setExt(resultSet.getString(EXT_COLUMN)); return weatherEntity; } }; } @Bean public ItemReader<WeatherEntity> jdbcCursorItemReader( @Qualifier(\\\"weatherEntityRowMapper\\\") RowMapper<WeatherEntity> rowMapper, DataSource datasource) { JdbcCursorItemReader<WeatherEntity> itemReader = new JdbcCursorItemReader<>(); itemReader.setDataSource(datasource); //设置DataSource //设置读取的SQL itemReader.setSql(\\\"SELECT siteId, month, type, value, ext from TMP_TEST_WEATHER\\\"); itemReader.setRowMapper(rowMapper); //设置转换 return itemReader; } } \"]},{\"header\":\"HibernateCursorItemReader\",\"slug\":\"hibernatecursoritemreader\",\"contents\":[\"在Java体系中数据库操作常见的规范有JPA或ORM，Spring Batch提供了HibernateCursorItemReader来实现HibernateTemplate,它可以通过Hibernate框架进行游标的控制。\",\"需要注意的是：使用Hibernate框架来处理批量数据到目前为止一直都有争议，核心原因是Hibernate最初是为在线联机事物型系统开发的。不过这并不意味着不能使用它来处理批数据，解决此问题就是让Hibernate使用StatelessSession用来保持游标，而不是standard session一次读写，这将导致Hibernate的缓存机制和数据脏读检查失效，进而影响批处理的过程。关于Hibernate的状态控制机制请阅读官方文档。\",\"HibernateCursorItemReader使用过程与JdbcCursorItemReader没多大差异都是逐条读取数据然后控制状态链接关闭。只不过他提供了Hibernate所使用的HSQL方案。\",\"@Bean public ItemReader<WeatherEntity> hibernateCursorItemReader(SessionFactory sessionFactory) { HibernateCursorItemReader<WeatherEntity> itemReader = new HibernateCursorItemReader<>(); itemReader.setName(\\\"hibernateCursorItemReader\\\"); itemReader.setQueryString(\\\"from WeatherEntity tmp_test_weather\\\"); itemReader.setSessionFactory(sessionFactory); return itemReader; } \",\"或\",\"public ItemReader<WeatherEntity> hibernateCursorItemReader(SessionFactory sessionFactory) { return new HibernateCursorItemReaderBuilder<CustomerCredit>() .name(\\\"creditReader\\\") .sessionFactory(sessionFactory) .queryString(\\\"from CustomerCredit\\\") .build(); } \",\"如果没有特别的需要，不推荐使用Hibernate。\"]},{\"header\":\"StoredProcedureItemReader\",\"slug\":\"storedprocedureitemreader\",\"contents\":[\"存储过程是在同一个数据库中处理大量数据的常用方法。StoredProcedureItemReader的执行过程和JdbcCursorItemReader一致，但是底层逻辑是先执行存储过程，然后返回存储过程执行结果游标。不同的数据库存储过程游标返回会有一些差异：\",\"作为一个ResultSet返回。（SQL Server Sybase, DB2, Derby以及MySQL）\",\"参数返回一个 ref-cursor实例。比如Oracle、PostgreSQL数据库，这类数据库存储过程是不会直接return任何内容的，需要从传参获取。\",\"返回存储过程调用后的返回值。\",\"针对以上3个类型，配置上有一些差异：\",\"//随风溜达的向日葵 chkui.com @Bean public StoredProcedureItemReader reader(DataSource dataSource) { StoredProcedureItemReader reader = new StoredProcedureItemReader(); reader.setDataSource(dataSource); reader.setProcedureName(\\\"sp_processor_weather\\\"); reader.setRowMapper(new weatherEntityRowMapper()); //第二种类型需要指定ref-cursor的参数位置 reader.setRefCursorPosition(1); //第三种类型需要明确的告知reader通过返回获取 reader.setFunction(true); return reader; } \",\"使用存储过程处理数据的好处是可以实现针对库内的数据进行合并、分割、排序等处理。如果数据在同一个数据库，性能也明显好于通过Java处理。\"]},{\"header\":\"分页读取数据\",\"slug\":\"分页读取数据\",\"contents\":[\"相对于游标，还有一个办法是进行分页查询。分页查询意味着再进行批处理的过程中同一个SQL会多次执行。在联机型事物系统中分页查询常用于列表功能，每一次查询需要指定开始位置和结束位置。\"]},{\"header\":\"JdbcPagingItemReader\",\"slug\":\"jdbcpagingitemreader\",\"contents\":[\"分页查询的默认实现类是JdbcPagingItemReader，它的核心功能是用分页器PagingQueryProvider进行分页控制。由于不同的数据库分页方法差别很大，所以针对不同的数据库有不同的实现类。框架提供了SqlPagingQueryProviderFactoryBean用于检查当前数据库并自动注入对应的PagingQueryProvider。\",\"JdbcPagingItemReader会从数据库中一次性读取一整页的数据，但是调用Reader的时候还是会一行一行的返回数据。框架会自行根据运行情况确定什么时候需要执行下一个分页的查询。\"]},{\"header\":\"分页读取数据执行源码\",\"slug\":\"分页读取数据执行源码\",\"contents\":[\"Gitee：https://gitee.com/chkui-com/spring-batch-sample\",\"Github：https://github.com/chkui/spring-batch-sample\",\"执行JdbcPagingItemReader的代码在org.chenkui.spring.batch.sample.items.pageReader。启动位置是org.chenkui.spring.batch.sample.database.paging.JdbcPagingApplication：\",\"//随风溜达的向日葵 chkui.com public class pageReader { final private boolean wrapperBuilder = false; @Bean //设置 queryProvider public SqlPagingQueryProviderFactoryBean queryProvider(DataSource dataSource) { SqlPagingQueryProviderFactoryBean provider = new SqlPagingQueryProviderFactoryBean(); provider.setDataSource(dataSource); provider.setSelectClause(\\\"select id, siteid, month, type, value, ext\\\"); provider.setFromClause(\\\"from tmp_test_weather\\\"); provider.setWhereClause(\\\"where id>:start\\\"); provider.setSortKey(\\\"id\\\"); return provider; } @Bean public ItemReader<WeatherEntity> jdbcPagingItemReader(DataSource dataSource, PagingQueryProvider queryProvider, RowMapper<WeatherEntity> rowMapper) { Map<String, Object> parameterValues = new HashMap<>(); parameterValues.put(\\\"start\\\", \\\"1\\\"); JdbcPagingItemReader<WeatherEntity> itemReader; if (wrapperBuilder) { itemReader = new JdbcPagingItemReaderBuilder<WeatherEntity>() .name(\\\"creditReader\\\") .dataSource(dataSource) .queryProvider(queryProvider) .parameterValues(parameterValues) .rowMapper(rowMapper) .pageSize(1000) .build(); } else { itemReader = new JdbcPagingItemReader<>(); itemReader.setName(\\\"weatherEntityJdbcPagingItemReader\\\"); itemReader.setDataSource(dataSource); itemReader.setQueryProvider(queryProvider); itemReader.setParameterValues(parameterValues); itemReader.setRowMapper(rowMapper); itemReader.setPageSize(1000); } return itemReader; } } \"]},{\"header\":\"数据写入\",\"slug\":\"数据写入\",\"contents\":[\"Spring Batch为不同类型的文件的写入提供了多个实现类，但并没有为数据库的写入提供任何实现类，而是交由开发者自己去实现接口。理由是：\",\"数据库的写入与文件写入有巨大的差别。对于一个Step而言，在写入一份文件时需要保持对文件的打开状态从而能够高效的向队尾添加数据。如果每次都重新打开文件，从开始位置移动到队尾会耗费大量的时间（很多文件流无法在open时就知道长度）。当整个Step结束时才能关闭文件的打开状态，框架提供的文件读写类都实现了这个控制过程。\",\"另外无论使用何种方式将数据写入文件都是\\\"逐行进行\\\"的（流数据写入、字符串逐行写入）。因此当数据写入与整个Step绑定为事物时还需要实现一个控制过程是：在写入数据的过程中出现异常时要擦除本次事物已经写入的数据，这样才能和整个Step的状态保持一致。框架中的类同样实现了这个过程。\",\"但是向数据库写入数据并不需要类似于文件的尾部写入控制，因为数据库的各种链接池本身就保证了链接->写入->释放的高效执行，也不存在向队尾添加数据的问题。而且几乎所有的数据库驱动都提供了事物能力，在任何时候出现异常都会自动回退，不存在擦除数据的问题。\",\"因此，对于数据库的写入操作只要按照常规的批量数据写入的方式即可，开发者使用任何工具都可以完成这个过程。\"]},{\"header\":\"写入数据一个简单的实现\",\"slug\":\"写入数据一个简单的实现\",\"contents\":[\"实现数据写入方法很多，这和常规的联机事务系统没任何区别。下面直接用JdbcTemplate实现了一个简单的数据库写入过程。\",\"执行数据库写入的核心代码在org.chenkui.spring.batch.sample.items.JdbcWriter。启动位置是org.chenkui.spring.batch.sample.database.output.JdbcWriterApplication。\",\"//随风溜达的向日葵 chkui.com public class JdbcWriter { @Bean public ItemWriter<WeatherEntity> jdbcBatchWriter(JdbcTemplate template) { return new ItemWriter<WeatherEntity>() { final private static String INSERt_SQL = \\\"INSERT INTO tmp_test_weather(siteid, month, type, value, ext) VALUES(?,?,?,?,?)\\\"; @Override public void write(List<? extends WeatherEntity> items) throws Exception { List<Object[]> batchArgs = new ArrayList<>(); for (WeatherEntity entity : items) { Object[] objects = new Object[5]; objects[0] = entity.getSiteId(); objects[1] = entity.getMonth(); objects[2] = entity.getType().name(); objects[3] = entity.getValue(); objects[4] = entity.getExt(); batchArgs.add(objects); } template.batchUpdate(INSERt_SQL, batchArgs); } }; } } \"]},{\"header\":\"组合使用案例\",\"slug\":\"组合使用案例\",\"contents\":[\"下面是一些组合使用过程，简单实现了文件到数据库、数据库到文件的过程。文件读写的过程已经在文件读写中介绍过，这里会重复使用之前介绍的文件读写的功能。\",\"下面的案例是将data.csv中的数据写入到数据库，然后再将数据写入到out-data.csv。案例组合使用已有的item完成任务：flatFileReader、jdbcBatchWriter、jdbcCursorItemReader、simpleProcessor、flatFileWriter。这种Reader、Processor、Writer组合的方式也是完成一个批处理工程的常见开发方式。\",\"案例的运行代码在org.chenkui.spring.batch.sample.database.complex包中，使用了2个Step来完成任务，一个将数据读取到数据库，一个将数据进行过滤，然后再写入到文件：\",\"//随风溜达的向日葵 chkui.com public class FileComplexProcessConfig { @Bean // 配置Step1 public Step file2DatabaseStep(StepBuilderFactory builder, @Qualifier(\\\"flatFileReader\\\") ItemReader<WeatherEntity> reader, @Qualifier(\\\"jdbcBatchWriter\\\") ItemWriter<WeatherEntity> writer) { return builder.get(\\\"file2DatabaseStep\\\") // 创建 .<WeatherEntity, WeatherEntity>chunk(50) // 分片 .reader(reader) // 读取 .writer(writer) // 写入 .faultTolerant() // 开启容错处理 .skipLimit(20) // 跳过设置 .skip(Exception.class) // 跳过异常 .build(); } @Bean // 配置Step2 public Step database2FileStep(StepBuilderFactory builder, @Qualifier(\\\"jdbcCursorItemReader\\\") ItemReader<WeatherEntity> reader, @Qualifier(\\\"simpleProcessor\\\") ItemProcessor<WeatherEntity, MaxTemperatureEntiry> processor, @Qualifier(\\\"flatFileWriter\\\") ItemWriter<MaxTemperatureEntiry> writer) { return builder.get(\\\"database2FileStep\\\") // 创建 .<WeatherEntity, MaxTemperatureEntiry>chunk(50) // 分片 .reader(reader) // 读取 .processor(processor) // .writer(writer) // 写入 .faultTolerant() // 开启容错处理 .skipLimit(20) // 跳过设置 .skip(Exception.class) // 跳过异常 .build(); } @Bean public Job file2DatabaseJob(@Qualifier(\\\"file2DatabaseStep\\\") Step step2Database, @Qualifier(\\\"database2FileStep\\\") Step step2File, JobBuilderFactory builder) { return builder.get(\\\"File2Database\\\").start(step2Database).next(step2File).build(); } } \"]},{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"Spring Batch(1)——数据批处理概念 (tencent.com)\",\"Spring Batch中@StepScope的适用范围及理解_lovepeacee的博客-CSDN博客\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatchJob.html\":{\"title\":\"Spring Batch——Job配置与运行\",\"contents\":[{\"header\":\"Spring Batch——Job配置与运行\",\"slug\":\"spring-batch——job配置与运行\",\"contents\":[\"​ 在上文中介绍了批处理的概念以及Spring Batch相关的使用场景，后续将会陆续说明在代码层面如何使用。\"]},{\"header\":\"引入\",\"slug\":\"引入\",\"contents\":[\"​ Spring batch的引入非常简单，只需要引入Spring Framework、Datasource以及Spring Batch。在Spring Boot体系下只需引入spring-boot-starter-batch 即可。他已经涵盖了以上所有内容。\"]},{\"header\":\"Job配置\",\"slug\":\"job配置\",\"contents\":[\"Job接口有多种多样的实现类，通常我们使用configuration类来构建获取一个Job：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") //Job名称 .start(playerLoad()) //Job Step .next(gameLoad()) //Job Step .next(playerSummarization()) //Job Step .end() .build(); } \",\"上面的代码定义了一个Job实例，并且在这个实例中包含了三个Step实例\"]},{\"header\":\"重启（启动）配置\",\"slug\":\"重启-启动-配置\",\"contents\":[\"​ 批处理的一个核心问题是需要定义重启（启动）时的一些行为。当指定的JobInstance 被JobExecution 执行时候即认为某个Job已经重启（启动）。理想状态下，所有的任务都应该可以从它们之前中断的位置启动，但是某些情况下这样做是无法实现的。开发人员可以关闭重启机制或认为每次启动都是新的JobInstance ：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") .preventRestart() //防止重启 ... .build(); } \"]},{\"header\":\"监听Job Execution\",\"slug\":\"监听job-execution\",\"contents\":[\"当任务执行完毕或开始执行时，需要执行一些处理工作。这个时候可以使用JobExecutionListener：\",\"public interface JobExecutionListener { void beforeJob(JobExecution jobExecution); void afterJob(JobExecution jobExecution); } \",\"添加方式：\",\"@Bean public Job footballJob() { return this.jobBuilderFactory.get(\\\"footballJob\\\") .listener(sampleListener()) //JobExecutionListener的实现类 ... .build(); } \",\"需要注意的是afterJob方法无论批处理任务成功还是失败都会被执行，所以增加以下判断：\",\"public void afterJob(JobExecution jobExecution){ if( jobExecution.getStatus() == BatchStatus.COMPLETED ){ //job success } else if(jobExecution.getStatus() == BatchStatus.FAILED){ //job failure } } \",\"除了直接实现接口还可以用 @BeforeJob 和 @AfterJob 注解。\"]},{\"header\":\"Java配置\",\"slug\":\"java配置\",\"contents\":[\"​ 在Spring Batch 2.2.0版本之后（Spring 3.0+）支持纯Java配置。其核心是@EnableBatchProcessing 注解和两个构造器。@EnableBatchProcessing的作用类似于Spring中的其他@Enable*,使用@EnableBatchProcessing之后会提供一个基本的配置用于执行批处理任务。\",\"​ 对应的会有一系列StepScope实例被注入到Ioc容器中：JobRepository、JobLauncher、JobRegistry、PlatformTransactionManager、JobBuilderFactory以及StepBuilderFactory。\",\"​ 配置的核心接口是BatchConfigurer，默认情况下需要在容器中指定DataSource，该数据源用于JobRepository相关的表。开发的过程中可以使用自定义的BatchConfigurer实现来提供以上所有的Bean。通常情况下可以扩展重载DefaultBatchConfigurer类中的Getter方法用于实现部分自定义功能：\",\"@Bean public BatchConfigurer batchConfigurer() { return new DefaultBatchConfigurer() { @Override public PlatformTransactionManager getTransactionManager() { return new MyTransactionManager(); } }; } \",\"使用了@EnableBatchProcessing之后开发人员可以使用以下的方法来配置一个Job：\",\"@Configuration @EnableBatchProcessing @Import(DataSourceConfiguration.class) public class AppConfig { @Autowired private JobBuilderFactory jobs; @Autowired private StepBuilderFactory steps; @Bean public Job job(@Qualifier(\\\"step1\\\") Step step1,@Qualifier(\\\"step2\\\") Step step2) { return jobs.get(\\\"myJob\\\").start(step1).next(step2).build(); } @Bean protected Step step1(ItemReader<Person> reader, ItemProcessor<Person, Person> processor, ItemWriter<Person> writer) { return steps.get(\\\"step1\\\") .<Person, Person> chunk(10) .reader(reader) .processor(processor) .writer(writer) .build(); } @Bean protected Step step2(Tasklet tasklet) { return steps.get(\\\"step2\\\") .tasklet(tasklet) .build(); } } \"]},{\"header\":\"JobRepository配置\",\"slug\":\"jobrepository配置\",\"contents\":[\"​ 一旦使用了@EnableBatchProcessing 注解，JobRepository即会被注入到IoCs容器中并自动使用容器中的DataSource。JobRepository用于处理批处理表的CURD，整个Spring Batch的运行都会使用到它。除了使用容器中默认的DataSoruce以及其他组件，还可以在BatchConfigurer中进行配置：\",\"@Override protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setIsolationLevelForCreate(\\\"ISOLATION_SERIALIZABLE\\\"); factory.setTablePrefix(\\\"BATCH_\\\"); factory.setMaxVarCharLength(1000); return factory.getObject(); } \",\"在代码中可以看到，设置JobRepository需要DataSource和TransactionManager，如果没有指定将会使用容器中的默认配置。\"]},{\"header\":\"JobRepository的事物配置\",\"slug\":\"jobrepository的事物配置\",\"contents\":[\"​ 默认情况下框架为JobRepository提供了默认PlatformTransactionManager事物管理。它用于确保批处理执行过程中的元数据正确的写入到指定数据源中。如果缺乏事物，那么框架产生元数据就无法和整个处理过程完全契合。\",\"​ 如下图，在BatchConfigurer中的setIsolationLevelForCreate方法中可以指定事物的隔离等级：\",\"protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setIsolationLevelForCreate(\\\"ISOLATION_REPEATABLE_READ\\\"); return factory.getObject(); } \",\"setIsolationLevelForCreate方法支持2个值：ISOLATION_SERIALIZABLE 、ISOLATION_REPEATABLE_READ ，前者是默认配置，类似于@Transactional(isolation = Isolation.SERIALIZABLE)，表示查询和写入都是一次事物，会对事物进行严格的锁定，当事物完成提交后才能进行其他的读写操作，容易死锁。后者是读事物开放，写事物锁定。任何时候都可以快速的读取数据，但是写入事物有严格的事物机制。当一个事物挂起某些记录时，其他写操作必须排队。\"]},{\"header\":\"修改表名称\",\"slug\":\"修改表名称\",\"contents\":[\"默认情况下，JobRepository管理的表都以*BATCH_*开头。需要时可以修改前缀：\",\"// This would reside in your BatchConfigurer implementation @Override protected JobRepository createJobRepository() throws Exception { JobRepositoryFactoryBean factory = new JobRepositoryFactoryBean(); factory.setDataSource(dataSource); factory.setTransactionManager(transactionManager); factory.setTablePrefix(\\\"SYSTEM.TEST_\\\"); //修改前缀 return factory.getObject(); } \"]},{\"header\":\"内存级存储\",\"slug\":\"内存级存储\",\"contents\":[\"​ Spring Batch支持将运行时的状态数据（元数据）仅保存在内存中。重载JobRepository不设置DataSource 即可：\",\"@Override protected JobRepository createJobRepository() throws Exception { MapJobRepositoryFactoryBean factory = new MapJobRepositoryFactoryBean(); factory.setTransactionManager(transactionManager); return factory.getObject(); } \",\"需要注意的是，内存级存储无法满足分布式系统。\"]},{\"header\":\"JobLauncher配置\",\"slug\":\"joblauncher配置\",\"contents\":[\"启用了@EnableBatchProcessing 之后JobLauncher 会自动注入到容器中以供使用。此外可以自行进行配置：\",\"@Override protected JobLauncher createJobLauncher() throws Exception { SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository); jobLauncher.afterPropertiesSet(); return jobLauncher; } \",\"JobLauncher 唯一的必要依赖只有JobRepository。如下图，Job的执行通常是一个同步过程：\",\"![bpq56xfz43](img/Spring Batch Job/bpq56xfz43.png)\",\"可以通过修改TaskExecutor来指定Job的执行过程：\",\"@Bean public JobLauncher jobLauncher() { SimpleJobLauncher jobLauncher = new SimpleJobLauncher(); jobLauncher.setJobRepository(jobRepository()); jobLauncher.setTaskExecutor(new SimpleAsyncTaskExecutor()); //转换为异步任务 jobLauncher.afterPropertiesSet(); return jobLauncher; } \",\"这样执行过程变为：\"]},{\"header\":\"运行一个Job\",\"slug\":\"运行一个job\",\"contents\":[\"以一个Http为例：\",\"@Controller public class JobLauncherController { @Autowired JobLauncher jobLauncher; @Autowired Job job; @RequestMapping(\\\"/jobLauncher.html\\\") public void handle() throws Exception{ jobLauncher.run(job, new JobParameters()); } } \",\"单单是配置好Job是肯定无法执行的，还需要对Step进行配置。后面会陆续介绍。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/SpringBatchStep.html\":{\"title\":\"Spring Batch——Step控制\",\"contents\":[{\"header\":\"Spring Batch——Step控制\",\"slug\":\"spring-batch——step控制\",\"contents\":[\"​ 批处理任务的主要业务逻辑都是在Step中去完成的。可以将Job理解为运行Step的框架，而Step理解为业务功能。\"]},{\"header\":\"Step配置\",\"slug\":\"step配置\",\"contents\":[\"​ Step是Job中的工作单元，每一个Step涵盖了单行记录的处理闭环。下图是一个Step的简要结构：\",\"​ 一个Step通常涵盖三个部分：读数据（Reader）、处理数据（Processor）和写数据（Writer）。但是并不是所有的Step 都需要自身来完成数据的处理，比如存储过程等方式是通过外部功能来完成，因此Spring Batch提供了2种Step的处理方式：1）面向分片的ChunkStep，2）面向过程的TaskletStep。但是基本上大部分情况下都是使用面向分片的方式来解决问题。\"]},{\"header\":\"面向分片的处理过程\",\"slug\":\"面向分片的处理过程\",\"contents\":[\"​ 在Step中数据是按记录（按行）处理的，但是每条记录处理完毕之后马上提交事物反而会导致IO的巨大压力。因此Spring Batch提供了数据处理的分片功能。设置了分片之后，一次工作会从Read开始，然后交由给Processor处理。处理完毕后会进行聚合，待聚合到一定的数量的数据之后一次性调用Write将数据提交到物理数据库。其过程大致为：\",\"在Spring Batch中所谓的事物和数据事物的概念一样，就是一次性提交多少数据。如果在聚合数据期间出现任何错误，所有的这些数据都将不执行写入。\"]},{\"header\":\"分区\",\"slug\":\"分区\",\"contents\":[\"Spring Batch也为Step的分区执行和远程执行提供了一个SPI(服务提供者接口)。在这种情况下,远端的执行程序只是一些简单的Step实例,配置和使用方式都和本机处理一样容易。下面是一幅实际的模型示意图:\",\"在左侧执行的作业(Job)是串行的Steps,而中间的那一个Step被标记为 Master。图中的 Slave 都是一个Step的相同实例,对于作业来说,这些Slave的执行结果实际上等价于就是Master的结果。Slaves通常是远程服务,但也有可能是本地执行的其他线程。在此模式中,Master发送给Slave的消息不需要持久化(durable) ,也不要求保证交付: 对每个作业执行步骤来说,保存在 JobRepository 中的Spring Batch元信息将确保每个Slave都会且仅会被执行一次。\",\"Spring Batch的SPI由Step的一个专门的实现( PartitionStep),以及需要由特定环境实现的两个策略接口组成。这两个策略接口分别是 PartitionHandler 和 StepExecutionSplitter,他们的角色如下面的序列图所示:此时在右边的Step就是“远程”Slave。\"]},{\"header\":\"分区处理器(PartitionHandler)\",\"slug\":\"分区处理器-partitionhandler\",\"contents\":[\"PartitionHandler组件知道远程网格环境的组织结构。 它可以发送StepExecution请求给远端Steps,采用某种具体的数据格式,例如DTO.它不需要知道如何分割输入数据,或者如何聚合多个步骤执行的结果。一般来说它可能也不需要了解弹性或故障转移,因为在许多情况下这些都是结构的特性,无论如何Spring Batch总是提供了独立于结构的可重启能力: 一个失败的作业总是会被重新启动,并且只会重新执行失败的步骤。\",\"PartitionHandler接口可以有各种结构的实现类: 如简单RMI远程方法调用,EJB远程调用,自定义web服务、JMS、Java Spaces, 共享内存网格(如Terracotta或Coherence)、网格执行结构(如GridGain)。Spring Batch自身不包含任何专有网格或远程结构的实现。\",\"但是 Spring Batch也提供了一个有用的PartitionHandler实现，在本地分开的线程中执行Steps,该实现类名为 TaskExecutorPartitionHandler,并且他是上面的XML配置中的默认处理器。还可以像下面这样明确地指定:\",\"<step id=\\\"step1.master\\\"> <partition step=\\\"step1\\\" handler=\\\"handler\\\"/> </step> <bean class=\\\"org.spr...TaskExecutorPartitionHandler\\\"> <property name=\\\"taskExecutor\\\" ref=\\\"taskExecutor\\\"/> <property name=\\\"step\\\" ref=\\\"step1\\\" /> <property name=\\\"gridSize\\\" value=\\\"10\\\" /> </bean> \",\"gridSize决定要创建的独立的step执行的数量,所以它可以配置为TaskExecutor中线程池的大小,或者也可以设置得比可用的线程数稍大一点,在这种情况下,执行块变得更小一些。\",\"TaskExecutorPartitionHandler 对于IO密集型步骤非常给力,比如要拷贝大量的文件,或复制文件系统到内容管理系统时。它还可用于远程执行的实现,通过为远程调用提供一个代理的步骤实现(例如使用Spring Remoting)，如下面代码注入到 Spring 容器当中使用\",\"@Bean public PartitionHandler suyqrPartitionHandler(){ // 分格，多线程处理 TaskExecutorPartitionHandler handler = new TaskExecutorPartitionHandler(); handler.setGridSize(10); handler.setStep(suyqrSlaveStep()); handler.setTaskExecutor(taskExecutor); try { handler.afterPropertiesSet(); } catch (Exception e) { e.printStackTrace(); } return handler; } \"]},{\"header\":\"分割器(Partitioner)\",\"slug\":\"分割器-partitioner\",\"contents\":[\"分割器有一个简单的职责: 仅为新的step实例生成执行环境(contexts),作为输入参数(这样重启时就不需要考虑)。 该接口只有一个方法:\",\"public interface Partitioner { Map<String, ExecutionContext> partition(int gridSize); } \",\"这个方法的返回值是一个Map对象,将每个Step执行分配的唯一名称(Map泛型中的 String),和与其相关的输入参数以ExecutionContext 的形式做一个映射。Step执行的名称( Partitioner接口返回的 Map 中的 key)在整个作业的执行过程中需要保持唯一,除此之外没有其他具体要求。简单说，实现 Partitioner 接口实现\"]},{\"header\":\"面向对象配置Step\",\"slug\":\"面向对象配置step\",\"contents\":[\"@Bean public Job sampleJob(JobRepository jobRepository, Step sampleStep) { return this.jobBuilderFactory.get(\\\"sampleJob\\\") .repository(jobRepository) .start(sampleStep) .build(); } @Bean public Step sampleStep(PlatformTransactionManager transactionManager) { return this.stepBuilderFactory.get(\\\"sampleStep\\\") .transactionManager(transactionManager) .<String, String>chunk(10) //分片配置 .reader(itemReader()) //reader配置 .writer(itemWriter()) //write配置 .build(); } \",\"观察sampleStep方法：\",\"reader: 使用ItemReader提供读数据的方法。\",\"write：ItemWrite提供写数据的方法。\",\"transactionManager：使用默认的 PlatformTransactionManager 对事物进行管理。当配置好事物之后Spring Batch会自动对事物进行管理，无需开发人员显示操作。\",\"chunk：指定一次性数据提交的记录数，因为任务是基于Step分次处理的，当累计到chunk配置的次数则进行一次提交。提交的内容除了业务数据，还有批处理任务运行相关的元数据。\",\"是否使用ItemProcessor是一个可选项。如果没有Processor可以将数据视为读取并直接写入。\"]},{\"header\":\"提交间隔\",\"slug\":\"提交间隔\",\"contents\":[\"​ Step使用PlatformTransactionManager管理事物。每次事物提交的间隔根据chunk方法中配置的数据执行。如果设置为1，那么在每一条数据处理完之后都会调用ItemWrite进行提交。提交间隔设置太小，那么会浪费需要多不必要的资源，提交间隔设置的太长，会导致事物链太长占用空间，并且出现失败会导致大量数据回滚。因此设定一个合理的间隔是非常必要的，这需要根据实际业务情况、性能要求、以及数据安全程度来设定。如果没有明确的评估目标，设置为10~20较为合适。\"]},{\"header\":\"配置Step重启\",\"slug\":\"配置step重启\",\"contents\":[\"前文介绍了Job的重启，但是每次重启对Step也是有很大的影响的，因此需要特定的配置。\"]},{\"header\":\"限定重启次数\",\"slug\":\"限定重启次数\",\"contents\":[\"​ 某些Step可能用于处理一些先决的任务，所以当Job再次重启时这Step就没必要再执行，可以通过设置startLimit来限定某个Step重启的次数。当设置为1时候表示仅仅运行一次，而出现重启时将不再执行：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(itemReader()) .writer(itemWriter()) .startLimit(1) .build(); } \"]},{\"header\":\"重启已经完成任务的Step\",\"slug\":\"重启已经完成任务的step\",\"contents\":[\"​ 在单个JobInstance的上下文中，如果某个Step已经处理完毕（COMPLETED）那么在默认情况下重启之后这个Step并不会再执行。可以通过设置allow-start-if-complete为true告知框架每次重启该Step都要执行：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(itemReader()) .writer(itemWriter()) .allowStartIfComplete(true) .build(); } \"]},{\"header\":\"配置略过逻辑\",\"slug\":\"配置略过逻辑\",\"contents\":[\"​ 某些时候在任务处理单个记录时中出现失败并不应该停止任务，而应该跳过继续处理下一条数据。是否跳过需要根据业务来判定，因此框架提供了跳过机制交给开发人员使用。如何配置跳过机制：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(flatFileItemReader()) .writer(itemWriter()) .faultTolerant() .skipLimit(10) .skip(FlatFileParseException.class) .build(); } \",\"代码的含义是当处理过程中抛出FlatFileParseException异常时就跳过该条记录的处理。skip-limit（skipLimit方法）配置的参数表示当跳过的次数超过数值时则会导致整个Step失败，从而停止继续运行。还可以通过反向配置的方式来忽略某些异常：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(flatFileItemReader()) .writer(itemWriter()) .faultTolerant() .skipLimit(10) .skip(Exception.class) .noSkip(FileNotFoundException.class) .build(); } \",\"skip表示要当捕捉到Exception异常就跳过。但是Exception有很多继承类，此时可以使用noSkip方法指定某些异常不能跳过。\"]},{\"header\":\"设置重试逻辑\",\"slug\":\"设置重试逻辑\",\"contents\":[\"当处理记录出个异常之后并不希望他立即跳过或者停止运行，而是希望可以多次尝试执行直到失败：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .faultTolerant() .retryLimit(3) .retry(DeadlockLoserDataAccessException.class) .build(); } \",\"retry(DeadlockLoserDataAccessException.class)表示只有捕捉到该异常才会重试，retryLimit(3)表示最多重试3次，faultTolerant()表示启用对应的容错功能。\"]},{\"header\":\"Step 中的事务\",\"slug\":\"step-中的事务\",\"contents\":[]},{\"header\":\"事物回滚控制\",\"slug\":\"事物回滚控制\",\"contents\":[\"​ 默认情况下，无论是设置了重试（retry）还是跳过（skip），只要从Writer抛出一个异常都会导致事物回滚。如果配置了skip机制，那么在Reader中抛出的异常不会导致回滚。有些从Writer抛出一个异常并不需要回滚数据，noRollback属性为Step提供了不必进行事物回滚的异常配置：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .faultTolerant() //不必回滚的异常 .noRollback(ValidationException.class) .build(); } \"]},{\"header\":\"事物数据读取的缓存\",\"slug\":\"事物数据读取的缓存\",\"contents\":[\"​ 一次Setp分为Reader、Processor 和Writer 三个阶段，这些阶段统称为Item。默认情况下如果错误不是发生在Reader阶段，那么没必要再去重新读取一次数据。但是某些场景下需要Reader部分也需要重新执行，比如Reader是从一个JMS队列中消费消息，当发生回滚的时候消息也会在队列上重放，因此也要将Reader纳入到回滚的事物中，根据这个场景可以使用readerIsTransactionalQueue 来配置数据重读：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .readerIsTransactionalQueue() //数据重读 .build(); } \"]},{\"header\":\"事物属性\",\"slug\":\"事物属性\",\"contents\":[\"事物的属性包括隔离等级（isolation）、传播方式（propagation）以及过期时间（timeout）。关于事物的控制详见Spring Data Access的说明，下面是相关配置的方法：\",\"@Bean public Step step1() { //配置事物属性 DefaultTransactionAttribute attribute = new DefaultTransactionAttribute(); attribute.setPropagationBehavior(Propagation.REQUIRED.value()); attribute.setIsolationLevel(Isolation.DEFAULT.value()); attribute.setTimeout(30); return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(itemWriter()) .transactionAttribute(attribute) //设置事物属性 .build(); } \"]},{\"header\":\"向Step注册 ItemStream\",\"slug\":\"向step注册-itemstream\",\"contents\":[\"​ ItemStream是用于每一个阶段（Reader、Processor、Writer）的”生命周期回调数据处理器“，后续的文章会详细介绍ItemStream。在4.×版本之后默认注入注册了通用的ItemStream。\",\"有2种方式将ItemStream注册到Step中，一是使用stream方法：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(2) .reader(itemReader()) .writer(compositeItemWriter()) .stream(fileItemWriter1()) .stream(fileItemWriter2()) .build(); } \",\"二是使用相关方法的代理：\",\"@Bean public CompositeItemWriter compositeItemWriter() { List<ItemWriter> writers = new ArrayList<>(2); writers.add(fileItemWriter1()); writers.add(fileItemWriter2()); CompositeItemWriter itemWriter = new CompositeItemWriter(); itemWriter.setDelegates(writers); return itemWriter; } \"]},{\"header\":\"StepExecution拦截器\",\"slug\":\"stepexecution拦截器\",\"contents\":[\"在Step执行的过程中会产生各种各样的事件，开发人员可以利用各种Listener接口对Step及Item进行监听。通常在创建一个Step的时候添加拦截器：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .<String, String>chunk(10) .reader(reader()) .writer(writer()) .listener(chunkListener()) //添加拦截器 .build(); } \",\"Spring Batch提供了多个接口以满足不同事件的监听。\"]},{\"header\":\"StepExecutionListener\",\"slug\":\"stepexecutionlistener\",\"contents\":[\"StepExecutionListener可以看做一个通用的Step拦截器，他的作用是在Step开始之前和结束之后进行拦截处理：\",\"public interface StepExecutionListener extends StepListener { void beforeStep(StepExecution stepExecution); //Step执行之前 ExitStatus afterStep(StepExecution stepExecution); //Step执行完毕之后 } \",\"在结束的时候开发人员可以自己定义返回的ExitStatus，用于配合流程控制（见后文）实现对整个Step执行过程的控制。\"]},{\"header\":\"ChunkListener\",\"slug\":\"chunklistener\",\"contents\":[\"ChunkListener是在数据事物发生的两端被触发。chunk的配置决定了处理多少项记录才进行一次事物提交，ChunkListener的作用就是对一次事物开始之后或事物提交之后进行拦截：\",\"public interface ChunkListener extends StepListener { void beforeChunk(ChunkContext context); //事物开始之后，ItemReader调用之前 void afterChunk(ChunkContext context); //事物提交之后 void afterChunkError(ChunkContext context); //事物回滚之后 } \",\"如果没有设定chunk也可以使用ChunkListener，它会被TaskletStep调用（TaskletStep见后文）。\"]},{\"header\":\"ItemReadListener\",\"slug\":\"itemreadlistener\",\"contents\":[\"该接口用于对Reader相关的事件进行监控：\",\"public interface ItemReadListener<T> extends StepListener { void beforeRead(); void afterRead(T item); void onReadError(Exception ex); } \",\"beforeRead在每次Reader调用之前被调用，afterRead在每次Reader成功返回之后被调用，而onReadError会在出现异常之后被调用，可以将其用于记录异常日志。\"]},{\"header\":\"ItemProcessListener\",\"slug\":\"itemprocesslistener\",\"contents\":[\"ItemProcessListener和ItemReadListener类似，是围绕着ItemProcessor进行处理的：\",\"public interface ItemProcessListener<T, S> extends StepListener { void beforeProcess(T item); //processor执行之前 void afterProcess(T item, S result); //processor直线成功之后 void onProcessError(T item, Exception e); //processor执行出现异常 } \"]},{\"header\":\"ItemWriteListener\",\"slug\":\"itemwritelistener\",\"contents\":[\"ItemWriteListener的功能和ItemReadListener、ItemReadListener类似，但是需要注意的是它接收和处理的数据对象是一个List。List的长度与chunk配置相关。\",\"public interface ItemWriteListener<S> extends StepListener { void beforeWrite(List<? extends S> items); void afterWrite(List<? extends S> items); void onWriteError(Exception exception, List<? extends S> items); } \"]},{\"header\":\"SkipListener\",\"slug\":\"skiplistener\",\"contents\":[\"ItemReadListener、ItemProcessListener和ItemWriteListener都提供了错误拦截处理的机制，但是没有处理跳过（skip）的数据记录。因此框架提供了SkipListener来专门处理那么被跳过的记录：\",\"public interface SkipListener<T,S> extends StepListener { void onSkipInRead(Throwable t); //Read期间导致跳过的异常 void onSkipInProcess(T item, Throwable t); //Process期间导致跳过的异常 void onSkipInWrite(S item, Throwable t); //Write期间导致跳过的异常 } \",\"SkipListener的价值是可以将那些未能成功处理的记录在某个位置保存下来，然后交给其他批处理进一步解决，或者人工来处理。Spring Batch保证以下2个特征：\",\"跳过的元素只会出现一次。\",\"SkipListener始终在事物提交之前被调用，这样可以保证监听器使用的事物资源不会被业务事物影响。\"]},{\"header\":\"TaskletStep\",\"slug\":\"taskletstep\",\"contents\":[\"​ 面向分片（Chunk-oriented processing ）的过程并不是Step的唯一执行方式。比如用数据库的存储过程来处理数据，这个时候使用标准的Reader、Processor、Writer会很奇怪，针对这些情况框架提供了TaskletStep。 TaskletStep是一个非常简单的接口，仅有一个方法——execute。TaskletStep会反复的调用这个方法直到获取一个RepeatStatus.FINISHED返回或者抛出一个异常。所有的Tasklet调用都会包装在一个事物中。\",\"注册一个TaskletStep非常简单，只要添加一个实现了Tasklet接口的类即可：\",\"@Bean public Step step1() { return this.stepBuilderFactory.get(\\\"step1\\\") .tasklet(myTasklet()) //注入Tasklet的实现 .build(); } \",\"TaskletStep还支持适配器处理等，详见官网说明。\"]},{\"header\":\"控制Step执行流程\",\"slug\":\"控制step执行流程\",\"contents\":[]},{\"header\":\"顺序执行\",\"slug\":\"顺序执行\",\"contents\":[\"顺序执行通过next方法来标记：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(stepA()) .next(stepB()) //顺序执行 .next(stepC()) .build(); } \"]},{\"header\":\"条件执行\",\"slug\":\"条件执行\",\"contents\":[\"在顺序执行的过程中，在整个执行链条中有一个Step执行失败则整个Job就会停止。但是通过条件执行，可以指定各种情况下的执行分。为了实现更加复杂的控制，可以通过Step执行后的退出命名来定义条件分之。先看一个简单的代码：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(stepA()) //启动时执行的step .on(\\\"*\\\").to(stepB()) //默认跳转到stepB //当返回的ExitStatus为\\\"FAILED\\\"时，执行。 .from(stepA()).on(\\\"FAILED\\\").to(stepC()) .end() .build(); } \",\"这里使用来表示默认处理，是一个通配符表示处理任意字符串，对应的还可以使用?表示匹配任意字符。在上文中介绍了Step的退出都会有ExitStatus，命名都来源于它。下面是一个更加全面的代码。\",\"配置拦截器处理ExitCode：\",\"public class SkipCheckingListener extends StepExecutionListenerSupport { public ExitStatus afterStep(StepExecution stepExecution) { String exitCode = stepExecution.getExitStatus().getExitCode(); if (!exitCode.equals(ExitStatus.FAILED.getExitCode()) && //当Skip的Item大于0时，则指定ExitStatus的内容 stepExecution.getSkipCount() > 0) { return new ExitStatus(\\\"COMPLETED WITH SKIPS\\\"); } else { return null; } } } \",\"拦截器指示当有一个以上被跳过的记录时，返回的ExitStatus为\\\"COMPLETED WITH SKIPS\\\"。对应的控制流程：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()).on(\\\"FAILED\\\").end() //执行失败直接退出 //有跳过元素执行 errorPrint1() .from(step1()).on(\\\"COMPLETED WITH SKIPS\\\").to(errorPrint1()) .from(step1()).on(\\\"*\\\").to(step2()) //默认（成功）情况下执行 Step2 .end() .build(); } \"]},{\"header\":\"Step的停机退出机制\",\"slug\":\"step的停机退出机制\",\"contents\":[\"​ Spring Batch为Job提供了三种退出机制，这些机制为批处理的执行提供了丰富的控制方法。在介绍退出机制之前需要回顾一下 数据批处理概念一文中关于StepExecution的内容。在StepExecution中有2个表示状态的值，一个名为status，另外一个名为exitStatus。前者也被称为BatchStatus。\",\"​ 前面以及介绍了ExitStatus的使用，他可以控制Step执行链条的条件执行过程。除此之外BatchStatus也会参与到过程的控制。\"]},{\"header\":\"End退出\",\"slug\":\"end退出\",\"contents\":[\"默认情况下（没有使用end、fail方法结束），Job要顺序执行直到退出，这个退出称为end。这个时候，BatchStatus=COMPLETED、ExitStatus=COMPLETED，表示成功执行。\",\"除了Step链式处理自然退出，也可以显示调用end来退出系统。看下面的例子：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) //启动 .next(step2()) //顺序执行 .on(\\\"FAILED\\\").end() .from(step2()).on(\\\"*\\\").to(step3()) //条件执行 .end() .build(); } \",\"上面的代码，step1到step2是顺序执行，当step2的exitStatus返回\\\"FAILED\\\"时则直接End退出。其他情况执行Step3。\"]},{\"header\":\"Fail退出\",\"slug\":\"fail退出\",\"contents\":[\"除了end还可以使用fail退出，这个时候，BatchStatus=FAILED、ExitStatus=EARLY TERMINATION，表示执行失败。这个状态与End最大的区别是Job会尝试重启执行新的JobExecution。看下面代码的例子：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) //执行step1 .next(step2()).on(\\\"FAILED\\\").fail() //step2的ExitStatus=FAILED 执行fail .from(step2()).on(\\\"*\\\").to(step3()) //否则执行step3 .end() .build(); } \"]},{\"header\":\"在指定的节点中断\",\"slug\":\"在指定的节点中断\",\"contents\":[\"Spring Batch还支持在指定的节点退出，退出后下次重启会从中断的点继续执行。中断的作用是某些批处理到某个步骤后需要人工干预，当干预完之后又接着处理：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") //如果step1的ExitStatus=COMPLETED则在step2中断 .start(step1()).on(\\\"COMPLETED\\\").stopAndRestart(step2()) //否则直接退出批处理 .end() .build(); } \"]},{\"header\":\"程序化流程的分支\",\"slug\":\"程序化流程的分支\",\"contents\":[\"可以直接进行编码来控制Step之间的扭转，Spring Batch提供了JobExecutionDecider接口来协助分支管理：\",\"public class MyDecider implements JobExecutionDecider { public FlowExecutionStatus decide(JobExecution jobExecution, StepExecution stepExecution) { String status; if (someCondition()) { status = \\\"FAILED\\\"; } else { status = \\\"COMPLETED\\\"; } return new FlowExecutionStatus(status); } } \",\"接着将MyDecider作为过滤器添加到配置过程中：\",\"@Bean public Job job() { return this.jobBuilderFactory.get(\\\"job\\\") .start(step1()) .next(decider()).on(\\\"FAILED\\\").to(step2()) .from(decider()).on(\\\"COMPLETED\\\").to(step3()) .end() .build(); } \"]},{\"header\":\"流程分裂\",\"slug\":\"流程分裂\",\"contents\":[\"在线性处理过程中，流程都是一个接着一个执行的。但是为了满足某些特殊的需要，Spring Batch提供了执行的过程分裂并行Step的方法。参看下面的Job配置：\",\"@Bean public Job job() { Flow flow1 = new FlowBuilder<SimpleFlow>(\\\"flow1\\\") .start(step1()) .next(step2()) .build();//并行流程1 Flow flow2 = new FlowBuilder<SimpleFlow>(\\\"flow2\\\") .start(step3()) .build();//并行流程2 return this.jobBuilderFactory.get(\\\"job\\\") .start(flow1) .split(new SimpleAsyncTaskExecutor()) //创建一个异步执行任务 .add(flow2) .next(step4()) //2个分支执行完毕之后再执行step4。 .end() .build(); } \",\"这里表示flow1和flow2会并行执行，待2者执行成功后执行step4。\"]},{\"header\":\"数据绑定\",\"slug\":\"数据绑定\",\"contents\":[\"在Job或Step的任何位置，都可以获取到统一配置的数据。比如使用标准的Spring Framework方式：\",\"@Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"${input.file.name}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"当我们通过配置文件（application.properties中 input.file.name=filepath）或者jvm参数（-Dinput.file.name=filepath）指定某些数据时，都可以通过这种方式获取到对应的配置参数。\",\"此外，也可以从JobParameters从获取到Job运行的上下文参数：\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobParameters['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"无论是JobExecution还是StepExecution，其中的内容都可以通过这种方式去获取参数，例如：\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobExecutionContext['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"或者\",\"@StepScope @Bean public FlatFileItemReader flatFileItemReader(@Value(\\\"#{stepExecutionContext['input.file.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \"]},{\"header\":\"Step Scope\",\"slug\":\"step-scope\",\"contents\":[\"注意看上面的代码例子，都有一个@StepScope注解。这是为了进行后期绑定进行的标识。因为在Spring的IoCs容器进行初始化的阶段并没有任何的*Execution在执行，进而也不存在任何*ExecutionContext，所以这个时候根本无法注入标记的数据。所以需要使用注解显式的告诉容器直到Step执行的阶段才初始化这个@Bean。\"]},{\"header\":\"Job Scope\",\"slug\":\"job-scope\",\"contents\":[\"Job Scope的概念和 Step Scope类似，都是用于标识在到了某个执行时间段再添加和注入Bean。@JobScope用于告知框架知道JobInstance存在时候才初始化对应的@Bean：\",\"@JobScope @Bean // 初始化获取 jobParameters中的参数 public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobParameters[input]}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \",\"@JobScope @Bean // 初始化获取jobExecutionContext中的参数 public FlatFileItemReader flatFileItemReader(@Value(\\\"#{jobExecutionContext中的参数['input.name']}\\\") String name) { return new FlatFileItemReaderBuilder<Foo>() .name(\\\"flatFileItemReader\\\") .resource(new FileSystemResource(name)) ... } \"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Spring Batch\",\"Java\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring.html\":{\"title\":\"Spring 小技巧\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"收集 Spring 未归类的小知识\"]},{\"header\":\"Spring 依赖注入\",\"slug\":\"spring-依赖注入\",\"contents\":[]},{\"header\":\"注入类\",\"slug\":\"注入类\",\"contents\":[\"Spring 两种方式可以注入类：\",\"在私有属性上加注解 @Autowired\",\"@Autowired private ITestService testService; \",\"@Autowired 还支持构造方法注入，在构造方法上加上该注解，就可，支持多个参数构造注入，Spring 官方更推荐。\",\"@Autowired public DataRecordServiceImplTest(IDataRecordService recordService){ this.recordService = recordService; } \"]},{\"header\":\"注入父类\",\"slug\":\"注入父类\",\"contents\":[\"当需要注入某个类的所有子类时，可以通过Map方式注入\",\"@Autowired private Map<String, IParentService> dictServiceMap; \",\"这样就能注入 IParentService 的所有子类\",\"这里 key 不会和子类名称完全一致，是截取前n个字符。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Java\",\"Spring\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring%E6%95%B0%E6%8D%AE%E9%AA%8C%E8%AF%81.html\":{\"title\":\"Spring 数据验证\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"​ Spring Validation 是对 Hibernate Validation进行了二次封装，在Spring MVC 模块中添加了自动校验，并将校验信息封装进了特定的类中。通过框架，可以轻松完成Spring 的校验。\"]},{\"header\":\"准备工作\",\"slug\":\"准备工作\",\"contents\":[\"使用 Spring Validation，需要导入下面这个依赖包\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-validation</artifactId> <version>${spring-boot.version}</version> </dependency> \"]},{\"header\":\"使用\",\"slug\":\"使用\",\"contents\":[]},{\"header\":\"默认校验\",\"slug\":\"默认校验\",\"contents\":[\"如果使用默认注解进行校验，无需使用其他配置，下面是常用的校验注解：\",\"注解\",\"说明\",\"@Null\",\"被注释的元素必须为null\",\"@NotNull\",\"被注释的元素必须不为null\",\"@AssertTrue\",\"被注释的元素必须为true\",\"@AssertFalse\",\"被注释的元素必须为false\",\"@Min(value)\",\"被注释的元素必须是一个数字，其值必须大于等于指定的最小值\",\"@Max(value)\",\"被注释的元素必须是一个数字，其值必须小于等于指定的最大值\",\"@DecimalMin(value)\",\"被注释的元素必须是一个数字，其值必须大于等于指定的最小值\",\"@DecimalMax(value)\",\"被注释的元素必须是一个数字，其值必须小于等于指定的最大值\",\"@Size(max, min)\",\"被注释的元素的大小必须在指定的范围内\",\"@Digits (integer, fraction)\",\"被注释的元素必须是一个数字，其值必须在可接受的范围内\",\"@Past\",\"被注释的元素必须是一个过去的日期\",\"@Future\",\"被注释的元素必须是一个将来的日期\",\"@Pattern(value)\",\"被注释的元素必须符合指定的正则表达式\",\"@Email\",\"被注释的元素必须是电子邮箱地址\",\"@Length\",\"被注释的字符串的大小必须在指定的范围内\",\"@NotEmpty\",\"被注释的字符串的必须非空\",\"@Range\",\"被注释的元素必须在合适的范围内\",\"使用内置默认注解校验，需要在对应属性上加上注解，同时在需要校验的接口上使用Valid 注解，在接口所在类加上Validated 注解。\",\"import javax.validation.Valid; import org.springframework.validation.annotation.Validated; @Validated public interface IPersonService { void savePerson(@Valid Person person); } \",\"内置校验注解支持对返回的消息进行自定义 message 返回错误信息，帮助异常捕捉更好的获取信息。多重校验还可以加不同注解来实现。\",\"@Data public class Person { @NotNull(message = \\\"身份证不能为空\\\") @Size(message = \\\"身份证不正确\\\") private String idCard; @Size(min = 2) private String name; private String phone; @NotNull @Email private String email; private Date birthDate; /** * true 男性， false 女性 */ private boolean sex; } \",\"如果有多个参数需要校验，形式可以如下，即一个校验类对应一个校验结果。\",\"void funct(@Validated Person person, BindingResult fooBindingResult ，@Validated Bar bar, BindingResult barBindingResult); \"]},{\"header\":\"自定义校验\",\"slug\":\"自定义校验\",\"contents\":[\"Spring Validation 自定义校验需要一些配置，创建自己的校验注解和校验实现\",\"添加自己的校验注解：\",\"@Documented //此处填写校验实现类 @Constraint(validatedBy = ValidPersonValidator.class) @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) public @interface ValidPerson { // 默认返回信息 String message() default \\\"\\\"; // 必要信息 Class<?>[] groups() default {}; Class<? extends Payload>[] payload() default {}; } \",\"添加自己的校验实现\",\"// ValidPerson为校验注解 // Person需要校验的类 public class ValidPersonValidator implements ConstraintValidator<ValidPerson, Person> { @SneakyThrows @Override public boolean isValid(Person person, ConstraintValidatorContext context){ String msg = \\\"\\\"; if(person.getIdCard() == null || person.getIdCard().length() != 18){ msg = \\\"身份证有误\\\"; // 取消多余message显示 context.disableDefaultConstraintViolation(); // 自定义返回msg context.buildConstraintViolationWithTemplate(msg).addConstraintViolation(); return false; } return true; } } \"]}],\"customFields\":{\"1\":[\"Java\",\"Spring\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/Spring%E7%8A%B6%E6%80%81%E6%9C%BA.html\":{\"title\":\"Spring 状态机\",\"contents\":[{\"header\":\"参考\",\"slug\":\"参考\",\"contents\":[\"Spring 状态机官方文档 (spring.io)\",\"Spring Statemachine 状态机初探 - 简书 (jianshu.com)\",\"Spring StateMachine - 代码天地 (codetd.com)\",\"状态机这一概念比 Java 出现的都要早，Spring 在这一模型的基础上，做出 Spring 状态机 框架。\"]},{\"header\":\"有限状态机\",\"slug\":\"有限状态机\",\"contents\":[\"有限状态机：简称状态机，是表示有限个状态以及在这些状态之间的转移和动作等行为的数学模型。应用FSM模型可以帮助对象生命周期的状态的顺序以及导致状态变化的事件进行管理。将状态和事件控制从不同的业务Service方法的if else中抽离出来。FSM的应用范围很广，对于有复杂状态流，扩展性要求比较高的场景都可以使用该模型。 下面是状态机模型中的4个要素，即现态、条件、动作、次态。 \",\"现态：是指当前所处的状态。\",\"条件：又称为“事件”。当一个条件被满足，将会触发一个动作，或者执行一次状态的迁移。\",\"动作：条件满足后执行的动作。动作执行完毕后，可以迁移到新的状态，也可以仍旧保持原状态。动作不是必需的，当条件满足后，也可以不执行任何动作，直接迁移到新状态。\",\"次态：条件满足后要迁往的新状态。“次态”是相对于“现态”而言的，“次态”一旦被激活，就转变成新的“现态”了。\"]},{\"header\":\"状态机 DEMO\",\"slug\":\"状态机-demo\",\"contents\":[\"使用 Spring 状态机要先导入Starter包\",\"<!--spring statemachine--> <dependency> <groupId>org.springframework.statemachine</groupId> <artifactId>spring-statemachine-core</artifactId> <version>${state-machine.version}</version> </dependency> \",\"编写相关 Java 配置文件\",\"@Configuration // 开启状态机 @EnableStateMachine(name = \\\"Order\\\") public class StateMachineConfig extends EnumStateMachineConfigurerAdapter<OrderState, OrderEvent> { // 状态机初始化 @Override public void configure(StateMachineStateConfigurer<OrderState, OrderEvent> states) throws Exception { states.withStates().initial(UNCREATE).states(EnumSet.allOf(OrderState.class)); } // 编写状态机触发流程 @Override public void configure(StateMachineTransitionConfigurer<OrderState, OrderEvent> transitions) throws Exception { transitions.withExternal().source(UNCREATE).target(CREATE).event(CREATE_ORDER).and() .withExternal().source(CREATE).target(PAIED).event(PAY).and() .withExternal().source(PAIED).target(HARVESTED).event(SEND).and() .withExternal().source(HARVESTED).target(FINISH).event(CONFIRM); } } \",\"状态机流程的相关写法：\",\"withExternal 是当source和target不同时的写法，比如付款成功后状态发生的变化。\",\"withInternal 当source和target相同时的串联写法，比如付款失败后都是待付款状态。\",\"withExterna l的source和target用于执行前后状态、event为触发的事件、guard判断是否执行action。同时满足source、target、event、guard的条件后执行最后的action。\",\"withChoice 当执行一个动作，可能导致多种结果时，可以选择使用choice+guard来跳转\",\"withChoice根据guard的判断结果执行first/then的逻辑。\",\"withChoice不需要发送事件来进行触发。\",\"这里还需要使用两个枚举来表示状态和触发事件\",\"/** * 订单状态 */ public enum OrderState { // 未创建 UNCREATE, // 创建,待支付 CREATE, // 已支付，待发货 PAIED, // 待收货 HARVESTED, // 完成 FINISH; } \",\"/** * 订单事件 */ public enum OrderEvent { // 创建订单 CREATE_ORDER, // 支付 PAY, // 发货 SEND, // 确认收货 CONFIRM; } \"]},{\"header\":\"简单状态机\",\"slug\":\"简单状态机\",\"contents\":[\"要保持状态机能够恢复读取，需要将状态机持久化\",\"/** * 状态机持久化 */ @Component public class OrderStateMachinePersist implements StateMachinePersist<OrderState, OrderEvent, OrderState> { @Override public void write(StateMachineContext<OrderState, OrderEvent> stateMachineContext, OrderState orderState) throws Exception { // 默认不持久化 } @Override public StateMachineContext<OrderState, OrderEvent> read(OrderState currentState) throws Exception { return new DefaultStateMachineContext<>(currentState, null, null, null); } } \",\"/** * 注入状态机状态持久化到 Spring容器 */ @Configuration public class Config { @Autowired private OrderStateMachinePersist stateMachinePersist; @Bean public StateMachinePersister<OrderState, OrderEvent, OrderState> getPersist(){ return new DefaultStateMachinePersister<>(stateMachinePersist); } } \",\"在简单状态流当中，我们设定一个简单的订单场景，订单的状态由状态机管理\",\"然后编写业务流程，运行代码，即完成这样一个简单的 Spring 状态机\",\"@Slf4j @Service public class OrderService { @Resource private StateMachine<OrderState, OrderEvent> stateMachine; @Autowired private StateMachinePersister<OrderState, OrderEvent, OrderState> machinePersister; // 创建订单 public void createOrder(){ Message message = MessageBuilder.withPayload(OrderEvent.CREATE_ORDER).setHeader(\\\"order\\\", \\\"1\\\").build(); sendEvent(message, OrderState.UNCREATE); } // 支付 public void payed(){ Message message = MessageBuilder.withPayload(OrderEvent.PAY).setHeader(\\\"order\\\", \\\"1\\\").build(); sendEvent(message, OrderState.CREATE); } @SneakyThrows private void sendEvent(Message message, OrderState currentState){ stateMachine.start(); // 恢复状态机到当前订单状态，可由我们控制（该方法实际是重新设置状态机实例状态） machinePersister.restore(stateMachine, currentState); log.info(\\\"事件前状态:{}\\\", stateMachine.getState().getId()); stateMachine.sendEvent(message); log.info(\\\"事件后状态:{}\\\", stateMachine.getState().getId()); stateMachine.stop(); } } \"]},{\"header\":\"触发事件\",\"slug\":\"触发事件\",\"contents\":[\"​ 前面提到如果让状态机状态发生变化，实际业务，状态的变更，也会涉及一系列数据的变更， Spring 状态机提供状态变更监听，来触发我们对应的事件。\",\"@Service // 监听绑定对应的状态机 @WithStateMachine(name = \\\"Order\\\") public class OrderStateService { @SneakyThrows // 监听对应注解 @OnTransition(source = \\\"UNCREATE\\\", target = \\\"CREATE\\\") public void createOrder(Message<OrderEvent> message){ log.info(\\\"创建订单：{}\\\", message.getPayload()); } } \",\"@OnTransition 注解能够监听状态变化，触发对应事件，通过 Message 参数，可以获取状态变更传递过来的值。\"]}]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/web%E5%BA%8F%E5%88%97%E5%8C%96%E5%99%A8%E5%8A%A0%E8%BD%BD%E9%A1%BA%E5%BA%8F%E9%97%AE%E9%A2%98.html\":{\"title\":\"Web序列化器加载顺序问题\",\"contents\":[{\"header\":\"\",\"slug\":\"\",\"contents\":[\"在项目中，配置多序列化转换，有多种方式实现，使用不同方式的Spring 底层实现，导致序列化器的选择结果有很大区别，我遇到问题，导致fastjson序列化没有正确的加载，也没有走jackjson的自定义序列化\",\"我的fastjson和jackjson都做了自定义处理，jackjson优先，fastjson其次\",\"开始是这样，我的WebMvc配置继承了 WebMvcConfigurationSupport 类，但是一些需要，我需要切换为实现 WebMvcConfigurer 接口，进行编写代码时，实际并没有走我的自定义序列化方式，jackjson和fastjson，都没有被调用。\",\"通过调试，我找到了下面这段代码\",\"package org.springframework.web.servlet.mvc.method.annotation; ... protected <T> void writeWithMessageConverters(@Nullable T value, MethodParameter returnType, ServletServerHttpRequest inputMessage, ServletServerHttpResponse outputMessage) throws IOException, HttpMediaTypeNotAcceptableException, HttpMessageNotWritableException { ... for (HttpMessageConverter<?> converter : this.messageConverters) { GenericHttpMessageConverter genericConverter = (converter instanceof GenericHttpMessageConverter ? (GenericHttpMessageConverter<?>) converter : null); if (genericConverter != null ? ((GenericHttpMessageConverter) converter).canWrite(targetType, valueType, selectedMediaType) : converter.canWrite(valueType, selectedMediaType)) { body = getAdvice().beforeBodyWrite(body, returnType, selectedMediaType, (Class<? extends HttpMessageConverter<?>>) converter.getClass(), inputMessage, outputMessage); if (body != null) { Object theBody = body; LogFormatUtils.traceDebug(logger, traceOn -> \\\"Writing [\\\" + LogFormatUtils.formatValue(theBody, !traceOn) + \\\"]\\\"); addContentDispositionHeader(inputMessage, outputMessage); if (genericConverter != null) { genericConverter.write(body, targetType, selectedMediaType, outputMessage); } else { ((HttpMessageConverter) converter).write(body, selectedMediaType, outputMessage); } } ... ... \",\"这段代码，是调用序列化转换，输出序列化结果的地方，断点这儿发现默认使用的是 MappingJackson2HttpMessageConverter 序列化转换器，这就奇怪了，为什么继承WebMvcConfigurationSupport 类并没有使用这个默认的序列化器。\",\"通过监听一个List<HttpMessageConverter<?>> converters 属性发现，实现 WebMvcConfigurer 接口，会多加载很多自带的序列化转换器，而且这些默认的序列化转换器，优先级很高，在上述代码中，找到第一个合适的转换器，就会返回调用，这才导致我的两种自定义序列化方式，没有生效。\"]},{\"header\":\"解决方法\",\"slug\":\"解决方法\",\"contents\":[\"找到原因，解决就简单了，我在WebMvc配置类中，将自定义序列化器排序放在最前面即可，这样我的序列化器优先级就会很高了\",\"@Override public void configureMessageConverters(List<HttpMessageConverter<?>> converters) { CostJackson2HttpMessageConverter costJackson2HttpMessageConverter = new CostJackson2HttpMessageConverter(); converters.add(0, costJackson2HttpMessageConverter); converters.add(1, fastJsonHttpMessageConverter()); } \"]}],\"customFields\":{\"1\":[\"java\",\"Spring Web\"]}},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/SpringData/Spring%20Data%20JDBC.html\":{\"title\":\"Spring Data JDBC\",\"contents\":[{\"header\":\"准备工作\",\"slug\":\"准备工作\",\"contents\":[\"Spring Data JDBC 是对 JDBC Template 的封装，简化对JDBC数据库连接的操作。在Spring Boot当中，使用JDBC只需要两个包 Spring Data JDBC 的 Starter 包和 数据库驱动包\",\"JDBC Starter 包 内含的Spring Core模块可能会和本身 Spring Boot 内含 Spring Core模块出现版本替换到最新，导致版本不兼容问题\",\"<dependency> <groupId>org.springframework.boot</groupId> <artifactId>spring-boot-starter-data-jdbc</artifactId> <version>${jdbc.template.version}</version> </dependency> \",\"配置项上和常规数据库连接配置相同，下面使用的是Mysql配置\",\"spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver spring.datasource.url=jdbc:mysql://localhost:3306/demo?useUnicode=true&characterEncoding=utf8&useSSL=false&rewriteBatchedStatements=true&serverTimezone=GMT%2b8&nullCatalogMeansCurrent=true spring.datasource.username= spring.datasource.password= spring.datasource.type=com.zaxxer.hikari.HikariDataSource spring.datasource.hikari.auto-commit=true \"]},{\"header\":\"单条数据执行\",\"slug\":\"单条数据执行\",\"contents\":[\"JDBC 查询单挑数据，注入JdbcTemplate 即可\",\"@Autowired public PersonService(JdbcTemplate jdbcTemplate, NamedParameterJdbcTemplate namedParameterJdbcTemplate){ this.jdbcTemplate = jdbcTemplate; this.namedParameterJdbcTemplate = namedParameterJdbcTemplate; } \",\"使用示例：\",\"// 查询 public Person query(String id){ Person person = jdbcTemplate.queryForObject(\\\"select * from tb_person where id = ?\\\", rowMapper, id); return person; } // 保存 public int savePeron(Person person){ Object[] args = new Object[3]; args[0] = person.getId(); args[1] = person.getName(); args[2] = person.getIdCard(); return jdbcTemplate.update(\\\"insert into tb_person values (?,?,?)\\\", args); } // 删除 public int deletePerson(String id){ return jdbcTemplate.update(\\\"delete from tb_person where id = ?\\\", id); } // 修改 public int updatePerson(Person person){ return jdbcTemplate.update(\\\"update tb_person set name = ? where id = ?\\\", person.getName(), person.getId()); } \",\"查询有一个 rowMapper 参数，是jdbc 对查询结果的映射，需要自己编写，这样方便我们接收查询结果。\",\"/** * 数据映射 */ private RowMapper<Person> rowMapper = new RowMapper<Person>() { @Override public Person mapRow(ResultSet resultSet, int i) throws SQLException { Person person = Person.builder().id(resultSet.getString(\\\"id\\\")) .name(resultSet.getString(\\\"name\\\")) .idCard(resultSet.getString(\\\"id_card\\\")) .build(); return person; } }; \"]},{\"header\":\"批量执行\",\"slug\":\"批量执行\",\"contents\":[]},{\"header\":\"JdbcTemplate\",\"slug\":\"jdbctemplate\",\"contents\":[\"对于批量执行，使用JdbcTemplate可以做，如下:\",\"/** * 批量查询 * @return */ public List<Person> queryList(){ List<Person> personList = jdbcTemplate.query(\\\"select * from tb_person\\\", rowMapper); return personList; } /*** * 批量保存 * @param people * @return 返回-2是数据库驱动问题 */ public int[] saveAll(List<Person> people){ // 方法1 List<Object[]> args = new ArrayList<>(); for (Person person : people) { Object[] arg = new Object[3]; arg[0] = person.getId(); arg[1] = person.getName(); arg[2] = person.getIdCard(); args.add(arg); } int[] ints1 = jdbcTemplate.batchUpdate(\\\"insert into tb_person values (?,?,?)\\\", args); // 方法2 int[] ints2 = namedParameterJdbcTemplate.batchUpdate(\\\"insert into tb_person values (:id,:name,:IdCard)\\\", SqlParameterSourceUtils.createBatch(people)); return ints2; } \"]},{\"header\":\"NamedParameterJdbcTemplate\",\"slug\":\"namedparameterjdbctemplate\",\"contents\":[\"可以看到，使用查询时候，JdbcTemplate还算方便，涉及修改操作，JdbcTemplate就稍显麻烦，JDBC框架提供了一系列的工具方便复杂场景使用。NamedParameterJdbcTemplate就是一个，可以通过对sql中的参数命名，帮助我们操作。上述批量保存方法中，方法就是使用了NamedParameterJdbcTemplate的批量保存。其他批量操作也非常简单。\",\"/** * 批量删除 * @param idList * @return */ public int deletePerson(List<String> idList){ Map<String, Object> params = new HashMap<>(); params.put(\\\"param\\\", idList); int update = namedParameterJdbcTemplate.update(\\\"delete from tb_person where id in (:param)\\\", params); return update; } \",\"我们可以使用 :参数名 的形式替换 JdbcTemplate 中的? ，通过\",\"Map<String, Object> params = new HashMap<>(); \",\"来传递参数，Map的key就对应填入参数的位置。\",\"这里还使用了框架内置的SqlParameterSourceUtils静态工具提供的批处理转换方法。\"]}],\"customFields\":{\"0\":[\"Java\"],\"1\":[\"Java\",\"Spring Data\"]}},\"/wait/%E5%B7%A5%E4%BD%9C%E6%B5%81/\":{\"title\":\"工作流\",\"contents\":[]},\"/wait/\":{\"title\":\"Wait\",\"contents\":[]},\"/zh/%E5%85%B6%E4%BB%96/\":{\"title\":\"其他\",\"contents\":[]},\"/zh/\":{\"title\":\"Zh\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Java%E5%9F%BA%E7%A1%80/\":{\"title\":\"Java基础\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/SpringBatch/\":{\"title\":\"Spring Batch\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/\":{\"title\":\"Spring框架们\",\"contents\":[]},\"/zh/%E5%90%8E%E7%AB%AF/Java/Spring%E6%A1%86%E6%9E%B6%E4%BB%AC/SpringData/\":{\"title\":\"Spring Data\",\"contents\":[]}}}");self.onmessage=({data:o})=>{self.postMessage($(o.query,m[o.routeLocale]))};
//# sourceMappingURL=original.js.map
